---
title: Logic
description: ""
layout: "../../../components/layouts/ChapterLayout.astro"
---

There are (roughly) two ways of figuring things out: the proof and the experiment. 

Proofs are the domain of mathematics and are rooted in logic, which studies how conclusions follow from *premises*. Logic and proof deal with certainties. Given a set of true premises, manipulations and combinations of these premises will lead to conclusions that are either true or false. Well, *usually*. In practice, there's a third kind of statement: the undecidable (more on that in section 1.X). 

Meanwhile, experiments are the domain of science and are rooted in probability theory, which studies how conclusions follow from *evidence*. Probability theory and experiment deal in uncertainties; they relax logic's duality of true and false for a spectrum of degrees of belief[^1]. 

[^1]: Throughout this book, we'll be sticking to a Bayesian framing. See chapter 4.

Logic and probability theory codify what it means to reason. As such, they've been the basis of artificial reasoning since day one. Though modern machine learning has drifted more towards the latter from the former, logic remains a core component of making sense of AI.

As Abram Demski [writes](https://www.lesswrong.com/posts/CvKnhXTu9BPcdKE4W/an-untrollable-mathematician-illustrated):

> Probability thery has the best tools for thinking about decisions, whereas logic has the best tools for thinking about self-reference.

Understanding the unification of the two is an active area of research, believed to be important to develop an understanding of *reflectve stability*, intelligent machines that can self-modify and need to make decisions about how to do so. 

In chapter X, we'll see how logical induction provides such a unification. To get there, we'll need to understand the ins and outs of logic.

# Propositional Logic



# First-Order Logic



# Computability



# Incompleteness