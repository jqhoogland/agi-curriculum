---
index: 3
title: Outline
description: ""
authors: 
  - Jesse Hoogland: https://jessehoogland.com
lastUpdated: "2022-11-16"
published: true
headings: []
part: 1-introduction
slug: 1-introduction/3-outline
---


## A Note On Pre-Paradigmatic Science

In 1962, philosopher of science Thomas Kuhn published his seminal work *The Structure of Scientific Revolutions*. In it, Kuhn argued that science is not a linear and continuous process of accumulating knowledge but a series of discrete "paradigm shifts." 

<figure>
  <div class="flex gap-4">
    <img src="" alt="Punctuated equilibria in biology" class="flex"/>
    <img src="" alt="Paradigm shifts in science" class="flex"/>
  </div>
  <caption> 
    Kuhn's theory is reminiscent of Eldredge and Gould's punctuated equilibrium theory of evolution. Rather than evolution being a gradual process, it consists of long periods of stasis, punctuated by sudden bursts of change. 
  </caption>
</figure>

A [common](https://www.alignmentforum.org/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency#Preparadigmicity) [view](https://www.alignmentforum.org/posts/Kcbo4rXu3jYPnauoK/challenges-with-breaking-into-miri-style-research#FiYdq6pirAYMRnPJz) within AI safety is that the field is "pre-paradigmatic": we don't yet know what the right problems, questions, tools, definitions, and approaches are. 

Paradigm formation often involves unifying disparate disciplines, so AI safety is likely to be broader than other disciplines you're used to. You'll encounter a variety of disciplines: from logic to probability theory, from economics to psychology, from analytic geometry to voting theory, from neuroscience to contemporary machine learning. 

It's quite likely that many of these current approaches and tools will be discarded in the future. That's part of the process. Since it's difficult to anticipate which tools will be discarded, we've chosen to err on the side of including too much. Know that you don't need to master everything in this book to be an effective AI safety researcher.

## The Outline

The structure of this book follows Richard Ngo's [Alignment 201 curriculum](https://www.agisafetyfundamentals.com/alignment-201-curriculum):

4. [Scalable Oversight](/3-safety/4-scalable-oversight)
5. [Adversarial Robustness](/3-safety/5-adversarial-robustness)
6. [Interpretability](/3-safety/6-interpretability)
7. [Agent Foundations](/3-safety/7-agent-foundations)
8. [Theory of Deep Learning](/3-safety/8-theory-of-dl)

This is mostly a placeholder until the content is filled in.