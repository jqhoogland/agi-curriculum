[
	{
		"id": "nilsson2009",
		"type": "book",
		"event-place": "Cambridge",
		"ISBN": "978-0-511-81934-6",
		"language": "en",
		"note": "DOI: 10.1017/CBO9780511819346",
		"publisher": "Cambridge University Press",
		"publisher-place": "Cambridge",
		"source": "DOI.org (Crossref)",
		"title": "The Quest for Artificial Intelligence: A History of Ideas and Achievements",
		"title-short": "The Quest for Artificial Intelligence",
		"URL": "http://ebooks.cambridge.org/ref/id/CBO9780511819346",
		"author": [
			{
				"family": "Nilsson",
				"given": "Nils J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					7,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009"
				]
			]
		}
	},
	{
		"id": "walls2009",
		"type": "article-journal",
		"abstract": "Until 1845, Humboldt was known in the United States mostly as an explorer and recorder of the exotic American tropics and one of Europe’s leading scientists. It had been nearly twenty years since a major new work by Humboldt had appeared in English.[1] Then he began to publish Kosmos, and everything changed. A raft of book reviews alerted the intelligentsia that something new was afoot. Suddenly translations issued by both British and American publishers flooded the market: three competing versions of Kosmos, one cheap and pirated, one elegant and expensive, and one for the mass market; two competing translations of Ansichten; a new and updated translation of Personal Narrative, conveniently packaged in three trim volumes; Thrasher’s expurgated Island of Cuba; and two new biographies, all in the space of a decade, attended with a torrent of reviews and notices. Major new works on or by Humboldt continued to appear for another fifteen years or so, capped by the authoritative life-and-letters biography of 1873. In the United States, the 1850s were the decade of Humboldt, and his popularity approached cult status.",
		"container-title": "Minding Nature",
		"issue": "2",
		"language": "en",
		"source": "www.humansandnature.org",
		"title": "Introducing Humboldt's Cosmos",
		"URL": "https://www.humansandnature.org/introducing-humboldt-s-cosmos",
		"volume": "2",
		"author": [
			{
				"family": "Walls",
				"given": "Laura Dassow"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					7,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009"
				]
			]
		}
	},
	{
		"id": "doherr2012",
		"type": "article-journal",
		"abstract": "Information technology is the only way to do justice to the broad range of visions, descriptions and the idea of nature of Humboldt’s legacy. It finally leads to virtual research environments as an adequate concept to redesign our digital archives, not only for Humboldt’s documents, but for all interconnected data.",
		"container-title": "The Environmentalist",
		"DOI": "10.1007/s10669-011-9369-y",
		"ISSN": "0251-1088, 1573-2991",
		"issue": "3",
		"journalAbbreviation": "Environmentalist",
		"language": "en",
		"page": "271-277",
		"source": "DOI.org (Crossref)",
		"title": "Humboldt digital library and interconnectedness",
		"URL": "http://link.springer.com/10.1007/s10669-011-9369-y",
		"volume": "32",
		"author": [
			{
				"family": "Doherr",
				"given": "Detlev"
			},
			{
				"family": "Baron",
				"given": "Frank"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					7,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					9
				]
			]
		}
	},
	{
		"id": "hadzigeorgiou2014",
		"type": "article-journal",
		"abstract": "ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.",
		"container-title": "Science & Education",
		"issue": "10",
		"language": "en",
		"page": "1963-2006",
		"source": "www.researchgate.net",
		"title": "Romanticism and Romantic Science: Their Contribution to Science Education",
		"title-short": "(PDF) Romanticism and Romantic Science",
		"URL": "https://www.researchgate.net/publication/265973420_Romanticism_and_Romantic_Science_Their_Contribution_to_Science_Education",
		"volume": "23",
		"author": [
			{
				"family": "Hadzigeorgiou",
				"given": "Yannis"
			},
			{
				"family": "Schulz",
				"given": "Roland M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					7,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					8
				]
			]
		}
	},
	{
		"id": "larsen2010",
		"type": "article-journal",
		"abstract": "The growth rate of scientific publication has been studied from 1907 to 2007 using available data from a number of literature databases, including Science Citation Index (SCI) and Social Sciences Citation Index (SSCI). Traditional scientific publishing, that is publication in peer-reviewed journals, is still increasing although there are big differences between fields. There are no indications that the growth rate has decreased in the last 50 years. At the same time publication using new channels, for example conference proceedings, open archives and home pages, is growing fast. The growth rate for SCI up to 2007 is smaller than for comparable databases. This means that SCI was covering a decreasing part of the traditional scientific literature. There are also clear indications that the coverage by SCI is especially low in some of the scientific areas with the highest growth rate, including computer science and engineering sciences. The role of conference proceedings, open access archives and publications published on the net is increasing, especially in scientific fields with high growth rates, but this has only partially been reflected in the databases. The new publication channels challenge the use of the big databases in measurements of scientific productivity or output and of the growth rate of science. Because of the declining coverage and this challenge it is problematic that SCI has been used and is used as the dominant source for science indicators based on publication and citation numbers. The limited data available for social sciences show that the growth rate in SSCI was remarkably low and indicate that the coverage by SSCI was declining over time. National Science Indicators from Thomson Reuters is based solely on SCI, SSCI and Arts and Humanities Citation Index (AHCI). Therefore the declining coverage of the citation databases problematizes the use of this source.",
		"container-title": "Scientometrics",
		"DOI": "10.1007/s11192-010-0202-z",
		"ISSN": "0138-9130",
		"issue": "3",
		"journalAbbreviation": "Scientometrics",
		"note": "PMID: 20700371\nPMCID: PMC2909426",
		"page": "575-603",
		"source": "PubMed Central",
		"title": "The rate of growth in scientific publication and the decline in coverage provided by Science Citation Index",
		"URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2909426/",
		"volume": "84",
		"author": [
			{
				"family": "Larsen",
				"given": "Peder Olesen"
			},
			{
				"family": "Ins",
				"given": "Markus",
				"non-dropping-particle": "von"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					7,
					31
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2010",
					9
				]
			]
		}
	},
	{
		"id": "wilemski1976",
		"type": "article-journal",
		"container-title": "Journal of Statistical Physics",
		"DOI": "10.1007/BF01011764",
		"ISSN": "0022-4715, 1572-9613",
		"issue": "2",
		"journalAbbreviation": "J Stat Phys",
		"language": "en",
		"page": "153-169",
		"source": "DOI.org (Crossref)",
		"title": "On the derivation of Smoluchowski equations with corrections in the classical theory of Brownian motion",
		"URL": "http://link.springer.com/10.1007/BF01011764",
		"volume": "14",
		"author": [
			{
				"family": "Wilemski",
				"given": "Gerald"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1976",
					2
				]
			]
		}
	},
	{
		"id": "piasecki2006",
		"type": "article-journal",
		"container-title": "Acta Physica Polonica B",
		"issue": "5",
		"language": "en",
		"note": "PACS numbers: 05.40.Jc, 05.20.Dd, 01.65.+g",
		"page": "7",
		"source": "Zotero",
		"title": "Centenary of Marian Smoluchowski's theory of Brownian motion",
		"volume": "38",
		"author": [
			{
				"family": "Piasecki",
				"given": "Jarosław"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2006"
				]
			]
		}
	},
	{
		"id": "einstein1990",
		"type": "book",
		"number-of-volumes": "15",
		"publisher": "Princeton University Press",
		"title": "The Collected Papers of Albert Einstein",
		"title-short": "Writing 1900-1909",
		"URL": "https://einsteinpapers.press.princeton.edu/vol2-doc/77",
		"volume": "2",
		"author": [
			{
				"family": "Einstein",
				"given": "Albert"
			}
		],
		"editor": [
			{
				"family": "Stachel",
				"given": "John"
			},
			{
				"family": "Cassidy",
				"given": "David"
			},
			{
				"family": "Renn",
				"given": "Jurgen"
			},
			{
				"family": "Schulmann",
				"given": "Robert"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1990"
				]
			]
		}
	},
	{
		"id": "jensen1998",
		"type": "book",
		"call-number": "BF433.G45 J46 1998",
		"collection-title": "Human evolution, behavior, and intelligence",
		"event-place": "Westport, Conn",
		"ISBN": "978-0-275-96103-9",
		"language": "en",
		"number-of-pages": "648",
		"publisher": "Praeger",
		"publisher-place": "Westport, Conn",
		"source": "Library of Congress ISBN",
		"title": "The g factor: the science of mental ability",
		"title-short": "The g factor",
		"author": [
			{
				"family": "Jensen",
				"given": "Arthur Robert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1998"
				]
			]
		}
	},
	{
		"id": "detterman1989",
		"type": "article-journal",
		"container-title": "Intelligence",
		"DOI": "10.1016/S0160-2896(89)80007-8",
		"ISSN": "01602896",
		"issue": "4",
		"journalAbbreviation": "Intelligence",
		"language": "en",
		"page": "349-359",
		"source": "DOI.org (Crossref)",
		"title": "Correlations of mental tests with each other and with cognitive variables are highest for low IQ groups",
		"URL": "https://linkinghub.elsevier.com/retrieve/pii/S0160289689800078",
		"volume": "13",
		"author": [
			{
				"family": "Detterman",
				"given": "Douglas K."
			},
			{
				"family": "Daniel",
				"given": "Mark H."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1989",
					10
				]
			]
		}
	},
	{
		"id": "sternberg2002",
		"type": "book",
		"abstract": "This edited volume presents a balanced approach to the ongoing debate of just how general the \"general factor\" of intelligence is. To accomplish this goal, the editors chose a number of distinct approaches to the study of intelligence--psychometric, genetic-epistemological, cognitive, biological, behavior-genetic, sociocultural, systems--and asked distinguished scholars to write from the standpoint of these approaches. Each approach comprises two chapters, one by a scholar leaning toward a view arguing for the greater generality of g, and the other by a scholar leaning toward a view arguing for the lesser generality of g. The scholars are not simply \"for\" or \"against\" these outlooks, rather they provide a more textured view of the general factor, attempting to explain it in psychological terms that are easily understandable.  Intended for psychologists in all areas, including clinical, consulting, educational, cognitive, school, developmental, and industrial-organizational, this book will also be of interest to educators, sociologists, anthropologists, and those interested in the nature of intelligence.",
		"ISBN": "978-1-135-65515-0",
		"language": "en",
		"note": "Google-Books-ID: eih5AgAAQBAJ",
		"number-of-pages": "518",
		"publisher": "Psychology Press",
		"source": "Google Books",
		"title": "The General Factor of Intelligence: How General Is It?",
		"title-short": "The General Factor of Intelligence",
		"author": [
			{
				"family": "Sternberg",
				"given": "Robert J."
			},
			{
				"family": "Grigorenko",
				"given": "Elena L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2002",
					5
				]
			]
		}
	},
	{
		"id": "einstein1905",
		"type": "article-journal",
		"container-title": "Annalen der Physik",
		"issue": "8",
		"language": "de",
		"page": "549-560",
		"source": "Zotero",
		"title": "Über die von der molekularkinetischen Theorie der Wärme geforderte Bewegung von in ruhenden Flüssigkeiten suspendierten Teilchen",
		"volume": "322",
		"author": [
			{
				"family": "Einstein",
				"given": "Albert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1905"
				]
			]
		}
	},
	{
		"id": "smoluchowski1915",
		"type": "article-journal",
		"container-title": "Annalen der Physik",
		"issue": "24",
		"language": "de",
		"page": "1103-1112",
		"title": "Über Brown'sche Molekularbewegung unter Einwirkung äusserer Kråfte und Deren Zusammenhang mit der Verallgemeinerten Diffusionsgleichung",
		"URL": "http://matwbn.icm.edu.pl/ksiazki/pms/pms2/pms2132.pdf",
		"volume": "353",
		"author": [
			{
				"family": "Smoluchowski",
				"given": "Marian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					18
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1915"
				]
			]
		}
	},
	{
		"id": "smoluchowksi1906",
		"type": "article-journal",
		"container-title": "Annalen der Physik",
		"issue": "14",
		"language": "de",
		"page": "756-780",
		"title": "Zur kinetishen Theorie der Brownschen Molekularbewegung und der Suspensionen",
		"URL": "http://myweb.rz.uni-augsburg.de/~eckern/adp/history/historic-papers/1906_326_756-780.pdf",
		"volume": "326",
		"author": [
			{
				"family": "Smoluchowksi",
				"given": "Marian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					18
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1906"
				]
			]
		}
	},
	{
		"id": "langevin1908",
		"type": "article-journal",
		"abstract": "We present a translation of Paul Langevin’s landmark paper. In it Langevin successfully applied Newtonian dynamics to a Brownian particle and so invented an analytical approach to random processes which has remained useful to this day.",
		"archive_location": "world",
		"container-title": "American Journal of Physics",
		"DOI": "10.1119/1.18725",
		"ISSN": "0002-9505",
		"issue": "11",
		"language": "en",
		"license": "© 1997 American Association of Physics Teachers.",
		"page": "1079",
		"source": "aapt.scitation.org",
		"title": "Paul Langevin’s 1908 paper “On the Theory of Brownian Motion” [“Sur la théorie du mouvement brownien,” C. R. Acad. Sci. (Paris) 146, 530–533 (1908)]",
		"URL": "https://aapt.scitation.org/doi/abs/10.1119/1.18725",
		"volume": "65",
		"author": [
			{
				"family": "Langevin",
				"given": "Paul"
			}
		],
		"translator": [
			{
				"family": "Lemons",
				"given": "Don S."
			},
			{
				"family": "Gythiel",
				"given": "Anthony"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1998",
					6,
					4
				]
			]
		}
	},
	{
		"id": "ahrens2017",
		"type": "book",
		"abstract": "The key to good and efficient writing lies in the intelligent organisation of ideas and notes. This book helps students, academics and no...",
		"title": "How to Take Smart Notes: One Simple Technique to Boost Writing, Learning and Thinking – for Students, Academics and Nonfiction Book Writers",
		"URL": "https://www.goodreads.com/work/best_book/55634026-how-to-take-smart-notes-one-simple-technique-to-boost-writing-learning",
		"author": [
			{
				"family": "Ahrens",
				"given": "Sönke"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "duckworth2005",
		"type": "article-journal",
		"abstract": "In a longitudinal study of 140 eighth-grade students, self-discipline measured by self-report, parent report, teacher report, and monetary choice questionnaires in the fall predicted final grades, school attendance, standardized achievement-test scores, and selection into a competitive high school program the following spring. In a replication with 164 eighth graders, a behavioral delay-of-gratification task, a questionnaire on study habits, and a group-administered IQ test were added. Self-discipline measured in the fall accounted for more than twice as much variance as IQ in final grades, high school selection, school attendance, hours spent doing homework, hours spent watching television (inversely), and the time of day students began their homework. The effect of self-discipline on final grades held even when controlling for first-marking-period grades, achievement-test scores, and measured IQ. These findings suggest a major reason for students falling short of their intellectual potential: their failure to exercise self-discipline.",
		"container-title": "Psychological Science",
		"DOI": "10.1111/j.1467-9280.2005.01641.x",
		"ISSN": "0956-7976",
		"issue": "12",
		"journalAbbreviation": "Psychol Sci",
		"language": "en",
		"note": "publisher: SAGE Publications Inc",
		"page": "939-944",
		"source": "SAGE Journals",
		"title": "Self-Discipline Outdoes IQ in Predicting Academic Performance of Adolescents",
		"URL": "https://journals.sagepub.com/doi/abs/10.1111/j.1467-9280.2005.01641.x",
		"volume": "16",
		"author": [
			{
				"family": "Duckworth",
				"given": "Angela L."
			},
			{
				"family": "Seligman",
				"given": "Martin E.P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2005",
					12,
					1
				]
			]
		}
	},
	{
		"id": "tangney2004",
		"type": "article-journal",
		"abstract": "What good is self-control? We incorporated a new measure of individual differences in self-control into two large investigations of a broad spectrum of behaviors. The new scale showed good internal consistency and retest reliability. Higher scores on self-control correlated with a higher grade point average, better adjustment (fewer reports of psychopathology, higher self-esteem), less binge eating and alcohol abuse, better relationships and interpersonal skills, secure attachment, and more optimal emotional responses. Tests for curvilinearity failed to indicate any drawbacks of so-called overcontrol, and the positive effects remained after controlling for social desirability. Low self-control is thus a significant risk factor for a broad range of personal and interpersonal problems.",
		"container-title": "Journal of Personality",
		"DOI": "10.1111/j.0022-3506.2004.00263.x",
		"ISSN": "1467-6494",
		"issue": "2",
		"language": "en",
		"note": "_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0022-3506.2004.00263.x",
		"page": "271-324",
		"source": "Wiley Online Library",
		"title": "High Self-Control Predicts Good Adjustment, Less Pathology, Better Grades, and Interpersonal Success",
		"URL": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0022-3506.2004.00263.x",
		"volume": "72",
		"author": [
			{
				"family": "Tangney",
				"given": "June P."
			},
			{
				"family": "Baumeister",
				"given": "Roy F."
			},
			{
				"family": "Boone",
				"given": "Angie Luzio"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2004"
				]
			]
		}
	},
	{
		"id": "baumeister1998",
		"type": "article-journal",
		"container-title": "Journal of Personality and Social Psychology",
		"issue": "5",
		"page": "1252-1265",
		"title": "Ego Depletion: Is the Active Self a Limited Resource?.",
		"URL": "https://ovidsp-dc2-ovid-com.proxy.uba.uva.nl:2443/ovid-b/ovidweb.cgi?QS2=434f4e1a73d37e8cc073476c0173649e725ac3b7ed089f52b68cdf3127bd91b78c043d15ebeea608e119b0c6d16b1126c094c9f77b61c6bc681e88164e641c708b0d1fa120edf104f870ae67ad57209b7b09bcfbd6e753e5d851394c8a7c8d6bf2524a21bbaf62bfe21dc34ffc5805c5cefc7ca5720e835c3cc168a949c8aa33886ba66b91d01b33d4f8276acf9fcc2a641f2c2c3b60e7674bb54d07e287c157270fd4f3e646c863a8c3943ddd3b954c15aa52d92a248c0b8dbd8e707c195be4b8152d1a1d321ab0ebb96ce33243942dce85018f1183d0c3a07228beb9a3bfa49850ca506e6a90dee6b842e288462898d20dc6484585b5068077dd34680001412fdf61ddef00cb5b",
		"volume": "74",
		"author": [
			{
				"family": "Baumeister",
				"given": "Roy F."
			},
			{
				"family": "Bratslavsky",
				"given": "Ellen"
			},
			{
				"literal": "Mark Muraven"
			},
			{
				"family": "Tice",
				"given": "Dianne M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1998",
					5
				]
			]
		}
	},
	{
		"id": "muraven1998",
		"type": "article-journal",
		"container-title": "Journal of Personality and Social Psychology",
		"issue": "3",
		"language": "en",
		"page": "774-789",
		"title": "Self-Control as Limited Resource: Regulatory Depletion Patterns.",
		"URL": "https://ovidsp-dc2-ovid-com.proxy.uba.uva.nl:2443/ovid-b/ovidweb.cgi?QS2=434f4e1a73d37e8c89b065ac1284285ff87cdba82a07662bee85f36f4230fd164fd5c900ea6e9376a356c6e201692b0f50a1f5600e0f000b4a80cba621b5dc1c60bac1cce7cfaa9b6b28c1a4e22eddf58dda6e1726888454517791af37fb5e1c6f3ebbce8d280b0029701790fc90c9f6e001ec7fb4940a050448873ae49d970fef285bde8185ba549a45f4c4b40a915218481ca5264b8c8e313811ff58d26456791b498cda306d24df789e4c10a52bd92af61d233dcb398e709237f4e99a439beb2f9643446bc3586f1444ab3783f7acb023b93e15d5239477936af50f848c2216e87453e8274f6384cd05d70628516c2d53c8690568cd1b005f083876ffe29ed2180c91bd83b670",
		"volume": "74",
		"author": [
			{
				"family": "Muraven",
				"given": "Mark"
			},
			{
				"family": "Tice",
				"given": "Dianne M."
			},
			{
				"family": "Baumeister",
				"given": "Roy F."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1998",
					3
				]
			]
		}
	},
	{
		"id": "schmeichel2003",
		"type": "article-journal",
		"container-title": "Journal of Personality and Social Psychology",
		"issue": "1",
		"page": "33-46",
		"title": "Intellectual Performance and Ego Depletion: Role of the Self in Logical Reasoning and Other Information Processing.",
		"URL": "https://ovidsp-dc2-ovid-com.proxy.uba.uva.nl:2443/ovid-b/ovidweb.cgi?QS2=434f4e1a73d37e8c4cb0e1de3812889002018612c0c9d7fe52e3c4c0b31ea967b7917a0cda3139c8e8f8559369f0feb8bef13245a58fe052392da849cfebce69b4abe205948dbc0471a886b03edd513b7699c68dbfe43bacbb1b82b9425065925715be82fb1bf74f319996f081b390ef97607cf7e2340ffe74288c6ee0baafdb42e3e18bfe7eca9cfe9757b70072fe9db5194aa0c2c071f474aa3d392854b287980a8751dd6d9ee466e062875b9d3719447e2cbb7cd3f349f78bade3102e1e65cb936ec0361cca96d3c94525275aeef3b97eee1ce35e0972f26731c4dd306dd800d1bef250b9284dfa44f9cb317a9ca9cf7716f81e74cf8cab5a017801f63934521446ac311ce2a3",
		"volume": "85",
		"author": [
			{
				"family": "Schmeichel",
				"given": "Brandon J."
			},
			{
				"family": "Vohs",
				"given": "Kathleen D."
			},
			{
				"family": "Baumeister",
				"given": "Roy F."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2003",
					7
				]
			]
		}
	},
	{
		"id": "csikszentmihalyi1990",
		"type": "book",
		"publisher": "Harper & Row New York",
		"title": "Flow: The psychology of optimal experience",
		"volume": "1990",
		"author": [
			{
				"family": "Csikszentmihalyi",
				"given": "Mihaly"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1990"
				]
			]
		}
	},
	{
		"id": "kruger1999",
		"type": "article-journal",
		"abstract": "People tend to hold overly favorable views of their abilities in many social and intellectual domains. The authors suggest that this overestimation occurs, in part, because people who are unskilled in these domains suffer a dual burden: Not only do these people reach erroneous conclusions and make unfortunate choices, but their incompetence robs them of the metacognitive ability to realize it. Across 4 studies, the authors found that participants scoring in the bottom quartile on tests of humor, grammar, and logic grossly overestimated their test performance and ability. Although their test scores put them in the 12th percentile, they estimated themselves to be in the 62nd. Several analyses linked this miscalibration to deficits in metacognitive skill, or the capacity to distinguish accuracy from error. Paradoxically, improving the skills of participants, and thus increasing their metacognitive competence, helped them recognize the limitations of their abilities. It is one of the essential features of such incompetence that the person so afflicted is incapable of knowing that he is incompetent. To have such knowledge would already be to remedy a good portion of the offense. (Miller, 1993, p. 4) In 1995, McArthur Wheeler walked into two Pittsburgh banks",
		"container-title": "Journal of Personality and Social Psychology",
		"page": "1121–1134",
		"source": "CiteSeer",
		"title": "Unskilled and Unaware of It: How Difficulties in Recognizing One’s Own Incompetence Lead to Inflated Self-Assessments",
		"title-short": "Unskilled and Unaware of It",
		"volume": "77",
		"author": [
			{
				"family": "Kruger",
				"given": "Justin"
			},
			{
				"family": "Dunning",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1999"
				]
			]
		}
	},
	{
		"id": "mmxx",
		"type": "webpage",
		"title": "Build tools around workflows, not workflows around tools | thesephist.com",
		"URL": "https://thesephist.com/posts/tools/",
		"author": [
			{
				"family": "MMXX",
				"given": "Linus"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					23
				]
			]
		}
	},
	{
		"id": "krizhevsky2012",
		"type": "paper-conference",
		"container-title": "Advances in Neural Information Processing Systems 25",
		"event-title": "NIPS",
		"page": "1097–1105",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "ImageNet Classification with Deep Convolutional Neural Networks",
		"URL": "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf",
		"author": [
			{
				"family": "Krizhevsky",
				"given": "Alex"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Hinton",
				"given": "Geoffrey E"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "toshev2014",
		"type": "paper-conference",
		"abstract": "We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regressors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning. We present a detailed empirical analysis with state-of-art or better performance on four academic benchmarks of diverse real-world images.",
		"container-title": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
		"DOI": "10.1109/CVPR.2014.214",
		"event-title": "IEEE Conference on Computer Vision andPattern Recognition",
		"note": "arXiv: 1312.4659",
		"page": "1653-1660",
		"source": "arXiv.org",
		"title": "DeepPose: Human Pose Estimation via Deep Neural Networks",
		"title-short": "DeepPose",
		"URL": "http://arxiv.org/abs/1312.4659",
		"author": [
			{
				"family": "Toshev",
				"given": "Alexander"
			},
			{
				"family": "Szegedy",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					6
				]
			]
		}
	},
	{
		"id": "babu2019",
		"type": "webpage",
		"container-title": "Nanonets",
		"title": "A 2019 guide to Human Pose Estimation with Deep Learning",
		"URL": "https://nanonets.com/blog/human-pose-estimation-2d-guide/",
		"author": [
			{
				"family": "Babu",
				"given": "Sudharshan Chandra"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "babu2019a",
		"type": "webpage",
		"abstract": "Human Pose Estimation is an important step towards understanding people in images and videos. In this post, I write about the basics of 3D Human Pose Estimation and review the literature on this topic.",
		"container-title": "Nanonets",
		"language": "en",
		"title": "A 2019 guide to 3D Pose Estimation",
		"URL": "https://nanonets.com/blog/human-pose-estimation-3d-guide/",
		"author": [
			{
				"family": "Babu",
				"given": "Sudharshan Chandra"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "cao2016",
		"type": "paper-conference",
		"abstract": "We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.",
		"container-title": "arXiv:1611.08050 [cs]",
		"event-place": "Honolulu",
		"note": "arXiv: 1611.08050",
		"publisher-place": "Honolulu",
		"source": "arXiv.org",
		"title": "Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields",
		"URL": "http://arxiv.org/abs/1611.08050",
		"author": [
			{
				"family": "Cao",
				"given": "Zhe"
			},
			{
				"family": "Simon",
				"given": "Tomas"
			},
			{
				"family": "Wei",
				"given": "Shih-En"
			},
			{
				"family": "Sheikh",
				"given": "Yaser"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "mathis2018",
		"type": "article-journal",
		"abstract": "Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled (~200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.",
		"container-title": "Nature Neuroscience",
		"DOI": "10.1038/s41593-018-0209-y",
		"ISSN": "1546-1726",
		"issue": "9",
		"language": "en",
		"license": "2018 The Author(s)",
		"note": "number: 9\npublisher: Nature Publishing Group",
		"page": "1281-1289",
		"source": "www.nature.com",
		"title": "DeepLabCut: markerless pose estimation of user-defined body parts with deep learning",
		"title-short": "DeepLabCut",
		"URL": "https://www.nature.com/articles/s41593-018-0209-y",
		"volume": "21",
		"author": [
			{
				"family": "Mathis",
				"given": "Alexander"
			},
			{
				"family": "Mamidanna",
				"given": "Pranav"
			},
			{
				"family": "Cury",
				"given": "Kevin M."
			},
			{
				"family": "Abe",
				"given": "Taiga"
			},
			{
				"family": "Murthy",
				"given": "Venkatesh N."
			},
			{
				"family": "Mathis",
				"given": "Mackenzie Weygandt"
			},
			{
				"family": "Bethge",
				"given": "Matthias"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					9
				]
			]
		}
	},
	{
		"id": "pereira2018",
		"type": "article-journal",
		"abstract": "The need for automated and efficient systems for tracking full animal pose has increased with the complexity of behavioral data and analyses. Here we introduce LEAP (LEAP estimates animal pose), a deep-learning-based method for predicting the positions of animal body parts. This framework consists of a graphical interface for labeling of body parts and training the network. LEAP offers fast prediction on new data, and training with as few as 100 frames results in 95% of peak performance. We validated LEAP using videos of freely behaving fruit flies and tracked 32 distinct points to describe the pose of the head, body, wings and legs, with an error rate of <3% of body length. We recapitulated reported findings on insect gait dynamics and demonstrated LEAP’s applicability for unsupervised behavioral classification. Finally, we extended the method to more challenging imaging situations and videos of freely moving mice.",
		"container-title": "Nature Methods",
		"DOI": "10.1038/s41592-018-0234-5",
		"ISSN": "1548-7105",
		"issue": "1",
		"language": "en",
		"license": "2018 The Author(s), under exclusive licence to Springer Nature America, Inc.",
		"note": "number: 1\npublisher: Nature Publishing Group",
		"page": "117-125",
		"source": "www.nature.com",
		"title": "Fast animal pose estimation using deep neural networks",
		"URL": "https://www.nature.com/articles/s41592-018-0234-5",
		"volume": "16",
		"author": [
			{
				"family": "Pereira",
				"given": "Talmo D."
			},
			{
				"family": "Aldarondo",
				"given": "Diego E."
			},
			{
				"family": "Willmore",
				"given": "Lindsay"
			},
			{
				"family": "Kislin",
				"given": "Mikhail"
			},
			{
				"family": "Wang",
				"given": "Samuel S.-H."
			},
			{
				"family": "Murthy",
				"given": "Mala"
			},
			{
				"family": "Shaevitz",
				"given": "Joshua W."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "graving2019",
		"type": "article-journal",
		"abstract": "Quantitative behavioral measurements are important for answering questions across scientific disciplines—from neuroscience to ecology. State-of-the-art deep-learning methods offer major advances in data quality and detail by allowing researchers to automatically estimate locations of an animal’s body parts directly from images or videos. However, currently available animal pose estimation methods have limitations in speed and robustness. Here, we introduce a new easy-to-use software toolkit, DeepPoseKit, that addresses these problems using an efficient multi-scale deep-learning model, called Stacked DenseNet, and a fast GPU-based peak-detection algorithm for estimating keypoint locations with subpixel precision. These advances improve processing speed >2x with no loss in accuracy compared to currently available methods. We demonstrate the versatility of our methods with multiple challenging animal pose estimation tasks in laboratory and field settings—including groups of interacting individuals. Our work reduces barriers to using advanced tools for measuring behavior and has broad applicability across the behavioral sciences.",
		"container-title": "eLife",
		"DOI": "10.7554/eLife.47994",
		"ISSN": "2050-084X",
		"note": "publisher: eLife Sciences Publications, Ltd",
		"page": "e47994",
		"source": "eLife",
		"title": "DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning",
		"URL": "https://doi.org/10.7554/eLife.47994",
		"volume": "8",
		"author": [
			{
				"family": "Graving",
				"given": "Jacob M"
			},
			{
				"family": "Chae",
				"given": "Daniel"
			},
			{
				"family": "Naik",
				"given": "Hemal"
			},
			{
				"family": "Li",
				"given": "Liang"
			},
			{
				"family": "Koger",
				"given": "Benjamin"
			},
			{
				"family": "Costelloe",
				"given": "Blair R"
			},
			{
				"family": "Couzin",
				"given": "Iain D"
			}
		],
		"editor": [
			{
				"family": "Baldwin",
				"given": "Ian T"
			},
			{
				"family": "Shaevitz",
				"given": "Josh W"
			},
			{
				"family": "Stephens",
				"given": "Greg"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					10,
					1
				]
			]
		}
	},
	{
		"id": "costa2020",
		"type": "thesis",
		"event-place": "Amsterdam",
		"genre": "PhD",
		"language": "English",
		"publisher": "VU",
		"publisher-place": "Amsterdam",
		"source": "research.vu.nl",
		"title": "Physics of behavior across scales: A dynamical systems approach to the representation and understanding of animal movement",
		"title-short": "Physics of behavior across scales",
		"URL": "https://research.vu.nl/en/publications/physics-of-behavior-across-scales-a-dynamical-systems-approach-to",
		"author": [
			{
				"family": "Costa",
				"given": "A. C. Borges Santos",
				"dropping-particle": "da"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "site:history-of-gases",
		"type": "webpage",
		"abstract": "Use this timeline for a look at some of the historical aspects in the development of our understanding of gases and plasmas.",
		"container-title": "Science Learning Hub",
		"language": "en",
		"title": "History of gases and plasmas – timeline",
		"URL": "https://www.sciencelearn.org.nz/resources/1689-history-of-gases-and-plasmas-timeline",
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		}
	},
	{
		"id": "site:mendeleev-pt",
		"type": "webpage",
		"abstract": "Learn about and revise the periodic table with this BBC Bitesize GCSE Chemistry (Edexcel) study guide.",
		"container-title": "BBC Bitesize",
		"language": "en-GB",
		"title": "Mendeleev's periodic table - The periodic table - Edexcel - GCSE Chemistry (Single Science) Revision - Edexcel",
		"URL": "https://www.bbc.co.uk/bitesize/guides/zxmmsrd/revision/1",
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		}
	},
	{
		"id": "img:flocking",
		"type": "motion_picture",
		"abstract": "Discover & share this Originals GIF with everyone you know. GIPHY is how you search, share, discover, and create GIFs.",
		"source": "media.giphy.com",
		"title": "Spain Flock GIF",
		"URL": "https://i.giphy.com/media/3oEduIOpXCpGxagbQY/giphy.gif",
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		}
	},
	{
		"id": "vicsek1995",
		"type": "article-journal",
		"abstract": "A simple model with a novel type of dynamics is introduced in order to investigate the emergence of self-ordered motion in systems of particles with biologically motivated interaction. In our model particles are driven with a constant absolute velocity and at each time step assume the average direction of motion of the particles in their neighborhood with some random perturbation (η) added. We present numerical evidence that this model results in a kinetic phase transition from no transport (zero average velocity, |va|=0) to finite net transport through spontaneous symmetry breaking of the rotational symmetry. The transition is continuous, since |va| is found to scale as (ηc−η)β with β≃0.45.",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.75.1226",
		"issue": "6",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"note": "publisher: American Physical Society",
		"page": "1226-1229",
		"source": "APS",
		"title": "Novel Type of Phase Transition in a System of Self-Driven Particles",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevLett.75.1226",
		"volume": "75",
		"author": [
			{
				"family": "Vicsek",
				"given": "Tamás"
			},
			{
				"family": "Czirók",
				"given": "András"
			},
			{
				"family": "Ben-Jacob",
				"given": "Eshel"
			},
			{
				"family": "Cohen",
				"given": "Inon"
			},
			{
				"family": "Shochet",
				"given": "Ofer"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1995",
					8,
					7
				]
			]
		}
	},
	{
		"id": "lewis2013",
		"type": "report",
		"publisher": "University of Warwick",
		"title": "CO904 Assignment II",
		"URL": "https://warwick.ac.uk/fac/cross_fac/complexity/people/students/dtc/students2013/lewis/co904/",
		"author": [
			{
				"family": "Lewis",
				"given": "Jason"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		}
	},
	{
		"id": "cavagna2008",
		"type": "article-journal",
		"abstract": "The most startling examples of collective animal behaviour are provided by very large and cohesive groups moving in three dimensions. Paradigmatic examples are bird flocks, fish schools and insect swarms. However, because of the sheer technical difficulty of obtaining 3D data, empirical studies conducted to date have only considered loose groups of a few tens of animals. Moreover, these studies were very seldom conducted in the field. Recently the STARFLAG project achieved the 3D reconstruction of thousands of birds under field conditions, thus opening the way to a new generation of quantitative studies of collective animal behaviour. Here, we review the main technical problems in 3D data collection of large animal groups and we outline some of the methodological solutions adopted by the STARFLAG project. In particular, we explain how to solve the stereoscopic correspondence - or matching - problem, which was the major bottleneck of all 3D studies in the past.",
		"container-title": "arXiv:0802.1668 [cond-mat, q-bio]",
		"note": "arXiv: 0802.1668",
		"source": "arXiv.org",
		"title": "The STARFLAG handbook on collective animal behaviour: Part I, empirical methods",
		"title-short": "The STARFLAG handbook on collective animal behaviour",
		"URL": "http://arxiv.org/abs/0802.1668",
		"author": [
			{
				"family": "Cavagna",
				"given": "Andrea"
			},
			{
				"family": "Giardina",
				"given": "Irene"
			},
			{
				"family": "Orlandi",
				"given": "Alberto"
			},
			{
				"family": "Parisi",
				"given": "Giorgio"
			},
			{
				"family": "Procaccini",
				"given": "Andrea"
			},
			{
				"family": "Viale",
				"given": "Massimiliano"
			},
			{
				"family": "Zdravkovic",
				"given": "Vladimir"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2008",
					2,
					12
				]
			]
		}
	},
	{
		"id": "bozek2018",
		"type": "article-journal",
		"abstract": "From human crowds to cells in tissue, the detection and efficient tracking of multiple objects in dense configurations is an important and unsolved problem. In the past, limitations of image analysis have restricted studies of dense groups to tracking a single or subset of marked individuals, or to coarse-grained group-level dynamics, all of which yield incomplete information. Here, we combine convolutional neural networks (CNNs) with the model environment of a honeybee hive to automatically recognize all individuals in a dense group from raw image data. We create new, adapted individual labeling and use the segmentation architecture U-Net with a loss function dependent on both object identity and orientation. We additionally exploit temporal regularities of the video recording in a recurrent manner and achieve near human-level performance while reducing the network size by 94% compared to the original U-Net architecture. Given our novel application of CNNs, we generate extensive problem-specific image data in which labeled examples are produced through a custom interface with Amazon Mechanical Turk. This dataset contains over 375,000 labeled bee instances across 720 video frames at 2 FPS, representing an extensive resource for the development and testing of tracking methods. We correctly detect 96% of individuals with a location error of ~7% of a typical body dimension, and orientation error of 12 degrees, approximating the variability of human raters. Our results provide an important step towards efficient image-based dense object tracking by allowing for the accurate determination of object location and orientation across time-series image data efficiently within one network architecture.",
		"container-title": "arXiv:1712.08324 [cs, q-bio, stat]",
		"note": "arXiv: 1712.08324\nversion: 1",
		"source": "arXiv.org",
		"title": "Towards dense object tracking in a 2D honeybee hive",
		"URL": "http://arxiv.org/abs/1712.08324",
		"author": [
			{
				"family": "Bozek",
				"given": "Katarzyna"
			},
			{
				"family": "Hebert",
				"given": "Laetitia"
			},
			{
				"family": "Mikheyev",
				"given": "Alexander S."
			},
			{
				"family": "Stephens",
				"given": "Greg J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					12,
					22
				]
			]
		}
	},
	{
		"id": "anderson1972",
		"type": "article-journal",
		"container-title": "Science",
		"DOI": "10.1126/science.177.4047.393",
		"ISSN": "0036-8075, 1095-9203",
		"issue": "4047",
		"language": "en",
		"license": "© 1972",
		"note": "publisher: American Association for the Advancement of Science\nsection: Articles\nPMID: 17796623",
		"page": "393-396",
		"source": "science.sciencemag.org",
		"title": "More Is Different",
		"URL": "https://science.sciencemag.org/content/177/4047/393",
		"volume": "177",
		"author": [
			{
				"family": "Anderson",
				"given": "P. W."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1972",
					8,
					4
				]
			]
		}
	},
	{
		"id": "bell1964",
		"type": "article-journal",
		"container-title": "Physics",
		"issue": "3",
		"language": "en",
		"page": "195-200",
		"source": "Zotero",
		"title": "On the Einstein Podolsky Rosen Paradox",
		"volume": "1",
		"author": [
			{
				"family": "Bell",
				"given": "John S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"1964"
				]
			]
		}
	},
	{
		"id": "lucretius",
		"type": "book",
		"title": "De Rerum Natura",
		"URL": "http://www.perseus.tufts.edu/hopper/text?doc=Lucr.",
		"author": [
			{
				"family": "Lucretius",
				"given": ""
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					26
				]
			]
		}
	},
	{
		"id": "img:leonardo-turbulence",
		"type": "article-journal",
		"abstract": "On the 500th anniversary of the Renaissance icon’s death, Martin Kemp looks anew at his innovative experimental models for the motion of water and blood.",
		"container-title": "Nature",
		"DOI": "10.1038/d41586-019-02144-z",
		"issue": "7765",
		"language": "en",
		"license": "2020 Nature",
		"note": "number: 7765\npublisher: Nature Publishing Group",
		"page": "322-323",
		"source": "www.nature.com",
		"title": "Leonardo da Vinci’s laboratory: studies in flow",
		"title-short": "Leonardo da Vinci’s laboratory",
		"URL": "https://www.nature.com/articles/d41586-019-02144-z",
		"volume": "571",
		"author": [
			{
				"family": "Kemp",
				"given": "Martin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					7,
					16
				]
			]
		}
	},
	{
		"id": "darrigol2005",
		"type": "book",
		"abstract": "The first of its kind, this book is an in-depth history of hydrodynamics from its eighteenth-century foundations to its first major successes in twentieth-century hydraulics and aeronautics. It documents the foundational role of fluid mechanics in developing a new mathematical physics. It gives full and clear accounts of the conceptual breakthroughs of physicists and engineers who tried to meet challenges in the practical worlds of hydraulics, navigation, blood circulation, meteorology, and aeronautics, and it shows how hydrodynamics at last began to fulfill its early promise to unify the different worlds of flow. Richly illustrated, technically thorough, and sensitive to cross-cultural effects, this history should attract a broad range of historians, scientists, engineers, and philosophers and be a standard reference for anyone interested in fluid mechanics.",
		"ISBN": "978-0-19-856843-8",
		"language": "en",
		"note": "Google-Books-ID: tIRBzdTWwyUC",
		"number-of-pages": "372",
		"publisher": "OUP Oxford",
		"source": "Google Books",
		"title": "Worlds of Flow: A History of Hydrodynamics from the Bernoullis to Prandtl",
		"title-short": "Worlds of Flow",
		"author": [
			{
				"family": "Darrigol",
				"given": "Olivier"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2005",
					9
				]
			]
		}
	},
	{
		"id": "rouse1983",
		"type": "book",
		"abstract": "by Hunter Rouse From Books at Iowa 38 (April 1983) Copyright: University of Iowa UI Special Collections [See also Rare Book Collection] If the word hydraulics is understood to mean the use of water…",
		"language": "en-US",
		"title": "History of Hydraulics",
		"title-short": "Highlights",
		"URL": "https://www.iihr.uiowa.edu/about/iihr-archives/rare-book-collection/highlights-history-of-hydraulics/",
		"author": [
			{
				"family": "Rouse",
				"given": "Hunter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1983"
				]
			]
		}
	},
	{
		"id": "lec:feynman-2-41",
		"type": "chapter",
		"container-title": "The Feynman Lectures on Physics",
		"number-of-volumes": "3",
		"title": "Lecture 41: The Flow of Wet Water",
		"URL": "https://www.feynmanlectures.caltech.edu/II_41.html",
		"volume": "2",
		"author": [
			{
				"family": "Feynman",
				"given": "Richard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1963"
				]
			]
		}
	},
	{
		"id": "lec:feynman-1-3",
		"type": "chapter",
		"container-title": "The Feynman Lectures on Physics",
		"number-of-volumes": "3",
		"title": "Lecture 3: The Relation of Physics to Other Sciences",
		"URL": "https://www.feynmanlectures.caltech.edu/I_03.html",
		"volume": "1",
		"author": [
			{
				"family": "Feynman",
				"given": "Richard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1963"
				]
			]
		}
	},
	{
		"id": "keys2011",
		"type": "article-journal",
		"abstract": "For several atomistic models of glass formers, at conditions below their glassy-dynamics–onset temperatures, To, we use importance sampling of trajectory space to study the structure, statistics, and dynamics of excitations responsible for structural relaxation. Excitations are detected in terms of persistent particle displacements of length a. At supercooled conditions, for a of the order of or smaller than a particle diameter, we find that excitations are associated with correlated particle motions that are sparse and localized, occupying a volume with an average radius that is temperature-independent and no larger than a few particle diameters. We show that the statistics and dynamics of these excitations are facilitated and hierarchical. Excitation-energy scales grow logarithmically with a. Excitations at one point in space facilitate the birth and death of excitations at neighboring locations, and space-time excitation structures are microcosms of heterogeneous dynamics at larger scales. This nature of dynamics becomes increasingly dominant as temperature T is lowered. We show that slowing of dynamics upon decreasing temperature below To is the result of a decreasing concentration of excitations and concomitantly growing length scales for dynamical correlations that develop in a hierarchical manner, and further that the structural-relaxation time τ follows the parabolic law, log (τ/τo)=J2(1/T−1/To)2, for T<To, where J, τo and To can be predicted quantitatively from dynamics at short time scales. Particle motion is facilitated and directional, and we show that this becomes more apparent with decreasing T. We show that stringlike motion is a natural consequence of facilitated, hierarchical dynamics.",
		"container-title": "Physical Review X",
		"DOI": "10.1103/PhysRevX.1.021013",
		"issue": "2",
		"journalAbbreviation": "Phys. Rev. X",
		"note": "publisher: American Physical Society",
		"page": "021013",
		"source": "APS",
		"title": "Excitations Are Localized and Relaxation Is Hierarchical in Glass-Forming Liquids",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevX.1.021013",
		"volume": "1",
		"author": [
			{
				"family": "Keys",
				"given": "Aaron S."
			},
			{
				"family": "Hedges",
				"given": "Lester O."
			},
			{
				"family": "Garrahan",
				"given": "Juan P."
			},
			{
				"family": "Glotzer",
				"given": "Sharon C."
			},
			{
				"family": "Chandler",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011",
					11,
					30
				]
			]
		}
	},
	{
		"id": "sokol2018",
		"type": "article-journal",
		"abstract": "By squeezing fluids into flat sheets, researchers can get a handle on the strange ways that turbulence feeds energy into a system instead of eating it away.",
		"container-title": "Quanta Magazine",
		"language": "en",
		"title": "Mathematicians Tame Turbulence in Flattened Fluids",
		"URL": "https://www.quantamagazine.org/mathematicians-tame-turbulence-in-flattened-fluids-20180627/",
		"author": [
			{
				"family": "Sokol",
				"given": "Joshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "img:jupiter",
		"type": "webpage",
		"container-title": "NASA/Goddard Space Flight Center Scientific Visualization Studio",
		"title": "SVS: Jupiter Cloud Sequence from Cassini",
		"URL": "https://svs.gsfc.nasa.gov/vis/a000000/a003600/a003610/index.html",
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009"
				]
			]
		}
	},
	{
		"id": "grucza2007",
		"type": "article-journal",
		"abstract": "In science, multiple measures of the same constructs can be useful, but they are unlikely to all be equally valid indicators. In psychological assessment, the many popular personality inventories available in the marketplace also may be useful, but their comparative validity has long remained unassessed. This is the first comprehensive comparison of 11 such multiscale instruments against each of three types of criteria: clusters of behavioral acts, descriptions by knowledgeable informants, and clinical indicators potentially associated with various types of psychopathology. Using 1,000 bootstrap resampling analyses from a sample of roughly 700 adult research participants, we assess the relative predictability of each criterion and the comparative validity of each inventory. Although there was a wide range of criterion predictability, most inventories exhibited quite similar cross-validities when averaged across all three types of criteria. On the other hand, there were important differences between inventories in their predictive capabilities for particular criteria. We discuss the factors that lead to differential validity across predictors and criteria.",
		"container-title": "Journal of Personality Assessment",
		"DOI": "10.1080/00223890701468568",
		"ISSN": "00223891",
		"issue": "2",
		"journalAbbreviation": "Journal of Personality Assessment",
		"note": "publisher: Taylor & Francis Ltd",
		"page": "167-187",
		"source": "EBSCOhost",
		"title": "The Comparative Validity of 11 Modern Personality Inventories: Predictions of Behavioral Acts, Informant Reports, and Clinical Indicators",
		"title-short": "The Comparative Validity of 11 Modern Personality Inventories",
		"URL": "https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=26436963&site=ehost-live&scope=site",
		"volume": "89",
		"author": [
			{
				"family": "Grucza",
				"given": "RichardA."
			},
			{
				"family": "Goldberg",
				"given": "LewisR."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					10
				]
			]
		}
	},
	{
		"id": "mershon1988",
		"type": "article-journal",
		"container-title": "Journal of Personality and Social Psychology",
		"issue": "4",
		"page": "675-680",
		"title": "Number of Factors in the Personality Sphere: Does Increase in Factors Increase Predictability of Real-Life Criteria?.",
		"URL": "https://ovidsp-dc2-ovid-com.proxy.uba.uva.nl:2443/ovid-b/ovidweb.cgi?QS2=434f4e1a73d37e8c54ea12160ae198c5018b0ff934c2a91005bd5286cf69bda4c548086b1a957fc9547a6c84d53fe433bb16463cc31980d67ff618bf7ec7f51013319519c22b9f646dc4a1e8ec1c901875c445c96e41e4bfa509486bbe03951953842d893735319b775cdd89622e8a24aefeaff744e146f2b28089eea10aa1e082e239eeb611e2833e48e06218317d48246333ece2acd7c8a90e524bc6ccc115a9f37fa01e418edd099e599efd291ced7146e4685b138dc59ed6362319011727b562616c203bfb1a555418524617b1058f0ca24cf30c0cc3bdc561c94ca19aca7473367fce221143b5a6b06da9f2e8647ced62c535664dacb944d458a721f5c1c1d3c0838072aeb3",
		"volume": "55",
		"author": [
			{
				"family": "Mershon",
				"given": "Bryan"
			},
			{
				"family": "Gorsuch",
				"given": "Richard L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1988"
				]
			]
		}
	},
	{
		"id": "watts1998",
		"type": "article-journal",
		"abstract": "Networks of coupled dynamical systems have been used to model biological oscillators1,2,3,4, Josephson junction arrays5,6, excitable media7, neural networks8,9,10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks ‘rewired’ to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them ‘small-world’ networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices.",
		"container-title": "Nature",
		"DOI": "10.1038/30918",
		"ISSN": "1476-4687",
		"issue": "6684",
		"language": "en",
		"license": "1998 Macmillan Magazines Ltd.",
		"note": "number: 6684\npublisher: Nature Publishing Group",
		"page": "440-442",
		"source": "www-nature-com.proxy.uba.uva.nl:2443",
		"title": "Collective dynamics of ‘small-world’ networks",
		"URL": "http://www.nature.com/articles/30918",
		"volume": "393",
		"author": [
			{
				"family": "Watts",
				"given": "Duncan J."
			},
			{
				"family": "Strogatz",
				"given": "Steven H."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1998",
					6
				]
			]
		}
	},
	{
		"id": "humboldt1845",
		"type": "book",
		"event-place": "Balitmore",
		"ISBN": "978-0-8018-5502-3",
		"language": "eng",
		"number-of-pages": "v. <1,2   > ;",
		"publisher": "Johns Hopkins Press",
		"publisher-place": "Balitmore",
		"source": "The Open Library",
		"title": "Cosmos: a sketch of a physical description of the universe",
		"title-short": "Cosmos",
		"author": [
			{
				"family": "Humboldt",
				"given": "Alexander",
				"dropping-particle": "von"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1997"
				]
			]
		}
	},
	{
		"id": "def:mw-system",
		"type": "webpage",
		"abstract": "a regularly interacting or interdependent group of items forming a unified whole : such as; a group of interacting bodies under the influence of related forces; an assemblage of substances that is in or tends to equilibrium… See the full definition",
		"container-title": "Merriam-Webster",
		"language": "en",
		"title": "Definition of SYSTEM",
		"URL": "https://www.merriam-webster.com/dictionary/system",
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					28
				]
			]
		}
	},
	{
		"id": "strogatz20011",
		"type": "book",
		"edition": "1",
		"publisher": "Westview Press",
		"title": "Nonlinear Dynamics and Chaos",
		"URL": "https://b-ok.cc/book/550919/343812",
		"author": [
			{
				"family": "Strogatz",
				"given": "Steven H."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					29
				]
			]
		},
		"issued": {
			"literal": "20011"
		}
	},
	{
		"id": "brush1968",
		"type": "article-journal",
		"container-title": "Archive for history of exact sciences",
		"issue": "1",
		"page": "1-36",
		"title": "A History of Random Processes: I. Brownian Movement from Brown to Perrin",
		"URL": "https://link-springer-com.proxy.uba.uva.nl:2443/content/pdf/10.1007/BF00328110.pdf",
		"volume": "5",
		"author": [
			{
				"family": "Brush",
				"given": "Stephen G."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1968"
				]
			]
		}
	},
	{
		"id": "boltzmann1896",
		"type": "book",
		"abstract": "Book digitized by Google from the library of the New York Public Library and uploaded to the Internet Archive by user tpb.; 1. Th. Theorie des Gase mit einatomigen Molekülen, deren dimensionen gegen die Mittlere weglänge verschwinden.--2. Th. Theorie van der Waals'; Gase mit zusammengesetzten Molekülen; Gasdissociation; Schlussbemerkungen",
		"language": "ger",
		"number-of-pages": "230",
		"publisher": "Leipzig, J. A. Barth",
		"source": "Internet Archive",
		"title": "Vorlesungen über Gastheorie",
		"URL": "http://archive.org/details/vorlesungenberg01boltgoog",
		"author": [
			{
				"family": "Boltzmann",
				"given": "Ludwig"
			}
		],
		"contributor": [
			{
				"literal": "New York Public Library"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1896"
				]
			]
		}
	},
	{
		"id": "pluckrose2020",
		"type": "book",
		"abstract": "Have you heard that language is violence and that science is sexist? Have you read that certain people shouldn’t practice yoga or cook Ch...",
		"event-place": "Durham, NC",
		"publisher": "Pithstone Publishing",
		"publisher-place": "Durham, NC",
		"title": "Cynical Theories",
		"URL": "https://www.goodreads.com/book/show/53052177-cynical-theories",
		"author": [
			{
				"family": "Pluckrose",
				"given": "Helen"
			},
			{
				"family": "Lindsay",
				"given": "James"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lindsay2020",
		"type": "post-weblog",
		"abstract": "Over the last few years, it has become apparent that, for whatever nobility and moral worth lies in the project called “social justice,” something has gone badly wrong with the ideological movement on the far left that repeatedly calls for—or, more accurately, demands—it.",
		"container-title": "New Discourses",
		"language": "en-US",
		"title": "Naming the Enemy: Critical Social Justice",
		"title-short": "Naming the Enemy",
		"URL": "https://newdiscourses.com/2020/02/naming-enemy-critical-social-justice/",
		"author": [
			{
				"family": "Lindsay",
				"given": "James"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					2,
					28
				]
			]
		}
	},
	{
		"id": "lounsbury2003",
		"type": "article-journal",
		"abstract": "General intelligence, Big Five personality constructs, and a measure of work drive were studied in relation to course grade in an undergraduate psychology course taught by the same professor for 175 students over a 5-year period. Using a hierarchical multiple regression analysis, general intelligence accounted significantly for 16% of the variance in course grade; Big Five personality measures accounted significantly for an additional 7% of the variance; and work drive accounted significantly for an additional 4% of the variance. However, when work drive was entered before the Big Five variables, the Big Five variables did not add significantly (either as a set or individually) to the prediction of course grade. Results were discussed in terms of the importance of personality constructs in uniquely predicting academic performance and the need for additional study using more diverse predictors and aggregated criterion measures.",
		"container-title": "Personality and Individual Differences",
		"DOI": "10.1016/S0191-8869(02)00330-6",
		"ISSN": "0191-8869",
		"issue": "6",
		"journalAbbreviation": "Personality and Individual Differences",
		"language": "en",
		"page": "1231-1239",
		"source": "ScienceDirect",
		"title": "Intelligence, “Big Five” personality traits, and work drive as predictors of course grade",
		"URL": "http://www.sciencedirect.com/science/article/pii/S0191886902003306",
		"volume": "35",
		"author": [
			{
				"family": "Lounsbury",
				"given": "John W"
			},
			{
				"family": "Sundstrom",
				"given": "Eric"
			},
			{
				"family": "Loveland",
				"given": "James M"
			},
			{
				"family": "Gibson",
				"given": "Lucy W"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					8,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2003",
					10,
					1
				]
			]
		}
	},
	{
		"id": "brown1828",
		"type": "article-journal",
		"container-title": "Edinburgh Journal of Science",
		"page": "358-371",
		"title": "A brief account of microscopical observations made in the months of June, July, and August, 1827, on the particles contained in the pollen of plants; and on the general existence of active molecules in organic and inorganic bodies.",
		"volume": "5",
		"author": [
			{
				"family": "Brown",
				"given": "Robert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1828"
				]
			]
		}
	},
	{
		"id": "poincare1904",
		"type": "paper-conference",
		"event-place": "St. Louis",
		"event-title": "Congress of Arts and Sciences, Universal Exposition",
		"page": "604-622",
		"publisher-place": "St. Louis",
		"title": "The principles of mathematical physics",
		"volume": "1",
		"author": [
			{
				"family": "Poincaré",
				"given": "Jules Henri"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1904"
				]
			]
		}
	},
	{
		"id": "born",
		"type": "article-magazine",
		"container-title": "Schlipp",
		"page": "161-177",
		"title": "Einstein's statistical theories",
		"author": [
			{
				"family": "Born",
				"given": "Max"
			}
		]
	},
	{
		"id": "einsteinalbert",
		"type": "chapter",
		"collection-number": "Library of Living Philosophers",
		"container-title": "Albert Einstein: Philosopher-Scientist",
		"page": "1-95",
		"publisher": "Tudor Pub. Co.",
		"title": "Autobiographical notes",
		"author": [
			{
				"family": "Einstein, Albert",
				"given": ""
			}
		],
		"editor": [
			{
				"family": "Schlipp",
				"given": "P.A."
			}
		]
	},
	{
		"id": "engelken2020",
		"type": "article-journal",
		"abstract": "Brains process information through the collective dynamics of large neural networks. Collective chaos was suggested to underlie the complex ongoing dynamics observed in cerebral cortical circuits and determine the impact and processing of incoming information streams. In dissipative systems, chaotic dynamics takes place on a subset of phase space of reduced dimensionality and is organized by a complex tangle of stable, neutral and unstable manifolds. Key topological invariants of this phase space structure such as attractor dimension, and Kolmogorov-Sinai entropy so far remained elusive. Here we calculate the complete Lyapunov spectrum of recurrent neural networks. We show that chaos in these networks is extensive with a size-invariant Lyapunov spectrum and characterized by attractor dimensions much smaller than the number of phase space dimensions. We find that near the onset of chaos, for very intense chaos, and discrete-time dynamics, random matrix theory provides analytical approximations to the full Lyapunov spectrum. We show that a generalized time-reversal symmetry of the network dynamics induces a point-symmetry of the Lyapunov spectrum reminiscent of the symplectic structure of chaotic Hamiltonian systems. Fluctuating input reduces both the entropy rate and the attractor dimension. For trained recurrent networks, we find that Lyapunov spectrum analysis provides a quantification of error propagation and stability achieved. Our methods apply to systems of arbitrary connectivity, and we describe a comprehensive set of controls for the accuracy and convergence of Lyapunov exponents. Our results open a novel avenue for characterizing the complex dynamics of recurrent neural networks and the geometry of the corresponding chaotic attractors. They also highlight the potential of Lyapunov spectrum analysis as a diagnostic for machine learning applications of recurrent networks.",
		"container-title": "arXiv:2006.02427 [nlin, q-bio]",
		"language": "en",
		"note": "arXiv: 2006.02427",
		"source": "arXiv.org",
		"title": "Lyapunov spectra of chaotic recurrent neural networks",
		"URL": "http://arxiv.org/abs/2006.02427",
		"author": [
			{
				"family": "Engelken",
				"given": "Rainer"
			},
			{
				"family": "Wolf",
				"given": "Fred"
			},
			{
				"family": "Abbott",
				"given": "L. F."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					6,
					3
				]
			]
		}
	},
	{
		"id": "cavagna2020",
		"type": "article-journal",
		"abstract": "Collective behavior, both in real biological systems as well as in theoretical models, often displays a rich combination of different kinds of order. A clear-cut and unique definition of \"phase\" based on the standard concept of order parameter may therefore be complicated, and made even trickier by the lack of thermodynamic equilibrium. Compression-based entropies have been proved useful in recent years in describing the different phases of out-of-equilibrium systems. Here, we investigate the performance of a compression-based entropy, namely the Computable Information Density (CID), within the Vicsek model of collective motion. Our entropy is defined through a crude coarse-graining of the particle positions, in which the key role of velocities in the model only enters indirectly through the velocity-density coupling. We discover that such entropy is a valid tool in distinguishing the various noise regimes, including the crossover between an aligned and misaligned phase of the velocities, despite the fact that velocities are not used by this entropy. Furthermore, we unveil the subtle role of the time coordinate, unexplored in previous studies on the CID: a new encoding recipe, where space and time locality are both preserved on the same ground, is demonstrated to reduce the CID. Such an improvement is particularly significant when working with partial and/or corrupted data, as it is often the case in real biological experiments.",
		"container-title": "arXiv:2007.11322 [cond-mat, physics:physics, q-bio]",
		"note": "arXiv: 2007.11322",
		"source": "arXiv.org",
		"title": "Vicsek Model by Time-Interlaced Compression: a Dynamical Computable Information Density",
		"title-short": "Vicsek Model by Time-Interlaced Compression",
		"URL": "http://arxiv.org/abs/2007.11322",
		"author": [
			{
				"family": "Cavagna",
				"given": "Andrea"
			},
			{
				"family": "Chaikin",
				"given": "Paul M."
			},
			{
				"family": "Levine",
				"given": "Dov"
			},
			{
				"family": "Martiniani",
				"given": "Stefano"
			},
			{
				"family": "Puglisi",
				"given": "Andrea"
			},
			{
				"family": "Viale",
				"given": "Massimiliano"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					22
				]
			]
		}
	},
	{
		"id": "nicoletti2020",
		"type": "article-journal",
		"abstract": "We present a systematic study to test a recently introduced phenomenological renormalization group, proposed to coarse-grain data of neural activity from their correlation matrix. The approach allows, at least in principle, to establish whether the collective behavior of the network of spiking neurons is described by a non-Gaussian critical fixed point. We test this renormalization procedure in a variety of models focusing in particular on the contact process, which displays an absorbing phase transition at λ=λc between a silent and an active state. We find that the results of the coarse graining do not depend on the presence of long-range interactions and, overall, the method proves to be able to distinguish the critical regime from the supercritical one. However, some scaling features persist in the supercritical regime, at least for a finite system, as we see in a contact process above λc. Our results provide both a systematic test of the method and insights on the possible subtleties that one needs to consider when applying such phenomenological approaches directly to data to infer signatures of criticality.",
		"container-title": "Physical Review Research",
		"DOI": "10.1103/PhysRevResearch.2.023144",
		"issue": "2",
		"journalAbbreviation": "Phys. Rev. Research",
		"note": "publisher: American Physical Society",
		"page": "023144",
		"source": "APS",
		"title": "Scaling and criticality in a phenomenological renormalization group",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevResearch.2.023144",
		"volume": "2",
		"author": [
			{
				"family": "Nicoletti",
				"given": "Giorgio"
			},
			{
				"family": "Suweis",
				"given": "Samir"
			},
			{
				"family": "Maritan",
				"given": "Amos"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					5,
					8
				]
			]
		}
	},
	{
		"id": "gfycat",
		"type": "webpage",
		"abstract": "Watch and share Common Starling Bird Call And Nest - Singing Birds GIFs on Gfycat",
		"container-title": "Gfycat",
		"language": "en",
		"title": "Common Starling Bird Call And Nest - Singing Birds GIF",
		"URL": "https://thumbs.gfycat.com/UncomfortablePoorBobolink-size_restricted.gif",
		"author": [
			{
				"family": "Gfycat",
				"given": ""
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					4
				]
			]
		}
	},
	{
		"id": "huge2020",
		"type": "article-journal",
		"abstract": "Read this arXiv paper as a responsive web page with clickable citations.",
		"language": "en",
		"source": "www.arxiv-vanity.com",
		"title": "Differential Machine Learning",
		"URL": "https://www.arxiv-vanity.com/papers/2005.02347/",
		"author": [
			{
				"family": "Huge",
				"given": "Brian"
			},
			{
				"family": "Savine",
				"given": "Antoine"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					4
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					5,
					5
				]
			]
		}
	},
	{
		"id": "einstein1921",
		"type": "speech",
		"abstract": "Einstein: Geometry and Experience",
		"event-place": "Berlin",
		"event-title": "Prussian Academy of Sciences",
		"language": "en",
		"publisher-place": "Berlin",
		"title": "Geometry and Experience",
		"title-short": "Einstein",
		"URL": "https://mathshistory.st-andrews.ac.uk/Extras/Einstein_geometry/",
		"author": [
			{
				"family": "Einstein",
				"given": "Albert"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					4
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1921"
				]
			]
		}
	},
	{
		"id": "img:kinetic-theory",
		"type": "webpage",
		"title": "Translational_motion.gif (300×263)",
		"URL": "https://upload.wikimedia.org/wikipedia/commons/6/6d/Translational_motion.gif",
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					5
				]
			]
		}
	},
	{
		"id": "kuhn1970",
		"type": "book",
		"call-number": "Q175 .K95 1970",
		"collection-title": "International encyclopedia of unified science. Foundations of the unity of science, v. 2, no. 2",
		"edition": "[2d ed., enl",
		"event-place": "Chicago",
		"ISBN": "978-0-226-45803-8",
		"language": "en",
		"number-of-pages": "210",
		"publisher": "University of Chicago Press",
		"publisher-place": "Chicago",
		"source": "Library of Congress ISBN",
		"title": "The structure of scientific revolutions",
		"author": [
			{
				"family": "Kuhn",
				"given": "Thomas S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"1970"
				]
			]
		}
	},
	{
		"id": "smolin2006",
		"type": "book",
		"title": "The Trouble with Physics: The Rise of String Theory, the Fall of a Science and What Comes Next by Lee Smolin",
		"URL": "https://www.goodreads.com/book/show/108939.The_Trouble_with_Physics",
		"author": [
			{
				"family": "Smolin",
				"given": "Lee"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2006"
				]
			]
		}
	},
	{
		"id": "genthon2020",
		"type": "article-journal",
		"abstract": "Interest in Brownian motion was shared by different communities: this phenomenon was first observed by the botanist Robert Brown in 1827, then theorised by physicists in the 1900s, and eventually modelled by mathematicians from the 1920s, while still evolving as a physical theory. Consequently, Brownian motion now refers to the natural phenomenon but also to the theories accounting for it. There is no published work telling its entire history from its discovery until today, but rather partial histories either from 1827 to Perrin’s experiments in the late 1900s, from a physicist’s point of view; or from the 1920s from a mathematician’s point of view. In this article, we tackle the period straddling the two ‘half-histories’ just mentioned, in order to highlight continuity, to investigate the domain-shift from physics to mathematics, and to survey the enhancements of later physical theories. We study the works of Einstein, Smoluchowski, Langevin, Wiener, Ornstein and Uhlenbeck from 1905 to 1934 as well as experimental results, using the concept of Brownian velocity as a leading thread. We show how Brownian motion became a research topic for the mathematician Wiener in the 1920s, why his model was an idealization of physical experiments, what Ornstein and Uhlenbeck added to Einstein’s results, and how Wiener, Ornstein and Uhlenbeck developed in parallel contradictory theories concerning Brownian velocity.",
		"container-title": "The European Physical Journal H",
		"DOI": "10.1140/epjh/e2020-10009-8",
		"ISSN": "2102-6467",
		"issue": "1",
		"journalAbbreviation": "EPJ H",
		"language": "en",
		"page": "49-105",
		"source": "Springer Link",
		"title": "The concept of velocity in the history of Brownian motion",
		"URL": "https://doi.org/10.1140/epjh/e2020-10009-8",
		"volume": "45",
		"author": [
			{
				"family": "Genthon",
				"given": "Arthur"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					1
				]
			]
		}
	},
	{
		"id": "sompolinsky1988",
		"type": "article-journal",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.61.259",
		"ISSN": "0031-9007",
		"issue": "3",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"language": "en",
		"page": "259-262",
		"source": "DOI.org (Crossref)",
		"title": "Chaos in Random Neural Networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevLett.61.259",
		"volume": "61",
		"author": [
			{
				"family": "Sompolinsky",
				"given": "H."
			},
			{
				"family": "Crisanti",
				"given": "A."
			},
			{
				"family": "Sommers",
				"given": "H. J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					10
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1988",
					7,
					18
				]
			]
		}
	},
	{
		"id": "vuijk2016",
		"type": "thesis",
		"event-place": "Amsterdam",
		"genre": "Masters Thesis",
		"publisher": "VU",
		"publisher-place": "Amsterdam",
		"title": "Time, Balance, and Criticality in Random Neural Networks",
		"author": [
			{
				"family": "Vuijk",
				"given": "Hidde"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "cardy1996",
		"type": "book",
		"collection-number": "5",
		"collection-title": "Cambridge lecture notes in physics",
		"publisher": "Cambridge University Press",
		"title": "Scaling and renormalization in statistical physics",
		"author": [
			{
				"family": "Cardy",
				"given": "John"
			}
		],
		"editor": [
			{
				"family": "Goddard",
				"given": "P."
			},
			{
				"family": "Yeomans",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"1996"
				]
			]
		}
	},
	{
		"id": "copernicus1543",
		"type": "book",
		"event-place": "Nuremberg",
		"language": "Latin",
		"number-of-pages": "405",
		"publisher": "Johannes Petreius",
		"publisher-place": "Nuremberg",
		"title": "De revolutionibus orbium coelestium",
		"author": [
			{
				"family": "Copernicus",
				"given": "Nicolaus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1543"
				]
			]
		}
	},
	{
		"id": "koch-janusz2018",
		"type": "article-journal",
		"abstract": "Physical systems differing in their microscopic details often display strikingly similar behaviour when probed at macroscopic scales. Those universal properties, largely determining their physical characteristics, are revealed by the powerful renormalization group (RG) procedure, which systematically retains ‘slow’ degrees of freedom and integrates out the rest. However, the important degrees of freedom may be difficult to identify. Here we demonstrate a machine-learning algorithm capable of identifying the relevant degrees of freedom and executing RG steps iteratively without any prior knowledge about the system. We introduce an artificial neural network based on a model-independent, information-theoretic characterization of a real-space RG procedure, which performs this task. We apply the algorithm to classical statistical physics problems in one and two dimensions. We demonstrate RG flow and extract the Ising critical exponent. Our results demonstrate that machine-learning techniques can extract abstract physical concepts and consequently become an integral part of theory- and model-building.",
		"container-title": "Nature Physics",
		"DOI": "10.1038/s41567-018-0081-4",
		"ISSN": "1745-2481",
		"issue": "6",
		"language": "en",
		"license": "2018 The Author(s)",
		"note": "number: 6\npublisher: Nature Publishing Group",
		"page": "578-582",
		"source": "www.nature.com",
		"title": "Mutual information, neural networks and the renormalization group",
		"URL": "https://www.nature.com/articles/s41567-018-0081-4",
		"volume": "14",
		"author": [
			{
				"family": "Koch-Janusz",
				"given": "Maciej"
			},
			{
				"family": "Ringel",
				"given": "Zohar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6
				]
			]
		}
	},
	{
		"id": "wolf1985",
		"type": "article-journal",
		"container-title": "Physica D: Nonlinear Phenomena",
		"DOI": "10.1016/0167-2789(85)90011-9",
		"ISSN": "01672789",
		"issue": "3",
		"journalAbbreviation": "Physica D: Nonlinear Phenomena",
		"language": "en",
		"page": "285-317",
		"source": "DOI.org (Crossref)",
		"title": "Determining Lyapunov exponents from a time series",
		"URL": "https://linkinghub.elsevier.com/retrieve/pii/0167278985900119",
		"volume": "16",
		"author": [
			{
				"family": "Wolf",
				"given": "Alan"
			},
			{
				"family": "Swift",
				"given": "Jack B."
			},
			{
				"family": "Swinney",
				"given": "Harry L."
			},
			{
				"family": "Vastano",
				"given": "John A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					15
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1985",
					7
				]
			]
		}
	},
	{
		"id": "shwartz-ziv2017",
		"type": "article-journal",
		"abstract": "Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \\textit{Information Plane}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on {\\emph compression} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.",
		"container-title": "arXiv:1703.00810 [cs]",
		"note": "arXiv: 1703.00810",
		"source": "arXiv.org",
		"title": "Opening the Black Box of Deep Neural Networks via Information",
		"URL": "http://arxiv.org/abs/1703.00810",
		"author": [
			{
				"family": "Shwartz-Ziv",
				"given": "Ravid"
			},
			{
				"family": "Tishby",
				"given": "Naftali"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					4,
					29
				]
			]
		}
	},
	{
		"id": "olah2014",
		"type": "webpage",
		"container-title": "colah's blog",
		"title": "Neural Networks, Manifolds, and Topology",
		"URL": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/",
		"author": [
			{
				"family": "Olah",
				"given": "Christopher"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "zhang2017",
		"type": "article-journal",
		"abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
		"container-title": "arXiv:1611.03530 [cs]",
		"note": "arXiv: 1611.03530",
		"source": "arXiv.org",
		"title": "Understanding deep learning requires rethinking generalization",
		"URL": "http://arxiv.org/abs/1611.03530",
		"author": [
			{
				"family": "Zhang",
				"given": "Chiyuan"
			},
			{
				"family": "Bengio",
				"given": "Samy"
			},
			{
				"family": "Hardt",
				"given": "Moritz"
			},
			{
				"family": "Recht",
				"given": "Benjamin"
			},
			{
				"family": "Vinyals",
				"given": "Oriol"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					2,
					26
				]
			]
		}
	},
	{
		"id": "neyshabur2017",
		"type": "chapter",
		"container-title": "Advances in Neural Information Processing Systems 30",
		"page": "5947–5956",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "Exploring Generalization in Deep Learning",
		"URL": "http://papers.nips.cc/paper/7176-exploring-generalization-in-deep-learning.pdf",
		"author": [
			{
				"family": "Neyshabur",
				"given": "Behnam"
			},
			{
				"family": "Bhojanapalli",
				"given": "Srinadh"
			},
			{
				"family": "Mcallester",
				"given": "David"
			},
			{
				"family": "Srebro",
				"given": "Nati"
			}
		],
		"editor": [
			{
				"family": "Guyon",
				"given": "I."
			},
			{
				"family": "Luxburg",
				"given": "U. V."
			},
			{
				"family": "Bengio",
				"given": "S."
			},
			{
				"family": "Wallach",
				"given": "H."
			},
			{
				"family": "Fergus",
				"given": "R."
			},
			{
				"family": "Vishwanathan",
				"given": "S."
			},
			{
				"family": "Garnett",
				"given": "R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "smith2018",
		"type": "article-journal",
		"abstract": "We consider two questions at the heart of machine learning; how can we predict if a minimum will generalize to the test set, and why does stochastic gradient descent find minima that generalize well? Our work responds to Zhang et al. (2016), who showed deep neural networks can easily memorize randomly labeled training data, despite generalizing well on real labels of the same inputs. We show that the same phenomenon occurs in small linear models. These observations are explained by the Bayesian evidence, which penalizes sharp minima but is invariant to model parameterization. We also demonstrate that, when one holds the learning rate fixed, there is an optimum batch size which maximizes the test set accuracy. We propose that the noise introduced by small mini-batches drives the parameters towards minima whose evidence is large. Interpreting stochastic gradient descent as a stochastic differential equation, we identify the \"noise scale\" $g = \\epsilon (\\frac{N}{B} - 1) \\approx \\epsilon N/B$, where $\\epsilon$ is the learning rate, $N$ the training set size and $B$ the batch size. Consequently the optimum batch size is proportional to both the learning rate and the size of the training set, $B_{opt} \\propto \\epsilon N$. We verify these predictions empirically.",
		"container-title": "arXiv:1710.06451 [cs, stat]",
		"note": "arXiv: 1710.06451",
		"source": "arXiv.org",
		"title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent",
		"URL": "http://arxiv.org/abs/1710.06451",
		"author": [
			{
				"family": "Smith",
				"given": "Samuel L."
			},
			{
				"family": "Le",
				"given": "Quoc V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					14
				]
			]
		}
	},
	{
		"id": "kawaguchi2020",
		"type": "article-journal",
		"abstract": "This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.",
		"container-title": "arXiv:1710.05468 [cs, stat]",
		"note": "arXiv: 1710.05468",
		"source": "arXiv.org",
		"title": "Generalization in Deep Learning",
		"URL": "http://arxiv.org/abs/1710.05468",
		"author": [
			{
				"family": "Kawaguchi",
				"given": "Kenji"
			},
			{
				"family": "Kaelbling",
				"given": "Leslie Pack"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					27
				]
			]
		}
	},
	{
		"id": "lu2017",
		"type": "article-journal",
		"abstract": "The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-$2$) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-$(n+4)$ ReLU networks, where $n$ is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-$n$ ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth is more effective than width for the expressiveness of ReLU networks.",
		"container-title": "arXiv:1709.02540 [cs]",
		"note": "arXiv: 1709.02540",
		"source": "arXiv.org",
		"title": "The Expressive Power of Neural Networks: A View from the Width",
		"title-short": "The Expressive Power of Neural Networks",
		"URL": "http://arxiv.org/abs/1709.02540",
		"author": [
			{
				"family": "Lu",
				"given": "Zhou"
			},
			{
				"family": "Pu",
				"given": "Hongming"
			},
			{
				"family": "Wang",
				"given": "Feicheng"
			},
			{
				"family": "Hu",
				"given": "Zhiqiang"
			},
			{
				"family": "Wang",
				"given": "Liwei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					11,
					1
				]
			]
		}
	},
	{
		"id": "lin2017",
		"type": "article-journal",
		"abstract": "We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through \"cheap learning\" with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various \"no-flattening theorems\" showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss, for example, we show that $n$ variables cannot be multiplied using fewer than 2^n neurons in a single hidden layer.",
		"container-title": "Journal of Statistical Physics",
		"DOI": "10.1007/s10955-017-1836-5",
		"ISSN": "0022-4715, 1572-9613",
		"issue": "6",
		"journalAbbreviation": "J Stat Phys",
		"note": "arXiv: 1608.08225",
		"page": "1223-1247",
		"source": "arXiv.org",
		"title": "Why does deep and cheap learning work so well?",
		"URL": "http://arxiv.org/abs/1608.08225",
		"volume": "168",
		"author": [
			{
				"family": "Lin",
				"given": "Henry W."
			},
			{
				"family": "Tegmark",
				"given": "Max"
			},
			{
				"family": "Rolnick",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					9
				]
			]
		}
	},
	{
		"id": "wainrib2013",
		"type": "article-journal",
		"abstract": "Random neural networks are dynamical descriptions of randomly interconnected neural units. These show a phase transition to chaos as a disorder parameter is increased. The microscopic mechanisms underlying this phase transition are unknown, and similarly to spin-glasses, shall be fundamentally related to the behavior of the system. In this Letter we investigate the explosion of complexity arising near that phase transition. We show that the mean number of equilibria undergoes a sharp transition from one equilibrium to a very large number scaling exponentially with the dimension on the system. Near criticality, we compute the exponential rate of divergence, called topological complexity. Strikingly, we show that it behaves exactly as the maximal Lyapunov exponent, a classical measure of dynamical complexity. This relationship unravels a microscopic mechanism leading to chaos which we further demonstrate on a simpler class of disordered systems, suggesting a deep and underexplored link between topological and dynamical complexity.",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.110.118101",
		"ISSN": "0031-9007, 1079-7114",
		"issue": "11",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"note": "arXiv: 1210.5082",
		"page": "118101",
		"source": "arXiv.org",
		"title": "Topological and Dynamical Complexity of Random Neural Networks",
		"URL": "http://arxiv.org/abs/1210.5082",
		"volume": "110",
		"author": [
			{
				"family": "Wainrib",
				"given": "Gilles"
			},
			{
				"family": "Touboul",
				"given": "Jonathan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013",
					3,
					11
				]
			]
		}
	},
	{
		"id": "lec:feynman-1-1",
		"type": "chapter",
		"container-title": "The Feynman Lectures on Physics",
		"number-of-volumes": "3",
		"title": "Lecture 1: Atoms in Motion",
		"URL": "https://www.feynmanlectures.caltech.edu/I_01.html#Ch1-S1",
		"volume": "1",
		"author": [
			{
				"family": "Feynman",
				"given": "Richard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1963"
				]
			]
		}
	},
	{
		"id": "graham2019",
		"type": "post-weblog",
		"title": "The Bus Ticket Theory of Genius",
		"URL": "http://paulgraham.com/genius.html",
		"author": [
			{
				"family": "Graham",
				"given": "Paul"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					18
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "frey2013",
		"type": "article-journal",
		"abstract": "We examine how susceptible jobs are to computerisation. To assess this, we begin by implementing a novel methodology to estimate the probability of computerisation for 702 detailed occupations, using a Gaussian process classiﬁer. Based on these estimates, we examine expected impacts of future computerisation on US labour market outcomes, with the primary objective of analysing the number of jobs at risk and the relationship between an occupation’s probability of computerisation, wages and educational attainment. According to our estimates, about 47 percent of total US employment is at risk. We further provide evidence that wages and educational attainment exhibit a strong negative relationship with an occupation’s probability of computerisation.",
		"container-title": "Technological Forecasting and Social Change",
		"DOI": "10.1016/j.techfore.2016.08.019",
		"ISSN": "00401625",
		"journalAbbreviation": "Technological Forecasting and Social Change",
		"language": "en",
		"page": "254-280",
		"source": "DOI.org (Crossref)",
		"title": "The future of employment: How susceptible are jobs to computerisation?",
		"title-short": "The future of employment",
		"URL": "https://linkinghub.elsevier.com/retrieve/pii/S0040162516302244",
		"volume": "114",
		"author": [
			{
				"family": "Frey",
				"given": "Carl Benedikt"
			},
			{
				"family": "Osborne",
				"given": "Michael A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013",
					1
				]
			]
		}
	},
	{
		"id": "bostrom2014",
		"type": "book",
		"abstract": "Superintelligence asks the questions: what happens when machines surpass humans in general intelligence? Will artificial agents save or d...",
		"event-place": "USA",
		"publisher": "Oxford University Press",
		"publisher-place": "USA",
		"title": "Superintelligence: Paths, Dangers, Strategies",
		"URL": "https://www.goodreads.com/work/best_book/37286000-superintelligence-paths-dangers-strategies",
		"author": [
			{
				"family": "Bostrom",
				"given": "Nick"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "mccarthy1955",
		"type": "paper-conference",
		"event-place": "Hanover, New Hampshire",
		"event-title": "Dartmouth Summer Research Project on Artificial Intelligence",
		"language": "en",
		"page": "13",
		"publisher-place": "Hanover, New Hampshire",
		"source": "Zotero",
		"title": "A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence",
		"author": [
			{
				"family": "McCarthy",
				"given": "J"
			},
			{
				"family": "Minsky",
				"given": "M L"
			},
			{
				"family": "Rochester",
				"given": "N"
			},
			{
				"family": "Corporation",
				"given": "I B M"
			},
			{
				"family": "Shannon",
				"given": "C E"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1955"
				]
			]
		}
	},
	{
		"id": "wiener1961",
		"type": "book",
		"edition": "2",
		"event-place": "Cambridge, Massachusetts",
		"publisher": "M.I.T. Press",
		"publisher-place": "Cambridge, Massachusetts",
		"title": "Cybernetics or Control and Communication in the Animal and the Machine",
		"author": [
			{
				"family": "Wiener",
				"given": "Norbert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1961"
				]
			]
		}
	},
	{
		"id": "achiam2017",
		"type": "article-journal",
		"abstract": "Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $\\epsilon$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the $k$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.",
		"container-title": "arXiv:1703.01732 [cs]",
		"note": "arXiv: 1703.01732",
		"source": "arXiv.org",
		"title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1703.01732",
		"author": [
			{
				"family": "Achiam",
				"given": "Joshua"
			},
			{
				"family": "Sastry",
				"given": "Shankar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					3,
					6
				]
			]
		}
	},
	{
		"id": "goodfellow2014",
		"type": "article-journal",
		"abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
		"container-title": "arXiv:1406.2661 [cs, stat]",
		"note": "arXiv: 1406.2661",
		"source": "arXiv.org",
		"title": "Generative Adversarial Networks",
		"URL": "http://arxiv.org/abs/1406.2661",
		"author": [
			{
				"family": "Goodfellow",
				"given": "Ian J."
			},
			{
				"family": "Pouget-Abadie",
				"given": "Jean"
			},
			{
				"family": "Mirza",
				"given": "Mehdi"
			},
			{
				"family": "Xu",
				"given": "Bing"
			},
			{
				"family": "Warde-Farley",
				"given": "David"
			},
			{
				"family": "Ozair",
				"given": "Sherjil"
			},
			{
				"family": "Courville",
				"given": "Aaron"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					6,
					10
				]
			]
		}
	},
	{
		"id": "benettin1980",
		"type": "article-journal",
		"abstract": "The present paper, together with the previous one (Part 1: Theory, published in this journal) is intended to give an explicit method for computing all Lyapunov Characteristic Exponents of a dynamical system. After the general theory on such exponents developed in the first part, in the present paper the computational method is described (Chapter A) and some numerical examples for mappings on manifolds and for Hamiltonian systems are given (Chapter B).",
		"container-title": "Meccanica",
		"DOI": "10.1007/BF02128237",
		"ISSN": "1572-9648",
		"issue": "1",
		"journalAbbreviation": "Meccanica",
		"language": "en",
		"page": "21-30",
		"source": "Springer Link",
		"title": "Lyapunov Characteristic Exponents for smooth dynamical systems and for hamiltonian systems; A method for computing all of them. Part 2: Numerical application",
		"title-short": "Lyapunov Characteristic Exponents for smooth dynamical systems and for hamiltonian systems; A method for computing all of them. Part 2",
		"URL": "https://doi.org/10.1007/BF02128237",
		"volume": "15",
		"author": [
			{
				"family": "Benettin",
				"given": "Giancarlo"
			},
			{
				"family": "Galgani",
				"given": "Luigi"
			},
			{
				"family": "Giorgilli",
				"given": "Antonio"
			},
			{
				"family": "Strelcyn",
				"given": "Jean-Marie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1980",
					3,
					1
				]
			]
		}
	},
	{
		"id": "holden1979",
		"type": "article-journal",
		"container-title": "Science",
		"DOI": "10.1126/science.377485",
		"ISSN": "0036-8075, 1095-9203",
		"issue": "4397",
		"language": "en",
		"license": "© 1979",
		"note": "publisher: American Association for the Advancement of Science\nsection: News &amp; Comment\nPMID: 377485",
		"page": "1066-1068",
		"source": "science.sciencemag.org",
		"title": "Paul MacLean and the triune brain",
		"URL": "https://science.sciencemag.org/content/204/4397/1066",
		"volume": "204",
		"author": [
			{
				"family": "Holden",
				"given": "C."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1979",
					6,
					8
				]
			]
		}
	},
	{
		"id": "tosches2018",
		"type": "article-journal",
		"abstract": "Evolution of the brain\nJust how related are reptilian and mammalian brains? Tosches et al. used single-cell transcriptomics to study turtle, lizard, mouse, and human brain samples. They assessed how the mammalian six-layered cortex might be derived from the reptilian three-layered cortex. Despite a lack of correspondence between layers, mammalian astrocytes and adult neural stem cells shared evolutionary origins. General classes of interneuron types were represented across the evolutionary span, although subtypes were species-specific. Pieces of the much-folded mammalian hippocampus were represented as adjacent fields in the reptile brains.\nScience, this issue p. 881\nComputations in the mammalian cortex are carried out by glutamatergic and γ-aminobutyric acid–releasing (GABAergic) neurons forming specialized circuits and areas. Here we asked how these neurons and areas evolved in amniotes. We built a gene expression atlas of the pallium of two reptilian species using large-scale single-cell messenger RNA sequencing. The transcriptomic signature of glutamatergic neurons in reptilian cortex suggests that mammalian neocortical layers are made of new cell types generated by diversification of ancestral gene-regulatory programs. By contrast, the diversity of reptilian cortical GABAergic neurons indicates that the interneuron classes known in mammals already existed in the common ancestor of all amniotes.\nTranscriptomics tracks the mix of evolutionary derivation and species-specific elaboration that generates brains from reptiles to mammals.\nTranscriptomics tracks the mix of evolutionary derivation and species-specific elaboration that generates brains from reptiles to mammals.",
		"container-title": "Science",
		"DOI": "10.1126/science.aar4237",
		"ISSN": "0036-8075, 1095-9203",
		"issue": "6391",
		"language": "en",
		"license": "Copyright © 2018 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.",
		"note": "publisher: American Association for the Advancement of Science\nsection: Research Article\nPMID: 29724907",
		"page": "881-888",
		"source": "science.sciencemag.org",
		"title": "Evolution of pallium, hippocampus, and cortical cell types revealed by single-cell transcriptomics in reptiles",
		"URL": "https://science.sciencemag.org/content/360/6391/881",
		"volume": "360",
		"author": [
			{
				"family": "Tosches",
				"given": "Maria Antonietta"
			},
			{
				"family": "Yamawaki",
				"given": "Tracy M."
			},
			{
				"family": "Naumann",
				"given": "Robert K."
			},
			{
				"family": "Jacobi",
				"given": "Ariel A."
			},
			{
				"family": "Tushev",
				"given": "Georgi"
			},
			{
				"family": "Laurent",
				"given": "Gilles"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					25
				]
			]
		}
	},
	{
		"id": "casey2011",
		"type": "article-journal",
		"container-title": "Proceedings of the National Academy of Sciences",
		"DOI": "10.1073/pnas.1108561108",
		"ISSN": "0027-8424, 1091-6490",
		"issue": "36",
		"journalAbbreviation": "Proceedings of the National Academy of Sciences",
		"language": "en",
		"page": "14998-15003",
		"source": "DOI.org (Crossref)",
		"title": "Behavioral and neural correlates of delay of gratification 40 years later",
		"URL": "http://www.pnas.org/cgi/doi/10.1073/pnas.1108561108",
		"volume": "108",
		"author": [
			{
				"family": "Casey",
				"given": "B. J."
			},
			{
				"family": "Somerville",
				"given": "L. H."
			},
			{
				"family": "Gotlib",
				"given": "I. H."
			},
			{
				"family": "Ayduk",
				"given": "O."
			},
			{
				"family": "Franklin",
				"given": "N. T."
			},
			{
				"family": "Askren",
				"given": "M. K."
			},
			{
				"family": "Jonides",
				"given": "J."
			},
			{
				"family": "Berman",
				"given": "M. G."
			},
			{
				"family": "Wilson",
				"given": "N. L."
			},
			{
				"family": "Teslovich",
				"given": "T."
			},
			{
				"family": "Glover",
				"given": "G."
			},
			{
				"family": "Zayas",
				"given": "V."
			},
			{
				"family": "Mischel",
				"given": "W."
			},
			{
				"family": "Shoda",
				"given": "Y."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011",
					9,
					6
				]
			]
		}
	},
	{
		"id": "wainrib2013a",
		"type": "article-journal",
		"abstract": "Random neural networks are dynamical descriptions of randomly interconnected neural units. These show a phase transition to chaos as a disorder parameter is increased. The microscopic mechanisms underlying this phase transition are unknown, and similarly to spin-glasses, shall be fundamentally related to the behavior of the system. In this Letter we investigate the explosion of complexity arising near that phase transition. We show that the mean number of equilibria undergoes a sharp transition from one equilibrium to a very large number scaling exponentially with the dimension on the system. Near criticality, we compute the exponential rate of divergence, called topological complexity. Strikingly, we show that it behaves exactly as the maximal Lyapunov exponent, a classical measure of dynamical complexity. This relationship unravels a microscopic mechanism leading to chaos which we further demonstrate on a simpler class of disordered systems, suggesting a deep and underexplored link between topological and dynamical complexity.",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.110.118101",
		"ISSN": "0031-9007, 1079-7114",
		"issue": "11",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"note": "arXiv: 1210.5082",
		"page": "118101",
		"source": "arXiv.org",
		"title": "Topological and Dynamical Complexity of Random Neural Networks",
		"URL": "http://arxiv.org/abs/1210.5082",
		"volume": "110",
		"author": [
			{
				"family": "Wainrib",
				"given": "Gilles"
			},
			{
				"family": "Touboul",
				"given": "Jonathan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013",
					3,
					11
				]
			]
		}
	},
	{
		"id": "fyodorov2004",
		"type": "article-journal",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.92.240601",
		"ISSN": "0031-9007, 1079-7114",
		"issue": "24",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"language": "en",
		"page": "240601",
		"source": "DOI.org (Crossref)",
		"title": "Complexity of Random Energy Landscapes, Glass Transition, and Absolute Value of the Spectral Determinant of Random Matrices",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevLett.92.240601",
		"volume": "92",
		"author": [
			{
				"family": "Fyodorov",
				"given": "Yan V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2004",
					6,
					15
				]
			]
		}
	},
	{
		"id": "davis2019",
		"type": "article-journal",
		"abstract": "Transcranial direct current stimulation (tDCS) is a non-invasive brain stimulation technique which provides unique potential to directly improve human capability on a temporary, at needs, basis. The purpose of this paper is to consider the utility of tDCS through analysis of the potential risks and benefits in the context of defence service personnel. First, we look at the potential benefits, focusing primarily on warfighter survivability and enriching cognition quality in support of command and control. Second, we look at the potential detriments to tDCS military use, focusing on adverse effects, safety considerations, and risk. Third, we examine how the level of risk can be mitigated through military doctrine development focusing on safety parameters and exclusion criteria. Finally, we explore the future prospects of military tDCS use, particularly in terms of addressing gaps in the literature so that tDCS can be used ethically and efficaciously at the level of individual personnel.",
		"container-title": "Frontiers in Human Neuroscience",
		"DOI": "10.3389/fnhum.2019.00114",
		"ISSN": "1662-5161",
		"journalAbbreviation": "Front Hum Neurosci",
		"note": "PMID: 31105538\nPMCID: PMC6499187",
		"source": "PubMed Central",
		"title": "Transcranial Direct Current Stimulation Use in Warfighting: Benefits, Risks, and Future Prospects",
		"title-short": "Transcranial Direct Current Stimulation Use in Warfighting",
		"URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6499187/",
		"volume": "13",
		"author": [
			{
				"family": "Davis",
				"given": "Steven E."
			},
			{
				"family": "Smith",
				"given": "Glen A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					4,
					18
				]
			]
		}
	},
	{
		"id": "brunoni2016",
		"type": "article-journal",
		"abstract": "Background\n        , Transcranial direct current stimulation (tDCS) is a non-pharmacological intervention for depression. It has mixed results, possibly caused by study heterogeneity., \n          Aims\n        , To assess tDCS efficacy and to explore individual response predictors., \n          Method\n        , Systematic review and individual patient data meta-analysis., \n          Results\n        , Data were gathered from six randomised sham-controlled trials, enrolling 289 patients. Active tDCS was significantly superior to sham for response (34% v. 19% respectively, odds ratio (OR) = 2.44, 95% CI 1.38–4.32, number needed to treat (NNT) = 7), remission (23.1% v. 12.7% respectively, OR = 2.38, 95% CI 1.22–4.64, NNT = 9) and depression improvement (B coefficient 0.35, 95% CI 0.12–0.57). Mixed-effects models showed that, after adjustment for other predictors and confounders, treatment-resistant depression and higher tDCS ‘doses’ were, respectively, negatively and positively associated with tDCS efficacy., \n          Conclusions\n        , The effect size of tDCS treatment was comparable with those reported for repetitive transcranial magnetic stimulation and antidepressant drug treatment in primary care. The most important parameters for optimisation in future trials are depression refractoriness and tDCS dose.",
		"container-title": "The British Journal of Psychiatry",
		"DOI": "10.1192/bjp.bp.115.164715",
		"ISSN": "0007-1250",
		"issue": "6",
		"journalAbbreviation": "Br J Psychiatry",
		"note": "PMID: 27056623\nPMCID: PMC4887722",
		"page": "522-531",
		"source": "PubMed Central",
		"title": "Transcranial direct current stimulation for acute major depressive episodes: meta-analysis of individual patient data",
		"title-short": "Transcranial direct current stimulation for acute major depressive episodes",
		"URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4887722/",
		"volume": "208",
		"author": [
			{
				"family": "Brunoni",
				"given": "André R."
			},
			{
				"family": "Moffa",
				"given": "Adriano H."
			},
			{
				"family": "Fregni",
				"given": "Felipe"
			},
			{
				"family": "Palm",
				"given": "Ulrich"
			},
			{
				"family": "Padberg",
				"given": "Frank"
			},
			{
				"family": "Blumberger",
				"given": "Daniel M."
			},
			{
				"family": "Daskalakis",
				"given": "Zafiris J."
			},
			{
				"family": "Bennabi",
				"given": "Djamila"
			},
			{
				"family": "Haffen",
				"given": "Emmanuel"
			},
			{
				"family": "Alonzo",
				"given": "Angelo"
			},
			{
				"family": "Loo",
				"given": "Colleen K."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					6
				]
			]
		}
	},
	{
		"id": "kadmon2015",
		"type": "article-journal",
		"container-title": "Physical Review X",
		"DOI": "10.1103/PhysRevX.5.041030",
		"ISSN": "2160-3308",
		"issue": "4",
		"journalAbbreviation": "Phys. Rev. X",
		"language": "en",
		"page": "041030",
		"source": "DOI.org (Crossref)",
		"title": "Transition to Chaos in Random Neuronal Networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevX.5.041030",
		"volume": "5",
		"author": [
			{
				"family": "Kadmon",
				"given": "Jonathan"
			},
			{
				"family": "Sompolinsky",
				"given": "Haim"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					11,
					19
				]
			]
		}
	},
	{
		"id": "saxe2014",
		"type": "article-journal",
		"abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.",
		"container-title": "arXiv:1312.6120 [cond-mat, q-bio, stat]",
		"note": "arXiv: 1312.6120",
		"source": "arXiv.org",
		"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
		"URL": "http://arxiv.org/abs/1312.6120",
		"author": [
			{
				"family": "Saxe",
				"given": "Andrew M."
			},
			{
				"family": "McClelland",
				"given": "James L."
			},
			{
				"family": "Ganguli",
				"given": "Surya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					9,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					2,
					19
				]
			]
		}
	},
	{
		"id": "valiant1984",
		"type": "article-journal",
		"container-title": "Communications of the ACM",
		"DOI": "10.1145/1968.1972",
		"ISSN": "0001-0782",
		"issue": "11",
		"journalAbbreviation": "Commun. ACM",
		"page": "1134–1142",
		"source": "Nov. 1984",
		"title": "A theory of the learnable",
		"URL": "https://doi.org/10.1145/1968.1972",
		"volume": "27",
		"author": [
			{
				"family": "Valiant",
				"given": "L. G."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1984",
					11,
					5
				]
			]
		}
	},
	{
		"id": "roth2005",
		"type": "article-journal",
		"abstract": "Intelligence has evolved many times independently among vertebrates. Primates, elephants and cetaceans are assumed to be more intelligent than ‘lower’ mammals, the great apes and humans more than monkeys, and humans more than the great apes. Brain properties assumed to be relevant for intelligence are the (absolute or relative) size of the brain, cortex, prefrontal cortex and degree of encephalization. However, factors that correlate better with intelligence are the number of cortical neurons and conduction velocity, as the basis for information-processing capacity. Humans have more cortical neurons than other mammals, although only marginally more than whales and elephants. The outstanding intelligence of humans appears to result from a combination and enhancement of properties found in non-human primates, such as theory of mind, imitation and language, rather than from ‘unique’ properties.",
		"container-title": "Trends in Cognitive Sciences",
		"DOI": "10.1016/j.tics.2005.03.005",
		"ISSN": "1364-6613",
		"issue": "5",
		"journalAbbreviation": "Trends in Cognitive Sciences",
		"language": "en",
		"page": "250-257",
		"source": "ScienceDirect",
		"title": "Evolution of the brain and intelligence",
		"URL": "http://www.sciencedirect.com/science/article/pii/S1364661305000823",
		"volume": "9",
		"author": [
			{
				"family": "Roth",
				"given": "Gerhard"
			},
			{
				"family": "Dicke",
				"given": "Ursula"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2005",
					5,
					1
				]
			]
		}
	},
	{
		"id": "carlsen2012",
		"type": "motion_picture",
		"publisher": "60 minutes",
		"title": "Magnus Carlsen 60 minutes",
		"URL": "https://duckduckgo.com/?q=magnus+carlsen+60+minutes&t=brave&iax=videos&ia=videos&iai=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DQc_v9mTfhC8",
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "vonnegut2007",
		"type": "book",
		"abstract": "In a volume that is penetrating, introspective, incisive, and laugh-out-loud funny, one of the great men of letters of this era—or any er...",
		"publisher": "Random House",
		"title": "A Man Without a Country",
		"URL": "https://www.goodreads.com/work/best_book/1119459-a-man-without-a-country",
		"author": [
			{
				"family": "Vonnegut",
				"given": "Kurt"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007"
				]
			]
		}
	},
	{
		"id": "southcarolina1860",
		"type": "legislation",
		"title": "Declaration of the Immediate Causes Which Induce and Justify the Secession of South Carolina from the Federal Union.",
		"URL": "https://web.archive.org/web/20170611201541/http://www.teachingushistory.org/pdfs/ImmCausesTranscription.pdf",
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1860",
					12,
					24
				]
			]
		}
	},
	{
		"id": "seattle2019",
		"type": "document",
		"title": "Seattle Public Schools K-12 Math Ethnic Studies Framework",
		"URL": "https://www.k12.wa.us/sites/default/files/public/socialstudies/pubdocs/Math%20SDS%20ES%20Framework.pdf",
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "masnov2020",
		"type": "article-magazine",
		"abstract": "Illinois State Representative, LaShawn K. Ford, held a press conference in Evanston to call for the suspension of history classes in Illinois public schools.",
		"container-title": "New Discourses",
		"language": "en-US",
		"title": "History Killers: Illinois Rep. Calls for End to History Education",
		"title-short": "History Killers",
		"URL": "https://newdiscourses.com/2020/08/history-killers-illinois-rep-calls-end-history-education/",
		"author": [
			{
				"family": "Masnov",
				"given": "James M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					8,
					31
				]
			]
		}
	},
	{
		"id": "1984",
		"type": "book",
		"abstract": "Among the seminal texts of the 20th century, Nineteen Eighty-Four is a rare work that grows more haunting as its futuristic purgatory bec...",
		"publisher": "Houghton Mifflin Harcourt",
		"title": "1984",
		"URL": "https://www.goodreads.com/work/best_book/153313-nineteen-eighty-four",
		"author": [
			{
				"family": "Orwell",
				"given": "George"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1949"
				]
			]
		}
	},
	{
		"id": "hannah-jones2019",
		"type": "article-newspaper",
		"abstract": "Our founding ideals of liberty and equality were false when they were written. For generations, black Americans have fought to make them true.",
		"container-title": "The New York Times",
		"ISSN": "0362-4331",
		"language": "en-US",
		"section": "Magazine",
		"source": "NYTimes.com",
		"title": "America Wasn’t a Democracy, Until Black Americans Made It One (Published 2019)",
		"URL": "https://www.nytimes.com/interactive/2019/08/14/magazine/black-history-american-democracy.html, https://www.nytimes.com/interactive/2019/08/14/magazine/black-history-american-democracy.html",
		"author": [
			{
				"family": "Hannah-Jones",
				"given": "Nikole"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					8,
					14
				]
			]
		}
	},
	{
		"id": "wood2019",
		"type": "personal_communication",
		"abstract": "The <em>Times</em> refused a request to correct what five leading historians described as “factual errors” which evinced “a displacement of historical understanding by ideology.” This is Professor Wood’s response.",
		"language": "en",
		"title": "Historian Gordon Wood responds to the New York Times’ defense of the 1619 Project",
		"URL": "https://www.wsws.org/en/articles/2019/12/24/nytr-d24.html",
		"author": [
			{
				"family": "Wood",
				"given": "Gordon"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					12,
					21
				]
			]
		}
	},
	{
		"id": "wood2019a",
		"type": "interview",
		"abstract": "Gordon Wood is professor emeritus at Brown University and author of the Pulitzer Prize-winning book <em>The Radicalism of the American Revolution</em>, as well as <em>Empire of Liberty: A History of the Early Republic, 1789–1815</em>.",
		"language": "en",
		"title": "An interview with historian Gordon Wood on the New York Times’ 1619 Project",
		"URL": "https://www.wsws.org/en/articles/2019/11/28/wood-n28.html",
		"author": [
			{
				"family": "Wood",
				"given": "Gordon"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					11,
					28
				]
			]
		}
	},
	{
		"id": "serwer2019",
		"type": "article-magazine",
		"abstract": "A dispute between a small group of scholars and the authors of The New York Times Magazine’s issue on slavery represents a fundamental disagreement over the trajectory of American society.",
		"container-title": "The Atlantic",
		"language": "en-US",
		"note": "section: Ideas",
		"title": "The Fight Over the 1619 Project Is Not About the Facts",
		"URL": "https://www.theatlantic.com/ideas/archive/2019/12/historians-clash-1619-project/604093/",
		"author": [
			{
				"family": "Serwer",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					12,
					23
				]
			]
		}
	},
	{
		"id": "kelly2019",
		"type": "book",
		"abstract": "Memory Craft book. Read 27 reviews from the world's largest community for readers. , Memory Craft book. Read 27 reviews from the world's largest community for readers.",
		"publisher": "Allen & Unwin",
		"title": "Memory Craft",
		"URL": "https://www.goodreads.com/book/show/45442000-memory-craft",
		"author": [
			{
				"family": "Kelly",
				"given": "Lynne"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "schweiger2020",
		"type": "article-journal",
		"abstract": "Many attempts have been made to classify and evaluate the nature of intelligence in humans and other species (referred to as the ‘g’ factor in the former and the G factor in the latter). The search for this essential structure of mental life has generated various models and definitions, yet open questions remain. Specifically, referring to intelligence by overemphasizing the anthropocentric terminology and its ethnocentric overlay is insufficient to account for individual differences and limits its generalizability in biological and cultural contexts. The present work is an attempt to adopt a different perspective on the ‘g/G’ factor and its measurement. We suggest that intelligence, or g/G, is reflected in a biological capacity that evolved from object manipulation in animals, into mental manipulation in humans, in response to various environmental conditions.",
		"container-title": "Animal Cognition",
		"DOI": "10.1007/s10071-020-01375-2",
		"ISSN": "1435-9456",
		"issue": "4",
		"journalAbbreviation": "Anim Cogn",
		"language": "en",
		"page": "691-701",
		"source": "Springer Link",
		"title": "The transition of object to mental manipulation: beyond a species-specific view of intelligence",
		"title-short": "The transition of object to mental manipulation",
		"URL": "https://doi.org/10.1007/s10071-020-01375-2",
		"volume": "23",
		"author": [
			{
				"family": "Schweiger",
				"given": "Moran Bar-Hen"
			},
			{
				"family": "Henik",
				"given": "Avishai"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					1
				]
			]
		}
	},
	{
		"id": "shettleworth2009",
		"type": "article-journal",
		"abstract": "In “The Snark is a Boojum”, Beach [Beach, F.A., 1950. The snark was a boojum. American Psychologist. 5, 115–124] famously asserted that animal psychology embraced too few species and too few problems to deserve the name comparative. Later in the 20th century, others [e.g. Kamil, A.C., 1988. A synthetic approach to the study of animal intelligence. In: Leger, D.W. (Ed.), Comparative Perspectives in Modern Psychology. Nebraska Symposium on Motivation, vol. 35. University of Nebraska Press, Lincoln, NE, pp. 230–257; Shettleworth, S.J., 1993. Where is the comparison in comparative cognition? Alternative research programs. Psychological Science. 4, 179–184] expressed similar concerns about the new subfield of comparative cognition, suggesting that a more biological approach to choice of species and problems was needed to balance a dominant anthropocentrism. The last 10–15 years have seen many new developments, and a recent survey like Beach’s reveals a very different picture. Not only are many more species being studied, contributions by researchers from different backgrounds are increasing, and research on comparative cognition is better connected with developmental psychology, behavioral neuroscience, primatology, behavioral ecology, and other fields. Contemporary research addresses three major aspects of cognition about equally: basic processes, physical cognition, and social cognition. This article describes a selected research program from each area, chosen to exemplify current trends and challenges for the field.",
		"collection-title": "Comparative cognition in context: A tribute to the contributions of Sara J. Shettleworth",
		"container-title": "Behavioural Processes",
		"DOI": "10.1016/j.beproc.2008.09.001",
		"ISSN": "0376-6357",
		"issue": "3",
		"journalAbbreviation": "Behavioural Processes",
		"language": "en",
		"page": "210-217",
		"source": "ScienceDirect",
		"title": "The evolution of comparative cognition: Is the snark still a boojum?",
		"title-short": "The evolution of comparative cognition",
		"URL": "http://www.sciencedirect.com/science/article/pii/S0376635708002143",
		"volume": "80",
		"author": [
			{
				"family": "Shettleworth",
				"given": "Sara J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009",
					3,
					1
				]
			]
		}
	},
	{
		"id": "shettleworth2009a",
		"type": "article-journal",
		"abstract": "In “The Snark is a Boojum”, Beach [Beach, F.A., 1950. The snark was a boojum. American Psychologist. 5, 115–124] famously asserted that animal psychology embraced too few species and too few problems to deserve the name comparative. Later in the 20th century, others [e.g. Kamil, A.C., 1988. A synthetic approach to the study of animal intelligence. In: Leger, D.W. (Ed.), Comparative Perspectives in Modern Psychology. Nebraska Symposium on Motivation, vol. 35. University of Nebraska Press, Lincoln, NE, pp. 230–257; Shettleworth, S.J., 1993. Where is the comparison in comparative cognition? Alternative research programs. Psychological Science. 4, 179–184] expressed similar concerns about the new subfield of comparative cognition, suggesting that a more biological approach to choice of species and problems was needed to balance a dominant anthropocentrism. The last 10–15 years have seen many new developments, and a recent survey like Beach’s reveals a very different picture. Not only are many more species being studied, contributions by researchers from different backgrounds are increasing, and research on comparative cognition is better connected with developmental psychology, behavioral neuroscience, primatology, behavioral ecology, and other fields. Contemporary research addresses three major aspects of cognition about equally: basic processes, physical cognition, and social cognition. This article describes a selected research program from each area, chosen to exemplify current trends and challenges for the field.",
		"collection-title": "Comparative cognition in context: A tribute to the contributions of Sara J. Shettleworth",
		"container-title": "Behavioural Processes",
		"DOI": "10.1016/j.beproc.2008.09.001",
		"ISSN": "0376-6357",
		"issue": "3",
		"journalAbbreviation": "Behavioural Processes",
		"language": "en",
		"page": "210-217",
		"source": "ScienceDirect",
		"title": "The evolution of comparative cognition: Is the snark still a boojum?",
		"title-short": "The evolution of comparative cognition",
		"URL": "http://www.sciencedirect.com/science/article/pii/S0376635708002143",
		"volume": "80",
		"author": [
			{
				"family": "Shettleworth",
				"given": "Sara J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009",
					3,
					1
				]
			]
		}
	},
	{
		"id": "premack2007",
		"type": "article-journal",
		"abstract": "Microscopic study of the human brain has revealed neural structures, enhanced wiring, and forms of connectivity among nerve cells not found in any animal, challenging the view that the human brain is simply an enlarged chimpanzee brain. On the other hand, cognitive studies have found animals to have abilities once thought unique to the human. This suggests a disparity between brain and mind. The suggestion is misleading. Cognitive research has not kept pace with neural research. Neural findings are based on microscopic study of the brain and are primarily cellular. Because cognition cannot be studied microscopically, we need to refine the study of cognition by using a different approach. In examining claims of similarity between animals and humans, one must ask: What are the dissimilarities? This approach prevents confusing similarity with equivalence. We follow this approach in examining eight cognitive cases—teaching, short-term memory, causal reasoning, planning, deception, transitive inference, theory of mind, and language—and find, in all cases, that similarities between animal and human abilities are small, dissimilarities large. There is no disparity between brain and mind.",
		"container-title": "Proceedings of the National Academy of Sciences",
		"DOI": "10.1073/pnas.0706147104",
		"ISSN": "0027-8424, 1091-6490",
		"issue": "35",
		"journalAbbreviation": "PNAS",
		"language": "en",
		"license": "© 2007 by The National Academy of Sciences of the USA",
		"note": "publisher: National Academy of Sciences\nsection: Review\nPMID: 17717081",
		"page": "13861-13867",
		"source": "www.pnas.org",
		"title": "Human and animal cognition: Continuity and discontinuity",
		"title-short": "Human and animal cognition",
		"URL": "https://www.pnas.org/content/104/35/13861",
		"volume": "104",
		"author": [
			{
				"family": "Premack",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					8,
					28
				]
			]
		}
	},
	{
		"id": "voigtlaender1984",
		"type": "article-journal",
		"abstract": "Eigenvalues of the Fokker—Planck and BGK operators for a d2x2/2 + d4x4/ double-well potential are calculated by the matrix continued-fraction method. A dependence of the eigenvalues on the friction constant or coupling strength is shown for the lowest non-zero real eigenvalue and for some higher, generally complex eigenvalues.",
		"container-title": "Chemical Physics Letters",
		"DOI": "10.1016/0009-2614(84)80100-1",
		"ISSN": "0009-2614",
		"issue": "5",
		"journalAbbreviation": "Chemical Physics Letters",
		"language": "en",
		"page": "506-510",
		"source": "ScienceDirect",
		"title": "Eigenvalues of the Fokker—Planck and BGK operators for a double-well potential",
		"URL": "http://www.sciencedirect.com/science/article/pii/0009261484801001",
		"volume": "105",
		"author": [
			{
				"family": "Voigtlaender",
				"given": "K."
			},
			{
				"family": "Risken",
				"given": "H."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1984",
					3,
					30
				]
			]
		}
	},
	{
		"id": "skinner1978",
		"type": "article-journal",
		"container-title": "The Journal of Chemical Physics",
		"DOI": "10.1063/1.436814",
		"ISSN": "0021-9606",
		"issue": "5",
		"journalAbbreviation": "J. Chem. Phys.",
		"note": "publisher: American Institute of Physics",
		"page": "2143-2150",
		"source": "aip.scitation.org (Atypon)",
		"title": "Relaxation processes and chemical kinetics",
		"URL": "https://aip.scitation.org/doi/abs/10.1063/1.436814",
		"volume": "69",
		"author": [
			{
				"family": "Skinner",
				"given": "James L."
			},
			{
				"family": "Wolynes",
				"given": "Peter G."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1978",
					9,
					1
				]
			]
		}
	},
	{
		"id": "schnell",
		"type": "article-journal",
		"abstract": "The soft-bodied cephalopods including octopus, cuttlefish, and squid are broadly considered to be the most cognitively advanced group of invertebrates. Previous research has demonstrated that these large-brained molluscs possess a suite of cognitive attributes that are comparable to those found in some vertebrates, including highly developed perception, learning, and memory abilities. Cephalopods are also renowned for performing sophisticated feats of flexible behaviour, which have led to claims of complex cognition such as causal reasoning, future planning, and mental attribution. Hypotheses to explain why complex cognition might have emerged in cephalopods suggest that a combination of predation, foraging, and competitive pressures are likely to have driven cognitive complexity in this group of animals. Currently, it is difficult to gauge the extent to which cephalopod behaviours are underpinned by complex cognition because many of the recent claims are largely based on anecdotal evidence. In this review, we provide a general overview of cephalopod cognition with a particular focus on the cognitive attributes that are thought to be prerequisites for more complex cognitive abilities. We then discuss different types of behavioural flexibility exhibited by cephalopods and, using examples from other taxa, highlight that behavioural flexibility could be explained by putatively simpler mechanisms. Consequently, behavioural flexibility should not be used as evidence of complex cognition. Fortunately, the field of comparative cognition centres on designing methods to pinpoint the underlying mechanisms that drive behaviours. To illustrate the utility of the methods developed in comparative cognition research, we provide a series of experimental designs aimed at distinguishing between complex cognition and simpler alternative explanations. Finally, we discuss the advantages of using cephalopods to develop a more comprehensive reconstruction of cognitive evolution.",
		"container-title": "Biological Reviews",
		"DOI": "10.1111/brv.12651",
		"ISSN": "1469-185X",
		"issue": "n/a",
		"language": "en",
		"license": "© 2020 The Authors. Biological Reviews published by John Wiley & Sons Ltd on behalf of Cambridge Philosophical Society.",
		"note": "_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/brv.12651",
		"source": "Wiley Online Library",
		"title": "How intelligent is a cephalopod? Lessons from comparative cognition",
		"title-short": "How intelligent is a cephalopod?",
		"URL": "https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12651",
		"volume": "n/a",
		"author": [
			{
				"family": "Schnell",
				"given": "Alexandra K."
			},
			{
				"family": "Amodio",
				"given": "Piero"
			},
			{
				"family": "Boeckle",
				"given": "Markus"
			},
			{
				"family": "Clayton",
				"given": "Nicola S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					13
				]
			]
		}
	},
	{
		"id": "ahamed2020",
		"type": "article-journal",
		"abstract": "Animal behaviour is often quantified through subjective, incomplete variables that mask essential dynamics. Here, we develop a maximally predictive behavioural-state space from multivariate measurements, in which the full instantaneous state is smoothly unfolded as a combination of short-time posture sequences. In the off-food behaviour of the roundworm Caenorhabditis elegans, we discover a low-dimensional state space dominated by three sets of cyclic trajectories corresponding to the worm’s basic stereotyped motifs: forward, backward and turning locomotion. We find similar results in the on-food behaviour of foraging worms and npr-1 mutants. In contrast to this broad stereotypy, we find variability in the presence of locally unstable dynamics with signatures of deterministic chaos: a collection of unstable periodic orbits together with a positive maximal Lyapunov exponent. The full Lyapunov spectrum is symmetric with positive, chaotic exponents driving variability balanced by negative, dissipative exponents driving stereotypy. The symmetry is indicative of damped–driven Hamiltonian dynamics underlying the worm’s movement control.",
		"container-title": "Nature Physics",
		"DOI": "10.1038/s41567-020-01036-8",
		"ISSN": "1745-2481",
		"language": "en",
		"license": "2020 The Author(s), under exclusive licence to Springer Nature Limited",
		"note": "publisher: Nature Publishing Group",
		"page": "1-9",
		"source": "www-nature-com.proxy.uba.uva.nl:2443",
		"title": "Capturing the continuous complexity of behaviour in Caenorhabditis elegans",
		"URL": "http://www.nature.com/articles/s41567-020-01036-8",
		"author": [
			{
				"family": "Ahamed",
				"given": "Tosif"
			},
			{
				"family": "Costa",
				"given": "Antonio C."
			},
			{
				"family": "Stephens",
				"given": "Greg J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					13
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					5
				]
			]
		}
	},
	{
		"id": "jenks2006",
		"type": "report",
		"genre": "Final Project",
		"title": "Introduction to Kramers Equation",
		"URL": "http://www.physics.drexel.edu/~jenks/Stat.Mech%20stochastic%20dynamics%20final%20project.pdf",
		"author": [
			{
				"family": "Jenks",
				"given": "Steven"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					13
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2006"
				]
			]
		}
	},
	{
		"id": "wittebol",
		"type": "thesis",
		"event-place": "Amsterdam",
		"genre": "Bachelor's",
		"language": "en",
		"publisher": "University of Amsterdam",
		"publisher-place": "Amsterdam",
		"source": "Zotero",
		"title": "The Fokker-Planck equation and its application to potential problems",
		"author": [
			{
				"family": "Wittebol",
				"given": "Jim"
			}
		]
	},
	{
		"id": "levin",
		"type": "article-magazine",
		"abstract": "Biology’s next great horizon is to understand cells, tissues and organisms as agents with agendas (even if unthinking ones)",
		"container-title": "Aeon",
		"language": "en",
		"note": "section: Philosophy",
		"title": "How to understand cells, tissues and organisms as agents with agendas",
		"URL": "https://aeon.co/essays/how-to-understand-cells-tissues-and-organisms-as-agents-with-agendas",
		"author": [
			{
				"family": "Levin",
				"given": "Michael"
			},
			{
				"family": "Dennett",
				"given": "Daniel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					15
				]
			]
		}
	},
	{
		"id": "willingham2015",
		"type": "article-journal",
		"abstract": "Theories of learning styles suggest that individuals think and learn best in different ways. These are not differences of ability but rather preferences for processing certain types of information or for processing information in certain types of way. If accurate, learning styles theories could have important implications for instruction because student achievement would be a product of the interaction of instruction and the student’s style. There is reason to think that people view learning styles theories as broadly accurate, but, in fact, scientific support for these theories is lacking. We suggest that educators’ time and energy are better spent on other theories that might aid instruction.",
		"container-title": "Teaching of Psychology",
		"DOI": "10.1177/0098628315589505",
		"ISSN": "0098-6283",
		"issue": "3",
		"journalAbbreviation": "Teaching of Psychology",
		"language": "en",
		"note": "publisher: SAGE Publications Inc",
		"page": "266-271",
		"source": "SAGE Journals",
		"title": "The Scientific Status of Learning Styles Theories",
		"URL": "https://doi.org/10.1177/0098628315589505",
		"volume": "42",
		"author": [
			{
				"family": "Willingham",
				"given": "Daniel T."
			},
			{
				"family": "Hughes",
				"given": "Elizabeth M."
			},
			{
				"family": "Dobolyi",
				"given": "David G."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					18
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					7,
					1
				]
			]
		}
	},
	{
		"id": "meadows2008",
		"type": "book",
		"abstract": "Meadows’ Thinking in Systems, is a concise and crucial book offering insight for problem solving on scales ranging from the personal to t...",
		"publisher": "Chelsea Green Publishing",
		"title": "Thinking in Systems",
		"URL": "https://www.goodreads.com/work/best_book/3873538-thinking-in-systems-a-primer",
		"author": [
			{
				"family": "Meadows",
				"given": "Donella"
			},
			{
				"family": "Wright",
				"given": "Diana"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2008"
				]
			]
		}
	},
	{
		"id": "costa2020a",
		"type": "article-journal",
		"title": "Maximally predictive ensemble dynamics from data",
		"author": [
			{
				"family": "Costa",
				"given": "Antonio C."
			},
			{
				"family": "Jordan",
				"given": "David"
			},
			{
				"family": "Ahamed",
				"given": "Tosif"
			},
			{
				"family": "Stephens",
				"given": "Greg J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "bailey2017",
		"type": "article-journal",
		"container-title": "Hypatia",
		"issue": "4",
		"page": "877",
		"title": "Tracking Privilege-Preserving Epistemic Pushback in Feminist and Critical Race Philosophy Classes",
		"volume": "32",
		"author": [
			{
				"family": "Bailey",
				"given": "Alison"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "hance2015",
		"type": "webpage",
		"container-title": "Mongabay",
		"language": "en-US",
		"note": "section: Environmental news",
		"title": "Amazon tribe creates 500-page traditional medicine encyclopedia",
		"URL": "https://news.mongabay.com/2015/06/amazon-tribe-creates-500-page-traditional-medicine-encyclopedia/",
		"author": [
			{
				"family": "Hance",
				"given": "Jeremy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					10,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					6,
					24
				]
			]
		}
	},
	{
		"id": "conklin1954",
		"type": "thesis",
		"event-place": "New Haven",
		"genre": "PhD",
		"note": "I haven't actually found an accessible version of this dissertation, only the references",
		"publisher": "Yale University",
		"publisher-place": "New Haven",
		"title": "The relation of Hanunóo culture to the plant world",
		"author": [
			{
				"family": "Conklin",
				"given": "Harold C"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1954"
				]
			]
		}
	},
	{
		"id": "wyman1964",
		"type": "book",
		"collection-number": "12",
		"collection-title": "University of New Mexico Publications in Anthropology",
		"event-place": "Albuquerque",
		"note": "Haven't been able to find a version (see @kelly2020, pg. 62)",
		"number-of-pages": "158",
		"publisher": "University of New Mexico Press",
		"publisher-place": "Albuquerque",
		"title": "Navaho indian ethnoentomology",
		"author": [
			{
				"family": "Wyman",
				"given": "Leland Clifton"
			},
			{
				"family": "Bailey",
				"given": "Flora L"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1964"
				]
			]
		}
	},
	{
		"id": "lange",
		"type": "article-journal",
		"language": "en",
		"page": "95",
		"source": "Zotero",
		"title": "THE TIME EVOLUTION OF THE COSMIC MICROWAVE BACKGROUND PHOTOSPHERE",
		"author": [
			{
				"family": "Lange",
				"given": "Stuart R"
			},
			{
				"family": "Page",
				"given": "Lyman"
			},
			{
				"family": "Steinhardt",
				"given": "Paul"
			}
		]
	},
	{
		"id": "kalmykov2006",
		"type": "article-journal",
		"container-title": "The Journal of Chemical Physics",
		"DOI": "10.1063/1.2140281",
		"ISSN": "0021-9606, 1089-7690",
		"issue": "2",
		"journalAbbreviation": "The Journal of Chemical Physics",
		"language": "en",
		"page": "024107",
		"source": "DOI.org (Crossref)",
		"title": "Thermally activated escape rate for a Brownian particle in a double-well potential for all values of the dissipation",
		"URL": "http://aip.scitation.org/doi/10.1063/1.2140281",
		"volume": "124",
		"author": [
			{
				"family": "Kalmykov",
				"given": "Yu. P."
			},
			{
				"family": "Coffey",
				"given": "W. T."
			},
			{
				"family": "Titov",
				"given": "S. V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					11,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2006",
					1,
					14
				]
			]
		}
	},
	{
		"id": "hanggi1990",
		"type": "article-journal",
		"container-title": "Reviews of Modern Physics",
		"DOI": "10.1103/RevModPhys.62.251",
		"ISSN": "0034-6861, 1539-0756",
		"issue": "2",
		"journalAbbreviation": "Rev. Mod. Phys.",
		"language": "en",
		"page": "251-341",
		"source": "DOI.org (Crossref)",
		"title": "Reaction-rate theory: fifty years after Kramers",
		"title-short": "Reaction-rate theory",
		"URL": "https://link.aps.org/doi/10.1103/RevModPhys.62.251",
		"volume": "62",
		"author": [
			{
				"family": "Hänggi",
				"given": "Peter"
			},
			{
				"family": "Talkner",
				"given": "Peter"
			},
			{
				"family": "Borkovec",
				"given": "Michal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					11,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1990",
					4,
					1
				]
			]
		}
	},
	{
		"id": "moss1989",
		"type": "book",
		"number-of-volumes": "3",
		"publisher": "Cambridge University Press",
		"volume": "1",
		"editor": [
			{
				"family": "Moss",
				"given": "Frank"
			},
			{
				"family": "McClintock",
				"given": "P.V.E"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1989"
				]
			]
		}
	},
	{
		"id": "pontryagin1933",
		"type": "article-journal",
		"journalAbbreviation": "Zh. Eksp. Teor. Fiz.",
		"note": "Through [@moss1989]",
		"page": "165-80",
		"volume": "3",
		"author": [
			{
				"family": "Pontryagin",
				"given": "L."
			},
			{
				"family": "Andronov",
				"given": "A."
			},
			{
				"family": "Vitt",
				"given": "A."
			}
		],
		"translator": [
			{
				"family": "Barvour",
				"given": "Julian B."
			}
		],
		"issued": {
			"date-parts": [
				[
					"1933"
				]
			]
		}
	},
	{
		"id": "gajamannage2015",
		"type": "article-journal",
		"abstract": "Collective motion of animal groups often undergoes changes due to perturbations. In a topological sense, we describe these changes as switching between low-dimensional embedding manifolds underlying a group of evolving agents. To characterize such manifolds, first we introduce a simple mapping of agents between time-steps. Then, we construct a novel metric which is susceptible to variations in the collective motion, thus revealing distinct underlying manifolds. The method is validated through three sample scenarios simulated using a Vicsek model, namely switching of speed, coordination, and structure of a group. Combined with a dimensionality reduction technique that is used to infer the dimensionality of the embedding manifold, this approach provides an effective model-free framework for the analysis of collective behavior across animal species.",
		"container-title": "The European Physical Journal Special Topics",
		"DOI": "10.1140/epjst/e2015-50088-2",
		"ISSN": "1951-6355, 1951-6401",
		"issue": "17-18",
		"journalAbbreviation": "Eur. Phys. J. Spec. Top.",
		"note": "arXiv: 1508.02809\nversion: 1",
		"page": "3245-3256",
		"source": "arXiv.org",
		"title": "Identifying manifolds underlying group motion in Vicsek agents",
		"URL": "http://arxiv.org/abs/1508.02809",
		"volume": "224",
		"author": [
			{
				"family": "Gajamannage",
				"given": "Kelum"
			},
			{
				"family": "Butail",
				"given": "Sachit"
			},
			{
				"family": "Porfiri",
				"given": "Maurizio"
			},
			{
				"family": "Bollt",
				"given": "Erik M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					11,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					12
				]
			]
		}
	},
	{
		"id": "wood2020",
		"type": "article-magazine",
		"abstract": "A historian believes he has discovered iron laws that predict the rise and fall of societies. He has bad news.",
		"container-title": "The Atlantic",
		"ISSN": "1072-7825",
		"source": "The Atlantic",
		"title": "The Next Decade Could Be Even Worse",
		"URL": "https://www.theatlantic.com/magazine/archive/2020/12/can-history-predict-future/616993/",
		"author": [
			{
				"family": "Wood",
				"given": "Story by Graeme"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "ruelle1997",
		"type": "article-journal",
		"abstract": "We consider systems of nonequilibrium statistical mechanics, driven by nonconservative forces and in contact with an ideal thermostat. These are smooth dynamical systems for which one can define natural stationary states μ (SRB in the simplest case) and entropy production e(μ) (minus the sum of the Lyapunov exponents in the simplest case).  We give exact and explicit definitions of the entropy production e(μ) for the various situations of physical interest.  We prove that e(μ)≥0 and indicate cases where e(μ)>0. The novelty of the approach is that we do not try to compute entropy production directly, but make it depend on the identification of a natural stationary state for the system.",
		"container-title": "Communications in Mathematical Physics",
		"DOI": "10.1007/s002200050207",
		"ISSN": "1432-0916",
		"issue": "2",
		"journalAbbreviation": "Comm Math Phys",
		"language": "en",
		"page": "365-371",
		"source": "Springer Link",
		"title": "Entropy Production in Nonequilibrium Statistical Mechanics",
		"URL": "https://doi.org/10.1007/s002200050207",
		"volume": "189",
		"author": [
			{
				"family": "Ruelle",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1997",
					11,
					1
				]
			]
		}
	},
	{
		"id": "lesne2014",
		"type": "article-journal",
		"abstract": "Statistical entropy was introduced by Shannon as a basic concept in information theory measuring the average missing information in a random source. Extended into an entropy rate, it gives bounds in coding and compression theorems. In this paper, I describe how statistical entropy and entropy rate relate to other notions of entropy that are relevant to probability theory (entropy of a discrete probability distribution measuring its unevenness), computer sciences (algorithmic complexity), the ergodic theory of dynamical systems (Kolmogorov–Sinai or metric entropy) and statistical physics (Boltzmann entropy). Their mathematical foundations and correlates (the entropy concentration, Sanov, Shannon–McMillan–Breiman, Lempel–Ziv and Pesin theorems) clarify their interpretation and offer a rigorous basis for maximum entropy principles. Although often ignored, these mathematical perspectives give a central position to entropy and relative entropy in statistical laws describing generic collective behaviours, and provide insights into the notions of randomness, typicality and disorder. The relevance of entropy beyond the realm of physics, in particular for living systems and ecosystems, is yet to be demonstrated.",
		"container-title": "Mathematical Structures in Computer Science",
		"DOI": "10.1017/S0960129512000783",
		"ISSN": "0960-1295, 1469-8072",
		"issue": "3",
		"language": "en",
		"note": "publisher: Cambridge University Press",
		"source": "Cambridge University Press",
		"title": "Shannon entropy: a rigorous notion at the crossroads between probability, information theory, dynamical systems and statistical physics",
		"title-short": "Shannon entropy",
		"URL": "https://www.cambridge.org/core/journals/mathematical-structures-in-computer-science/article/shannon-entropy-a-rigorous-notion-at-the-crossroads-between-probability-information-theory-dynamical-systems-and-statistical-physics/4A4B7B069BCF64CC595635D865317C83",
		"volume": "24",
		"author": [
			{
				"family": "Lesne",
				"given": "Annick"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					6
				]
			]
		}
	},
	{
		"id": "gell-mann1954",
		"type": "article-journal",
		"container-title": "Physical Review",
		"DOI": "10.1103/PhysRev.95.1300",
		"ISSN": "0031-899X",
		"issue": "5",
		"journalAbbreviation": "Phys. Rev.",
		"language": "en",
		"page": "1300-1312",
		"source": "DOI.org (Crossref)",
		"title": "Quantum Electrodynamics at Small Distances",
		"URL": "https://link.aps.org/doi/10.1103/PhysRev.95.1300",
		"volume": "95",
		"author": [
			{
				"family": "Gell-Mann",
				"given": "M."
			},
			{
				"family": "Low",
				"given": "F. E."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					11,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1954",
					9,
					1
				]
			]
		}
	},
	{
		"id": "myron1971",
		"type": "article-magazine",
		"container-title": "Scientific American",
		"issue": "3",
		"language": "180",
		"title": "Energy and Information",
		"volume": "225",
		"contributor": [
			{
				"family": "Claude",
				"given": "Shannon"
			}
		],
		"author": [
			{
				"family": "Myron",
				"given": "Tribus"
			},
			{
				"family": "Elwin",
				"given": "McIrvine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1971"
				]
			]
		}
	},
	{
		"id": "kesavan2009",
		"type": "chapter",
		"abstract": "Article OutlineKeywordsEntropy and UncertaintyWhy Choose Maximum Uncertainty?Shannon EntropyJaynes’ Maximum Entropy FormalismApplications of MaxEnt and ConclusionsSee alsoReferences",
		"container-title": "Encyclopedia of Optimization",
		"event-place": "Boston, MA",
		"ISBN": "978-0-387-74759-0",
		"language": "en",
		"note": "DOI: 10.1007/978-0-387-74759-0_312",
		"page": "1779-1782",
		"publisher": "Springer US",
		"publisher-place": "Boston, MA",
		"source": "Springer Link",
		"title": "Jaynes’ maximum entropy principleJaynes’ Maximum Entropy Principle",
		"URL": "https://doi.org/10.1007/978-0-387-74759-0_312",
		"author": [
			{
				"family": "Kesavan",
				"given": "H. K."
			}
		],
		"editor": [
			{
				"family": "Floudas",
				"given": "Christodoulos A."
			},
			{
				"family": "Pardalos",
				"given": "Panos M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					11,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009"
				]
			]
		}
	},
	{
		"id": "shannon1948",
		"type": "article-journal",
		"language": "en",
		"page": "55",
		"source": "Zotero",
		"title": "A Mathematical Theory of Communication",
		"author": [
			{
				"family": "Shannon",
				"given": "C E"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1948"
				]
			]
		}
	},
	{
		"id": "toner1998",
		"type": "article-journal",
		"abstract": "We present a quantitative continuum theory of “flocking”: the collective coherent motion of large numbers of self-propelled organisms. In agreement with everyday experience, our model predicts the existence of an “ordered phase” of flocks, in which all members of even an arbitrarily large flock move together with the same mean velocity ⟨→v⟩≠0. This coherent motion of the flock is an example of spontaneously broken symmetry: no preferred direction for the motion is picked out a priori in the model; rather, each flock is allowed to, and does, spontaneously pick out some completely arbitrary direction to move in. By analyzing our model we can make detailed, quantitative predictions for the long-distance, long-time behavior of this “broken symmetry state.” The “Goldstone modes” associated with this “spontaneously broken rotational symmetry” are fluctuations in the direction of motion of a large part of the flock away from the mean direction of motion of the flock as a whole. These “Goldstone modes” mix with modes associated with conservation of bird number to produce propagating sound modes. These sound modes lead to enormous fluctuations of the density of the flock, far larger, at long wavelengths, than those in, e.g., an equilibrium gas. Our model is similar in many ways to the Navier-Stokes equations for a simple compressible fluid; in other ways, it resembles a relaxational time-dependent Ginsburg-Landau theory for an n=d component isotropic ferromagnet. In spatial dimensions d>4, the long-distance behavior is correctly described by a linearized theory, and is equivalent to that of an unusual but nonetheless equilibrium model for spin systems. For d<4, nonlinear fluctuation effects radically alter the long distance behavior, making it different from that of any known equilibrium model. In particular, we find that in d=2, where we can calculate the scaling exponents exactly, flocks exhibit a true, long-range ordered, spontaneously broken symmetry state, in contrast to equilibrium systems, which cannot spontaneously break a continuous symmetry in d=2 (the “Mermin-Wagner” theorem). We make detailed predictions for various correlation functions that could be measured either in simulations, or by quantitative imaging of real flocks. We also consider an anisotropic model, in which the birds move preferentially in an “easy” (e.g., horizontal) plane, and make analogous, but quantitatively different, predictions for that model as well. For this anisotropic model, we obtain exact scaling exponents for all spatial dimensions, including the physically relevant case d=3. , This article appears in the following collections:",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.58.4828",
		"issue": "4",
		"journalAbbreviation": "Phys. Rev. E",
		"note": "publisher: American Physical Society",
		"page": "4828-4858",
		"source": "APS",
		"title": "Flocks, herds, and schools: A quantitative theory of flocking",
		"title-short": "Flocks, herds, and schools",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.58.4828",
		"volume": "58",
		"author": [
			{
				"family": "Toner",
				"given": "John"
			},
			{
				"family": "Tu",
				"given": "Yuhai"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					11,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1998",
					10,
					1
				]
			]
		}
	},
	{
		"id": "berestycki2014",
		"type": "report",
		"genre": "Lecture Notes",
		"language": "en",
		"page": "72",
		"publisher": "Cambrdige University",
		"source": "Zotero",
		"title": "Lectures on Mixing Times",
		"author": [
			{
				"family": "Berestycki",
				"given": "Nathanael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "mitra2020",
		"type": "article-journal",
		"abstract": "In this article, we first give a comprehensive description of random walk (RW) problem focusing on self-similarity, dynamic scaling and its connection to diffusion phenomena. One of the main goals of our work is to check how robust the RW problem is under various different choices of the step size. We show that RW with random step size or uniformly shrinking step size is exactly the same as for RW with fixed step size. Krapivsky and Redner in 2004 showed that RW with geometric shrinking step size, such that the size of the $n$th step is given by $S_n=\\lambda^n$ with a fixed $\\lambda<1$ value, exhibits some interesting features which are different from the RW with fixed step size. Motivated by this, we investigate what if $\\lambda$ is not a fixed number rather it depends on the step number $n$? To this end, we first generate $N$ random numbers for RW of $t=N$ which are then arranged in a descending order so that the size of the $n$th step is $\\lambda_n^n$. We have shown, both numerically and analytically, that $\\lambda_n=(1-n/N)$, the root mean square displacement increases as $t^{1/4}$ which are different from all the known results on RW problems.",
		"container-title": "arXiv:2010.02579 [cond-mat]",
		"note": "arXiv: 2010.02579",
		"source": "arXiv.org",
		"title": "Similarity and self-similarity in random walk with fixed, random and shrinking steps",
		"URL": "http://arxiv.org/abs/2010.02579",
		"author": [
			{
				"family": "Mitra",
				"given": "Tushar"
			},
			{
				"family": "Hossain",
				"given": "Tomal"
			},
			{
				"family": "Banerjee",
				"given": "Santo"
			},
			{
				"family": "Hassan",
				"given": "Md Kamrul"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					11,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					6
				]
			]
		}
	},
	{
		"id": "schehr2010",
		"type": "article-journal",
		"abstract": "We use the real space renormalization group (RSRG) method to study extreme value statistics for a variety of Brownian motions, free or constrained, such as the Brownian bridge, excursion, meander and reﬂected bridge, recovering some standard results, and extending others. We apply the same method to compute the distribution of extrema of Bessel processes. We brieﬂy show how the continuous time random walk (CTRW) corresponds to a non-standard ﬁxed point of the RSRG transformation.",
		"container-title": "Journal of Statistical Mechanics: Theory and Experiment",
		"DOI": "10.1088/1742-5468/2010/01/P01009",
		"ISSN": "1742-5468",
		"issue": "01",
		"journalAbbreviation": "J. Stat. Mech.",
		"language": "en",
		"page": "P01009",
		"source": "DOI.org (Crossref)",
		"title": "Extreme value statistics from the real space renormalization group: Brownian motion, Bessel processes and continuous time random walks",
		"title-short": "Extreme value statistics from the real space renormalization group",
		"URL": "https://iopscience.iop.org/article/10.1088/1742-5468/2010/01/P01009",
		"volume": "2010",
		"author": [
			{
				"family": "Schehr",
				"given": "Grégory"
			},
			{
				"family": "Le Doussal",
				"given": "Pierre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					11,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2010",
					1,
					15
				]
			]
		}
	},
	{
		"id": "lee2020",
		"type": "webpage",
		"container-title": "The Sephist",
		"title": "Build tools around workflows, not workflows around tools | thesephist.com",
		"URL": "https://thesephist.com/posts/tools/",
		"author": [
			{
				"family": "Lee",
				"given": "Linus"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					11,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					8,
					4
				]
			]
		}
	},
	{
		"id": "seiberg2007",
		"type": "article-journal",
		"abstract": "We summarize the arguments that space and time are likely to be emergent notions; i.e. they are not present in the fundamental formulation of the theory, but appear as approximate macroscopic concepts. Along the way we briefly review certain topics. These include ambiguities in the geometry and the topology of space which arise from dualities, questions associated with locality, various known examples of emergent space, and the puzzles and the prospects of emergent time.",
		"container-title": "The Quantum Structure of Space and Time",
		"DOI": "10.1142/9789812706768_0005",
		"note": "arXiv: hep-th/0601234",
		"page": "163-213",
		"source": "arXiv.org",
		"title": "Emergent Spacetime",
		"URL": "http://arxiv.org/abs/hep-th/0601234",
		"author": [
			{
				"family": "Seiberg",
				"given": "Nathan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					12,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					1
				]
			]
		}
	},
	{
		"id": "apenko2012",
		"type": "article-journal",
		"abstract": "We present a possible approach to the study of the renormalization group (RG) flow based entirely on the information theory. The average information loss under a single step of Wilsonian RG transformation is evaluated as a conditional entropy of the fast variables, which are integrated out, when the slow ones are held fixed. Its positivity results in the monotonic decrease of the informational entropy under renormalization. This, however, does not necessarily imply the irreversibility of the RG flow, because entropy is an extensive quantity and explicitly depends on the total number of degrees of freedom, which is reduced. Only some size-independent additive part of the entropy could possibly provide the required Lyapunov function. We also introduce a mutual information of fast and slow variables as probably a more adequate quantity to represent the changes in the system under renormalization and evaluate it for some simple systems. It is shown that for certain real space decimation transformations the positivity of the mutual information directly leads to the monotonic growth of the entropy per lattice site along the RG flow and hence to its irreversibility.",
		"container-title": "Physica A: Statistical Mechanics and its Applications",
		"DOI": "10.1016/j.physa.2011.08.014",
		"ISSN": "0378-4371",
		"issue": "1",
		"journalAbbreviation": "Physica A: Statistical Mechanics and its Applications",
		"language": "en",
		"page": "62-77",
		"source": "ScienceDirect",
		"title": "Information theory and renormalization group flows",
		"URL": "http://www.sciencedirect.com/science/article/pii/S037843711100642X",
		"volume": "391",
		"author": [
			{
				"family": "Apenko",
				"given": "S. M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					12,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					1,
					1
				]
			]
		}
	},
	{
		"id": "mezic2013",
		"type": "article-journal",
		"abstract": "This article reviews theory and applications of Koopman modes in fluid mechanics. Koopman mode decomposition is based on the surprising fact, discovered in Mezić (2005), that normal modes of linear oscillations have their natural analogs—Koopman modes—in the context of nonlinear dynamics. To pursue this analogy, one must change the representation of the system from the state-space representation to the dynamics governed by the linear Koopman operator on an infinite-dimensional space of observables. Whereas Koopman in his original paper dealt only with measure-preserving transformations, the discussion here is predominantly on dissipative systems arising from Navier-Stokes evolution. The analysis is based on spectral properties of the Koopman operator. Aspects of point and continuous parts of the spectrum are discussed. The point spectrum corresponds to isolated frequencies of oscillation present in the fluid flow, and also to growth rates of stable and unstable modes. The continuous part of the spectrum corresponds to chaotic motion on the attractor. A method of computation of the spectrum and the associated Koopman modes is discussed in terms of generalized Laplace analysis. When applied to a generic observable, this method uncovers the full point spectrum. A computational alternative is given by Arnoldi-type methods, leading to so-called dynamic mode decomposition, and I discuss the connection and differences between these two methods. A number of applications are reviewed in which decompositions of this type have been pursued. Koopman mode theory unifies and provides a rigorous background for a number of different concepts that have been advanced in fluid mechanics, including global mode analysis, triple decomposition, and dynamic mode decomposition.",
		"container-title": "Annual Review of Fluid Mechanics",
		"DOI": "10.1146/annurev-fluid-011212-140652",
		"issue": "1",
		"note": "_eprint: https://doi.org/10.1146/annurev-fluid-011212-140652",
		"page": "357-378",
		"source": "Annual Reviews",
		"title": "Analysis of Fluid Flows via Spectral Properties of the Koopman Operator",
		"URL": "https://doi.org/10.1146/annurev-fluid-011212-140652",
		"volume": "45",
		"author": [
			{
				"family": "Mezić",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					12,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/6773082/items/TAZEAM9T",
		"type": "document",
		"title": "Full Text PDF",
		"URL": "http://www.annualreviews.org/doi/pdf/10.1146/annurev-fluid-011212-140652",
		"accessed": {
			"date-parts": [
				[
					"2020",
					12,
					2
				]
			]
		}
	},
	{
		"id": "williams",
		"type": "article-journal",
		"abstract": "In recent years, methods for data-driven Koopman spectral analysis, such as Dynamic Mode Decomposition (DMD), have become increasingly popular approaches for extracting dynamically relevant features from data sets. However to establish the connection between techniques like DMD or Extended DMD (EDMD) and the Koopman operator, assumptions are made about the nature of the supplied data. In particular, both methods assume the data were generated by an autonomous dynamical system, which can be limiting in certain experimental or computational settings, such as when system actuation is present. We present a modiﬁcation of EDMD that overcomes this limitation by compensating for the e↵ects of actuation, and is capable of recovering the leading Koopman eigenvalues, eigenfunctions, and modes of the unforced system. To highlight the e cacy of this approach, we apply it to two examples with (quasi)-periodic forcing: the ﬁrst is the Du ng oscillator, which demonstrates eigenfunction approximation, and the second is a lattice Boltzmann code that approximates the FitzHugh-Nagumo partial di↵erential equation and shows Koopman mode and eigenvalue computation.",
		"language": "en",
		"page": "6",
		"source": "Zotero",
		"title": "Extending Data-Driven Koopman Analysis to Actuated Systems",
		"author": [
			{
				"family": "Williams",
				"given": "Matthew O"
			},
			{
				"family": "Hemati",
				"given": "Maziar S"
			},
			{
				"family": "Rowley",
				"given": "Clarence W"
			}
		]
	},
	{
		"id": "mezic2017",
		"type": "article-journal",
		"abstract": "We examine spectral operator-theoretic properties of linear and nonlinear dynamical systems with equilibrium and quasi-periodic attractors and use such properties to characterize a class of datasets and introduce a new notion of the principal dimension of the data.",
		"source": "ResearchGate",
		"title": "Koopman Operator Spectrum and Data Analysis",
		"author": [
			{
				"family": "Mezic",
				"given": "Igor"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017",
					2,
					22
				]
			]
		}
	},
	{
		"id": "cardy",
		"type": "article-journal",
		"abstract": "The universal relation between critical exponents and the amplitude of the correlation length divergence as a function of finite size at the critical point of twodimensional systems is shown to be a consequence of conformal invariance. Both periodic and free boundary conditions are considered.",
		"language": "en",
		"page": "4",
		"source": "Zotero",
		"title": "Conformal invariance and universality in finite-size scaling",
		"author": [
			{
				"family": "Cardy",
				"given": "John L"
			}
		]
	},
	{
		"id": "nightingale1982",
		"type": "article-journal",
		"container-title": "Journal of Applied Physics",
		"DOI": "10.1063/1.330232",
		"ISSN": "0021-8979, 1089-7550",
		"issue": "11",
		"journalAbbreviation": "Journal of Applied Physics",
		"language": "en",
		"page": "7927-7932",
		"source": "DOI.org (Crossref)",
		"title": "Finite‐size scaling and phenomenological renormalization (invited)",
		"URL": "http://aip.scitation.org/doi/10.1063/1.330232",
		"volume": "53",
		"author": [
			{
				"family": "Nightingale",
				"given": "Peter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					12,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1982",
					11
				]
			]
		}
	},
	{
		"id": "onsager1944",
		"type": "article-journal",
		"container-title": "Physical Review",
		"DOI": "10.1103/PhysRev.65.117",
		"ISSN": "0031-899X",
		"issue": "3-4",
		"journalAbbreviation": "Phys. Rev.",
		"language": "en",
		"page": "117-149",
		"source": "DOI.org (Crossref)",
		"title": "Crystal Statistics. I. A Two-Dimensional Model with an Order-Disorder Transition",
		"URL": "https://link.aps.org/doi/10.1103/PhysRev.65.117",
		"volume": "65",
		"author": [
			{
				"family": "Onsager",
				"given": "Lars"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					12,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1944",
					2,
					1
				]
			]
		}
	},
	{
		"id": "kramers1941",
		"type": "article-journal",
		"container-title": "Physical Review",
		"DOI": "10.1103/PhysRev.60.252",
		"ISSN": "0031-899X",
		"issue": "3",
		"journalAbbreviation": "Phys. Rev.",
		"language": "en",
		"page": "252-262",
		"source": "DOI.org (Crossref)",
		"title": "Statistics of the Two-Dimensional Ferromagnet. Part I",
		"URL": "https://link.aps.org/doi/10.1103/PhysRev.60.252",
		"volume": "60",
		"author": [
			{
				"family": "Kramers",
				"given": "H. A."
			},
			{
				"family": "Wannier",
				"given": "G. H."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					12,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1941",
					8,
					1
				]
			]
		}
	},
	{
		"id": "1986",
		"type": "article-journal",
		"abstract": "It is shown how conformal invariance relates many numerically accessible properties of the transfer matrix of a critical system in a finite-width infi…",
		"container-title": "Nuclear Physics B",
		"DOI": "10.1016/0550-3213(86)90552-3",
		"ISSN": "0550-3213",
		"language": "en",
		"note": "publisher: North-Holland",
		"page": "186-204",
		"source": "www-sciencedirect-com.proxy.uba.uva.nl:2443",
		"title": "Operator content of two-dimensional conformally invariant theories",
		"URL": "http://www.sciencedirect.com/science/article/pii/0550321386905523",
		"volume": "270",
		"accessed": {
			"date-parts": [
				[
					"2020",
					12,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1986",
					1,
					1
				]
			]
		}
	},
	{
		"id": "tishby2000",
		"type": "article-journal",
		"abstract": "We define the relevant information in a signal $x\\in X$ as being the information that this signal provides about another signal $y\\in \\Y$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal $x$ requires more than just predicting $y$, it also requires specifying which features of $\\X$ play a role in the prediction. We formalize this problem as that of finding a short code for $\\X$ that preserves the maximum information about $\\Y$. That is, we squeeze the information that $\\X$ provides about $\\Y$ through a `bottleneck' formed by a limited set of codewords $\\tX$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure $d(x,\\x)$ emerges from the joint statistics of $\\X$ and $\\Y$. This approach yields an exact set of self consistent equations for the coding rules $X \\to \\tX$ and $\\tX \\to \\Y$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.",
		"container-title": "arXiv:physics/0004057",
		"note": "arXiv: physics/0004057",
		"source": "arXiv.org",
		"title": "The information bottleneck method",
		"URL": "http://arxiv.org/abs/physics/0004057",
		"author": [
			{
				"family": "Tishby",
				"given": "Naftali"
			},
			{
				"family": "Pereira",
				"given": "Fernando C."
			},
			{
				"family": "Bialek",
				"given": "William"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					12,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2000",
					4,
					24
				]
			]
		}
	},
	{
		"id": "gollwitzer2009",
		"type": "article-journal",
		"abstract": "Based on Lewinian goal theory in general and self-completion theory in particular, four experiments examined the implications of other people taking notice of one’s identity-related behavioral intentions (e.g., the intention to read law periodicals regularly to reach the identity goal of becoming a lawyer). Identity-related behavioral intentions that had been noticed by other people were translated into action less intensively than those that had been ignored (Studies 1–3). This effect was evident in the ﬁeld (persistent striving over 1 week’s time; Study 1) and in the laboratory (jumping on opportunities to act; Studies 2 and 3), and it held among participants with strong but not weak commitment to the identity goal (Study 3). Study 4 showed, in addition, that when other people take notice of an individual’s identity-related behavioral intention, this gives the individual a premature sense of possessing the aspired-to identity.",
		"container-title": "Psychological Science",
		"DOI": "10.1111/j.1467-9280.2009.02336.x",
		"ISSN": "0956-7976, 1467-9280",
		"issue": "5",
		"journalAbbreviation": "Psychol Sci",
		"language": "en",
		"page": "612-618",
		"source": "DOI.org (Crossref)",
		"title": "When Intentions Go Public: Does Social Reality Widen the Intention-Behavior Gap?",
		"title-short": "When Intentions Go Public",
		"URL": "http://journals.sagepub.com/doi/10.1111/j.1467-9280.2009.02336.x",
		"volume": "20",
		"author": [
			{
				"family": "Gollwitzer",
				"given": "Peter M."
			},
			{
				"family": "Sheeran",
				"given": "Paschal"
			},
			{
				"family": "Michalski",
				"given": "Verena"
			},
			{
				"family": "Seifert",
				"given": "Andrea E."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2020",
					12,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009",
					5
				]
			]
		}
	},
	{
		"id": "geiger2020",
		"type": "article-journal",
		"abstract": "Deep learning algorithms are responsible for a technological revolution in a variety of tasks including image recognition or Go playing. Yet, why they work is not understood. Ultimately, they manage to classify data lying in high dimension -- a feat generically impossible due to the geometry of high dimensional space and the associated curse of dimensionality. Understanding what kind of structure, symmetry or invariance makes data such as images learnable is a fundamental challenge. Other puzzles include that (i) learning corresponds to minimizing a loss in high dimension, which is in general not convex and could well get stuck bad minima. (ii) Deep learning predicting power increases with the number of fitting parameters, even in a regime where data are perfectly fitted. In this manuscript, we review recent results elucidating (i,ii) and the perspective they offer on the (still unexplained) curse of dimensionality paradox. We base our theoretical discussion on the $(h,\\alpha)$ plane where $h$ is the network width and $\\alpha$ the scale of the output of the network at initialization, and provide new systematic measures of performance in that plane for MNIST and CIFAR 10. We argue that different learning regimes can be organized into a phase diagram. A line of critical points sharply delimits an under-parametrised phase from an over-parametrized one. In over-parametrized nets, learning can operate in two regimes separated by a smooth cross-over. At large initialization, it corresponds to a kernel method, whereas for small initializations features can be learnt, together with invariants in the data. We review the properties of these different phases, of the transition separating them and some open questions. Our treatment emphasizes analogies with physical systems, scaling arguments and the development of numerical observables to quantitatively test these results empirically.",
		"container-title": "arXiv:2012.15110 [cs]",
		"note": "arXiv: 2012.15110",
		"source": "arXiv.org",
		"title": "Perspective: A Phase Diagram for Deep Learning unifying Jamming, Feature Learning and Lazy Training",
		"title-short": "Perspective",
		"URL": "http://arxiv.org/abs/2012.15110",
		"author": [
			{
				"family": "Geiger",
				"given": "Mario"
			},
			{
				"family": "Petrini",
				"given": "Leonardo"
			},
			{
				"family": "Wyart",
				"given": "Matthieu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					1,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12,
					30
				]
			]
		}
	},
	{
		"id": "jr1976",
		"type": "article-newspaper",
		"abstract": "Kurt Vonnegut Jr, Bernard Malamud and Desmond Morris articles on recent banning of their books by Levittown school com; illus (L)",
		"container-title": "The New York Times",
		"ISSN": "0362-4331",
		"language": "en-US",
		"section": "Archives",
		"source": "NYTimes.com",
		"title": "Banned Authors Answer Back (Published 1976)",
		"URL": "https://www.nytimes.com/1976/03/28/archives/long-island-weekly-banned-authors-answer-back-they-see-the-move-as.html",
		"author": [
			{
				"family": "Jr",
				"given": "Kurt Vonnegut"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					1,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1976",
					3,
					28
				]
			]
		}
	},
	{
		"id": "coscia2021",
		"type": "article-journal",
		"abstract": "Network science is the field dedicated to the investigation and analysis of complex systems via their representations as networks. We normally model such networks as graphs: sets of nodes connected by sets of edges and a number of node and edge attributes. This deceptively simple object is the starting point of never-ending complexity, due to its ability to represent almost every facet of reality: chemical interactions, protein pathways inside cells, neural connections inside the brain, scientific collaborations, financial relations, citations in art history, just to name a few examples. If we hope to make sense of complex networks, we need to master a large analytic toolbox: graph and probability theory, linear algebra, statistical physics, machine learning, combinatorics, and more. This book aims at providing the first access to all these tools. It is intended as an \"Atlas\", because its interest is not in making you a specialist in using any of these techniques. Rather, after reading this book, you will have a general understanding about the existence and the mechanics of all these approaches. You can use such an understanding as the starting point of your own career in the field of network science. This has been, so far, an interdisciplinary endeavor. The founding fathers of this field come from many different backgrounds: mathematics, sociology, computer science, physics, history, digital humanities, and more. This Atlas is charting your path to be something different from all of that: a pure network scientist.",
		"container-title": "arXiv:2101.00863 [physics]",
		"note": "arXiv: 2101.00863",
		"source": "arXiv.org",
		"title": "The Atlas for the Aspiring Network Scientist",
		"URL": "http://arxiv.org/abs/2101.00863",
		"author": [
			{
				"family": "Coscia",
				"given": "Michele"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					1,
					9
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					1,
					4
				]
			]
		}
	},
	{
		"id": "dai2011",
		"type": "article-journal",
		"abstract": "This article reviews recent literature on drought of the last millennium, followed by an update on global aridity changes from 1950 to 2008. Projected future aridity is presented based on recent studies and our analysis of model simulations. Dry periods lasting for years to decades have occurred many times during the last millennium over, for example, North America, West Africa, and East Asia. These droughts were likely triggered by anomalous tropical sea surface temperatures (SSTs), with La Niña-like SST anomalies leading to drought in North America, and El-Niño-like SSTs causing drought in East China. Over Africa, the southward shift of the warmest SSTs in the Atlantic and warming in the Indian Ocean are responsible for the recent Sahel droughts. Local feedbacks may enhance and prolong drought. Global aridity has increased substantially since the 1970s due to recent drying over Africa, southern Europe, East and South Asia, and eastern Australia. Although El Niño-Southern Oscillation (ENSO), tropical Atlantic SSTs, and Asian monsoons have played a large role in the recent drying, recent warming has increased atmospheric moisture demand and likely altered atmospheric circulation patterns, both contributing to the drying. Climate models project increased aridity in the 21st century over most of Africa, southern Europe and the Middle East, most of the Americas, Australia, and Southeast Asia. Regions like the United States have avoided prolonged droughts during the last 50 years due to natural climate variations, but might see persistent droughts in the next 20–50 years. Future efforts to predict drought will depend on models' ability to predict tropical SSTs. WIREs Clim Change 2011 2 45–65 DOI: 10.1002/wcc.81 This article is categorized under: Paleoclimates and Current Trends > Detection and Attribution Paleoclimates and Current Trends > Modern Climate Change Assessing Impacts of Climate Change > Evaluating Future Impacts of Climate Change Assessing Impacts of Climate Change > Observed Impacts of Climate Change",
		"container-title": "WIREs Climate Change",
		"DOI": "https://doi.org/10.1002/wcc.81",
		"ISSN": "1757-7799",
		"issue": "1",
		"language": "en",
		"note": "_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcc.81",
		"page": "45-65",
		"source": "Wiley Online Library",
		"title": "Drought under global warming: a review",
		"title-short": "Drought under global warming",
		"URL": "http://onlinelibrary.wiley.com/doi/abs/10.1002/wcc.81",
		"volume": "2",
		"author": [
			{
				"family": "Dai",
				"given": "Aiguo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					1,
					13
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		}
	},
	{
		"id": "brown2020",
		"type": "article-journal",
		"abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
		"container-title": "arXiv:2005.14165 [cs]",
		"note": "arXiv: 2005.14165",
		"source": "arXiv.org",
		"title": "Language Models are Few-Shot Learners",
		"URL": "http://arxiv.org/abs/2005.14165",
		"author": [
			{
				"family": "Brown",
				"given": "Tom B."
			},
			{
				"family": "Mann",
				"given": "Benjamin"
			},
			{
				"family": "Ryder",
				"given": "Nick"
			},
			{
				"family": "Subbiah",
				"given": "Melanie"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Neelakantan",
				"given": "Arvind"
			},
			{
				"family": "Shyam",
				"given": "Pranav"
			},
			{
				"family": "Sastry",
				"given": "Girish"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Agarwal",
				"given": "Sandhini"
			},
			{
				"family": "Herbert-Voss",
				"given": "Ariel"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Ramesh",
				"given": "Aditya"
			},
			{
				"family": "Ziegler",
				"given": "Daniel M."
			},
			{
				"family": "Wu",
				"given": "Jeffrey"
			},
			{
				"family": "Winter",
				"given": "Clemens"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Chen",
				"given": "Mark"
			},
			{
				"family": "Sigler",
				"given": "Eric"
			},
			{
				"family": "Litwin",
				"given": "Mateusz"
			},
			{
				"family": "Gray",
				"given": "Scott"
			},
			{
				"family": "Chess",
				"given": "Benjamin"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Berner",
				"given": "Christopher"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					1,
					18
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					22
				]
			]
		}
	},
	{
		"id": "mcnamara2001",
		"type": "article-journal",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.64.051103",
		"ISSN": "1063-651X, 1095-3787",
		"issue": "5",
		"journalAbbreviation": "Phys. Rev. E",
		"language": "en",
		"page": "051103",
		"source": "DOI.org (Crossref)",
		"title": "Origin of the hydrodynamic Lyapunov modes",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.64.051103",
		"volume": "64",
		"author": [
			{
				"family": "McNamara",
				"given": "Sean"
			},
			{
				"family": "Mareschal",
				"given": "Michel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					1,
					18
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2001",
					10,
					16
				]
			]
		}
	},
	{
		"id": "froyland2009",
		"type": "article-journal",
		"abstract": "We study the transport and mixing properties of flows in a variety of settings, connecting the classical geometrical approach via invariant manifolds with a probabilistic approach via transfer operators. For non-divergent fluid-like flows, we demonstrate that eigenvectors of numerical transfer operators efficiently decompose the domain into invariant regions. For dissipative chaotic flows such a decomposition into invariant regions does not exist; instead, the transfer operator approach detects almost-invariant sets. We demonstrate numerically that the boundaries of these almost-invariant regions are predominantly comprised of segments of co-dimension 1 invariant manifolds. For a mixing periodically driven fluid-like flow we show that while sets bounded by stable and unstable manifolds are almost-invariant, the transfer operator approach can identify almost-invariant sets with smaller mass leakage. Thus the transport mechanism of lobe dynamics need not correspond to minimal transport.",
		"container-title": "Physica D",
		"language": "en",
		"page": "17",
		"source": "Zotero",
		"title": "Almost-invariant sets and invariant manifolds — Connecting probabilistic and geometric descriptions of coherent structures in flows",
		"author": [
			{
				"family": "Froyland",
				"given": "Gary"
			},
			{
				"family": "Padberg",
				"given": "Kathrin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2009"
				]
			]
		}
	},
	{
		"id": "dellnitz1997",
		"type": "article-journal",
		"container-title": "Chaos: An Interdisciplinary Journal of Nonlinear Science",
		"DOI": "10.1063/1.166223",
		"ISSN": "1054-1500",
		"issue": "2",
		"journalAbbreviation": "Chaos",
		"note": "publisher: American Institute of Physics",
		"page": "221-228",
		"source": "aip.scitation.org (Atypon)",
		"title": "Exploring invariant sets and invariant measures",
		"URL": "https://aip.scitation.org/doi/abs/10.1063/1.166223",
		"volume": "7",
		"author": [
			{
				"family": "Dellnitz",
				"given": "Michael"
			},
			{
				"family": "Hohmann",
				"given": "Andreas"
			},
			{
				"family": "Junge",
				"given": "Oliver"
			},
			{
				"family": "Rumpf",
				"given": "Martin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					1,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1997",
					6,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/6773082/items/EI9TQD2N",
		"type": "document",
		"title": "Full Text PDF",
		"URL": "https://aip.scitation.org/doi/pdf/10.1063/1.166223?casa_token=-GLu7bS088MAAAAA:xkEUCPLu_t6zDgyHwzf0BZkVejSe2vG-X__wK4uSMAW2zkCsd2yhrgjaOpEAkpH6cNDTie3JddrAk7o",
		"accessed": {
			"date-parts": [
				[
					"2021",
					1,
					21
				]
			]
		}
	},
	{
		"id": "wainrib2013b",
		"type": "article-journal",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.110.118101",
		"ISSN": "0031-9007, 1079-7114",
		"issue": "11",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"language": "en",
		"page": "118101",
		"source": "DOI.org (Crossref)",
		"title": "Topological and Dynamical Complexity of Random Neural Networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevLett.110.118101",
		"volume": "110",
		"author": [
			{
				"family": "Wainrib",
				"given": "Gilles"
			},
			{
				"family": "Touboul",
				"given": "Jonathan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					1,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013",
					3,
					11
				]
			]
		}
	},
	{
		"id": "aljadeff2016",
		"type": "article-journal",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.93.022302",
		"ISSN": "2470-0045, 2470-0053",
		"issue": "2",
		"journalAbbreviation": "Phys. Rev. E",
		"language": "en",
		"page": "022302",
		"source": "DOI.org (Crossref)",
		"title": "Low-dimensional dynamics of structured random networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.93.022302",
		"volume": "93",
		"author": [
			{
				"family": "Aljadeff",
				"given": "Johnatan"
			},
			{
				"family": "Renfrew",
				"given": "David"
			},
			{
				"family": "Vegué",
				"given": "Marina"
			},
			{
				"family": "Sharpee",
				"given": "Tatyana O."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					2,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					2,
					5
				]
			]
		}
	},
	{
		"id": "kadmon2015a",
		"type": "article-journal",
		"container-title": "Physical Review X",
		"DOI": "10.1103/PhysRevX.5.041030",
		"ISSN": "2160-3308",
		"issue": "4",
		"journalAbbreviation": "Phys. Rev. X",
		"language": "en",
		"page": "041030",
		"source": "DOI.org (Crossref)",
		"title": "Transition to Chaos in Random Neuronal Networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevX.5.041030",
		"volume": "5",
		"author": [
			{
				"family": "Kadmon",
				"given": "Jonathan"
			},
			{
				"family": "Sompolinsky",
				"given": "Haim"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					2,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					11,
					19
				]
			]
		}
	},
	{
		"id": "hagmann2007",
		"type": "article-journal",
		"abstract": "Understanding the large-scale structural network formed by neurons is a major challenge in system neuroscience. A detailed connectivity map covering the entire brain would therefore be of great value. Based on diffusion MRI, we propose an efficient methodology to generate large, comprehensive and individual white matter connectional datasets of the living or dead, human or animal brain. This non-invasive tool enables us to study the basic and potentially complex network properties of the entire brain. For two human subjects we find that their individual brain networks have an exponential node degree distribution and that their global organization is in the form of a small world.",
		"container-title": "PLoS ONE",
		"DOI": "10.1371/journal.pone.0000597",
		"ISSN": "1932-6203",
		"issue": "7",
		"journalAbbreviation": "PLoS One",
		"note": "PMID: 17611629\nPMCID: PMC1895920",
		"source": "PubMed Central",
		"title": "Mapping Human Whole-Brain Structural Networks with Diffusion MRI",
		"URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1895920/",
		"volume": "2",
		"author": [
			{
				"family": "Hagmann",
				"given": "Patric"
			},
			{
				"family": "Kurant",
				"given": "Maciej"
			},
			{
				"family": "Gigandet",
				"given": "Xavier"
			},
			{
				"family": "Thiran",
				"given": "Patrick"
			},
			{
				"family": "Wedeen",
				"given": "Van J."
			},
			{
				"family": "Meuli",
				"given": "Reto"
			},
			{
				"family": "Thiran",
				"given": "Jean-Philippe"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					2,
					15
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					7,
					4
				]
			]
		}
	},
	{
		"id": "rubinov2010",
		"type": "article-journal",
		"abstract": "Brain connectivity datasets comprise networks of brain regions connected by anatomical tracts or by functional associations. Complex network analysis—a new multidisciplinary approach to the study of complex systems—aims to characterize these brain networks with a small number of neurobiologically meaningful and easily computable measures. In this article, we discuss construction of brain networks from connectivity data and describe the most commonly used network measures of structural and functional connectivity. We describe measures that variously detect functional integration and segregation, quantify centrality of individual brain regions or pathways, characterize patterns of local anatomical circuitry, and test resilience of networks to insult. We discuss the issues surrounding comparison of structural and functional network connectivity, as well as comparison of networks across subjects. Finally, we describe a Matlab toolbox (http://www.brain-connectivity-toolbox.net) accompanying this article and containing a collection of complex network measures and large-scale neuroanatomical connectivity datasets.",
		"collection-title": "Computational Models of the Brain",
		"container-title": "NeuroImage",
		"DOI": "10.1016/j.neuroimage.2009.10.003",
		"ISSN": "1053-8119",
		"issue": "3",
		"journalAbbreviation": "NeuroImage",
		"language": "en",
		"page": "1059-1069",
		"source": "ScienceDirect",
		"title": "Complex network measures of brain connectivity: Uses and interpretations",
		"title-short": "Complex network measures of brain connectivity",
		"URL": "https://www.sciencedirect.com/science/article/pii/S105381190901074X",
		"volume": "52",
		"author": [
			{
				"family": "Rubinov",
				"given": "Mikail"
			},
			{
				"family": "Sporns",
				"given": "Olaf"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					2,
					15
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2010",
					9,
					1
				]
			]
		}
	},
	{
		"id": "ahmadian2015",
		"type": "article-journal",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.91.012820",
		"ISSN": "1539-3755, 1550-2376",
		"issue": "1",
		"journalAbbreviation": "Phys. Rev. E",
		"language": "en",
		"page": "012820",
		"source": "DOI.org (Crossref)",
		"title": "Properties of networks with partially structured and partially random connectivity",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.91.012820",
		"volume": "91",
		"author": [
			{
				"family": "Ahmadian",
				"given": "Yashar"
			},
			{
				"family": "Fumarola",
				"given": "Francesco"
			},
			{
				"family": "Miller",
				"given": "Kenneth D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					1,
					26
				]
			]
		}
	},
	{
		"id": "ipsen2020",
		"type": "article-journal",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.101.052412",
		"ISSN": "2470-0045, 2470-0053",
		"issue": "5",
		"journalAbbreviation": "Phys. Rev. E",
		"language": "en",
		"page": "052412",
		"source": "DOI.org (Crossref)",
		"title": "Consequences of Dale's law on the stability-complexity relationship of random neural networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.101.052412",
		"volume": "101",
		"author": [
			{
				"family": "Ipsen",
				"given": "J. R."
			},
			{
				"family": "Peterson",
				"given": "A. D. H."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					5,
					28
				]
			]
		}
	},
	{
		"id": "sommers1988",
		"type": "article-journal",
		"abstract": "The average eigenvalue distribution $\\rho(\\lambda)$ of N*N real random asymmetric matrices $J_{ij}$ ($J_{ji} \\ne J_{ij}$) is calculated in the limit $N \\rightarrow \\infty$. It is found that $\\rho(\\lambda)$ is uniform in an ellipse in the complex plane, whose real and imaginary axes are $1+\\tau$ and $1-\\tau$ respectively. The parameter \\tau is given by $\\tau= N[J_{ij} J_{ji})]_J$ and $N[J_{ij}^2)]_J$ is normalized to 1. In the $\\tau=1$ limit, Wigner's semicircle law is recovered. The results are extended to complex asymmetric matrices.",
		"container-title": "Physical review letters",
		"DOI": "10.1103/PhysRevLett.60.1895",
		"journalAbbreviation": "Physical review letters",
		"page": "1895-1898",
		"source": "ResearchGate",
		"title": "Spectrum of Large Random Asymmetric Matrices",
		"volume": "60",
		"author": [
			{
				"family": "Sommers",
				"given": "H."
			},
			{
				"family": "Crisanti",
				"given": "Andrea"
			},
			{
				"family": "Sompolinsky",
				"given": "Haim"
			},
			{
				"family": "Stein",
				"given": "Yaakov"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1988",
					6,
					1
				]
			]
		}
	},
	{
		"id": "sommers1988a",
		"type": "article-journal",
		"abstract": "The average eigenvalue distribution $\\rho(\\lambda)$ of N*N real random asymmetric matrices $J_{ij}$ ($J_{ji} \\ne J_{ij}$) is calculated in the limit $N \\rightarrow \\infty$. It is found that $\\rho(\\lambda)$ is uniform in an ellipse in the complex plane, whose real and imaginary axes are $1+\\tau$ and $1-\\tau$ respectively. The parameter \\tau is given by $\\tau= N[J_{ij} J_{ji})]_J$ and $N[J_{ij}^2)]_J$ is normalized to 1. In the $\\tau=1$ limit, Wigner's semicircle law is recovered. The results are extended to complex asymmetric matrices.",
		"container-title": "Physical review letters",
		"DOI": "10.1103/PhysRevLett.60.1895",
		"journalAbbreviation": "Physical review letters",
		"page": "1895-1898",
		"source": "ResearchGate",
		"title": "Spectrum of Large Random Asymmetric Matrices",
		"volume": "60",
		"author": [
			{
				"family": "Sommers",
				"given": "H."
			},
			{
				"family": "Crisanti",
				"given": "Andrea"
			},
			{
				"family": "Sompolinsky",
				"given": "Haim"
			},
			{
				"family": "Stein",
				"given": "Yaakov"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1988",
					6,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/6773082/items/SFIGC4Q9",
		"type": "document",
		"title": "Comment: 21 pages, contribution to 'The Oxford Handbook of Random Matrix Theory'"
	},
	{
		"id": "burda2009",
		"type": "article-journal",
		"abstract": "We discuss non-Gaussian random matrices whose elements are random variables with heavy-tailed probability distributions. In probability theory heavy tails of the distributions describe rare but violent events which usually have dominant influence on the statistics. They also completely change universal properties of eigenvalues and eigenvectors of random matrices. We concentrate here on the universal macroscopic properties of (1) Wigner matrices belonging to the Levy basin of attraction, (2) matrices representing stable free random variables and (3) a class of heavy-tailed matrices obtained by parametric deformations of standard ensembles.",
		"container-title": "arXiv:0909.5228 [cond-mat, physics:math-ph]",
		"note": "arXiv: 0909.5228\nversion: 1",
		"source": "arXiv.org",
		"title": "Heavy-tailed random matrices",
		"URL": "http://arxiv.org/abs/0909.5228",
		"author": [
			{
				"family": "Burda",
				"given": "Z."
			},
			{
				"family": "Jurkiewicz",
				"given": "J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009",
					9,
					28
				]
			]
		}
	},
	{
		"id": "woollett2009",
		"type": "article-journal",
		"abstract": "Grey matter volume increases have been associated with expertise in a range of domains. Much less is known, however, about the broader cognitive advantages or costs associated with skills and their concomitant neuroanatomy. In this study we investigated a group of highly skilled navigators, licensed London taxi drivers. We replicated findings from previous studies by showing taxi drivers had greater grey matter volume in posterior hippocampus and less grey matter volume in anterior hippocampus compared to matched control subjects. We then employed an extensive battery of tests to investigate the neuropsychological consequences of being a skilled taxi driver. Their learning of and recognition memory for individual items was comparable with control subjects, as were working memory, retrograde memory, perceptual and executive functions. By contrast, taxi drivers were significantly more knowledgeable about London landmarks and their spatial relationships. However, they were significantly worse at forming and retaining new associations involving visual information. We consider possible reasons for this decreased performance including the reduced grey matter volume in the anterior hippocampus of taxi drivers, similarities with models of aging, and saturation of long-term potentiation which may reduce information-storage capacity.",
		"container-title": "Neuropsychologia",
		"DOI": "10.1016/j.neuropsychologia.2008.12.036",
		"ISSN": "0028-3932",
		"issue": "4",
		"journalAbbreviation": "Neuropsychologia",
		"language": "eng",
		"note": "PMID: 19171158\nPMCID: PMC2670971",
		"page": "1088-1095",
		"source": "PubMed",
		"title": "Navigational expertise may compromise anterograde associative memory",
		"volume": "47",
		"author": [
			{
				"family": "Woollett",
				"given": "Katherine"
			},
			{
				"family": "Maguire",
				"given": "Eleanor A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2009",
					3
				]
			]
		}
	},
	{
		"id": "maguire2000",
		"type": "article-journal",
		"abstract": "Structural MRIs of the brains of humans with extensive navigation experience, licensed London taxi drivers, were analyzed and compared with those of control subjects who did not drive taxis. The posterior hippocampi of taxi drivers were significantly larger relative to those of control subjects. A more anterior hippocampal region was larger in control subjects than in taxi drivers. Hippocampal volume correlated with the amount of time spent as a taxi driver (positively in the posterior and negatively in the anterior hippocampus). These data are in accordance with the idea that the posterior hippocampus stores a spatial representation of the environment and can expand regionally to accommodate elaboration of this representation in people with a high dependence on navigational skills. It seems that there is a capacity for local plastic change in the structure of the healthy adult human brain in response to environmental demands.",
		"container-title": "Proceedings of the National Academy of Sciences",
		"DOI": "10.1073/pnas.070039597",
		"ISSN": "0027-8424, 1091-6490",
		"issue": "8",
		"journalAbbreviation": "PNAS",
		"language": "en",
		"license": "Copyright © The National Academy of Sciences",
		"note": "PMID: 10716738",
		"page": "4398-4403",
		"source": "www.pnas.org",
		"title": "Navigation-related structural change in the hippocampi of taxi drivers",
		"URL": "https://www.pnas.org/content/97/8/4398",
		"volume": "97",
		"author": [
			{
				"family": "Maguire",
				"given": "Eleanor A."
			},
			{
				"family": "Gadian",
				"given": "David G."
			},
			{
				"family": "Johnsrude",
				"given": "Ingrid S."
			},
			{
				"family": "Good",
				"given": "Catriona D."
			},
			{
				"family": "Ashburner",
				"given": "John"
			},
			{
				"family": "Frackowiak",
				"given": "Richard S. J."
			},
			{
				"family": "Frith",
				"given": "Christopher D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2000",
					4,
					11
				]
			]
		}
	},
	{
		"id": "wallace2013",
		"type": "book",
		"abstract": "Two friends, both of them vocational snoots, sat down to film an interview in February 2006. Their subjects: language and writing. The in...",
		"publisher": "Penrose Pub",
		"title": "Quack This Way",
		"URL": "https://www.goodreads.com/work/best_book/26503683-quack-this-way",
		"author": [
			{
				"family": "Wallace",
				"given": "David Foster"
			},
			{
				"family": "Garner",
				"given": "Bryan A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		}
	},
	{
		"id": "twain1907",
		"type": "book",
		"title": "Mark Twain's Autobiography",
		"URL": "https://www.goodreads.com/work/best_book/23640909-chapters-from-my-autobiography",
		"author": [
			{
				"family": "Twain",
				"given": "Mark"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1907"
				]
			]
		}
	},
	{
		"id": "franklin1791",
		"type": "webpage",
		"abstract": "The Autobiography of Benjamin Franklin book. Read 3,014 reviews from the world's largest community for readers. , The Autobiography of Benjamin Franklin book. Read 3,014 reviews from the world's largest community for readers.",
		"title": "The Autobiography of Benjamin Franklin",
		"URL": "https://www.goodreads.com/work/best_book/598905-the-autobiography-of-benjamin-franklin",
		"author": [
			{
				"family": "Franklin",
				"given": "Benjamin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1791"
				]
			]
		}
	},
	{
		"id": "oakley2014",
		"type": "book",
		"abstract": "Whether you are a student struggling to fulfill a math or science requirement, or you are embarking on a career change that requires a hi...",
		"publisher": "The Penguin Group",
		"title": "A Mind for Numbers",
		"URL": "https://www.goodreads.com/work/best_book/26542201-a-mind-for-numbers-how-to-excel-at-math-and-science",
		"author": [
			{
				"family": "Oakley",
				"given": "Barbara"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "corb2020",
		"type": "book",
		"abstract": "A powerful examination of what we think we know about the brain and why -- despite technological advances -- the workings of our most ess...",
		"publisher": "Basic Books",
		"title": "The Idea of the Brain",
		"URL": "https://www.goodreads.com/book/show/51719771-the-idea-of-the-brain",
		"author": [
			{
				"family": "Corb",
				"given": "Matthew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "azevedo2009",
		"type": "article-journal",
		"abstract": "The human brain is often considered to be the most cognitively capable among mammalian brains and to be much larger than expected for a mammal of our body size. Although the number of neurons is generally assumed to be a determinant of computational power, and despite the widespread quotes that the human brain contains 100 billion neurons and ten times more glial cells, the absolute number of neurons and glial cells in the human brain remains unknown. Here we determine these numbers by using the isotropic fractionator and compare them with the expected values for a human-sized primate. We find that the adult male human brain contains on average 86.1 +/- 8.1 billion NeuN-positive cells (\"neurons\") and 84.6 +/- 9.8 billion NeuN-negative (\"nonneuronal\") cells. With only 19% of all neurons located in the cerebral cortex, greater cortical size (representing 82% of total brain mass) in humans compared with other primates does not reflect an increased relative number of cortical neurons. The ratios between glial cells and neurons in the human brain structures are similar to those found in other primates, and their numbers of cells match those expected for a primate of human proportions. These findings challenge the common view that humans stand out from other primates in their brain composition and indicate that, with regard to numbers of neuronal and nonneuronal cells, the human brain is an isometrically scaled-up primate brain.",
		"container-title": "The Journal of Comparative Neurology",
		"DOI": "10.1002/cne.21974",
		"ISSN": "1096-9861",
		"issue": "5",
		"journalAbbreviation": "J Comp Neurol",
		"language": "eng",
		"note": "PMID: 19226510",
		"page": "532-541",
		"source": "PubMed",
		"title": "Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain",
		"volume": "513",
		"author": [
			{
				"family": "Azevedo",
				"given": "Frederico A. C."
			},
			{
				"family": "Carvalho",
				"given": "Ludmila R. B."
			},
			{
				"family": "Grinberg",
				"given": "Lea T."
			},
			{
				"family": "Farfel",
				"given": "José Marcelo"
			},
			{
				"family": "Ferretti",
				"given": "Renata E. L."
			},
			{
				"family": "Leite",
				"given": "Renata E. P."
			},
			{
				"family": "Jacob Filho",
				"given": "Wilson"
			},
			{
				"family": "Lent",
				"given": "Roberto"
			},
			{
				"family": "Herculano-Houzel",
				"given": "Suzana"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2009",
					4,
					10
				]
			]
		}
	},
	{
		"id": "zotero-503",
		"type": "webpage",
		"title": "Synaptic Transmission | Principles of Neural Science, Fifth Edition | AccessNeurology | McGraw-Hill Medical",
		"URL": "https://neurology.mhmedical.com/content.aspx?bookid=1049&sectionid=59138631",
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					22
				]
			]
		}
	},
	{
		"id": "pakkenberg2003",
		"type": "article-journal",
		"abstract": "Neurostereology has been applied to quantitative anatomical study of the human brain. Such studies have included the total neocortical number of neurons and glial cells, the estimated size distribution of neocortical neurons, the total myelinated fiber length in the brain white matter, the total number of synapses in the neocortex, and the effect of normal aging on these structural elements. The difference in total number of neurons was found to be less than 10% over the age range from 20 to 90 years, while the glial cell number in six elderly individuals, mean age 89.2 years, showed an average number of 36 billion glial cells, which was not statistically significantly different from the 39 billion glial cells in the neocortex of six young individuals with a mean age of 26.2 years. The total myelinated fiber length varied from 150,000 to 180,000km in young individuals and showed a large reduction as a function of age. The total number of synapses in the human neocortex is approximately 0.15×1015 (0.15 quadrillion). Although the effect of aging is seen in all estimated structural elements, the effect of sex is actually higher. The functional relevance of these differences in neuron numbers in both age and gender is not known.",
		"collection-title": "Proceedings of the 6th International Symposium on the Neurobiology and Neuroendocrinology of Aging",
		"container-title": "Experimental Gerontology",
		"DOI": "10.1016/S0531-5565(02)00151-1",
		"ISSN": "0531-5565",
		"issue": "1",
		"journalAbbreviation": "Experimental Gerontology",
		"language": "en",
		"page": "95-99",
		"source": "ScienceDirect",
		"title": "Aging and the human neocortex",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0531556502001511",
		"volume": "38",
		"author": [
			{
				"family": "Pakkenberg",
				"given": "Bente"
			},
			{
				"family": "Pelvig",
				"given": "Dorte"
			},
			{
				"family": "Marner",
				"given": "Lisbeth"
			},
			{
				"family": "Bundgaard",
				"given": "Mads J."
			},
			{
				"family": "Gundersen",
				"given": "Hans Jørgen G."
			},
			{
				"family": "Nyengaard",
				"given": "Jens R."
			},
			{
				"family": "Regeur",
				"given": "Lisbeth"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2003",
					1,
					1
				]
			]
		}
	},
	{
		"id": "tang2001",
		"type": "article-journal",
		"abstract": "An estimator of the total number of synapses in neocortex of human autopsy brains based on unbiased stereological principles is described. Each randomly chosen cerebral hemisphere was stratified into the four major neocortical regions. Uniform sampling with a varying sampling fraction in each region of neocortex was performed. The total volume of each neocortical region was estimated using point counting according to Cavalieri's principle. The ethanolic phosphotungstic acid staining technique was modified for synapses in human autopsy brains. The numerical density of synapses in each neocortical region studied was estimated using the disector at the electron microscopical level. The total number of neocortical synapses in each region was estimated as the product of the total volume of neocortex and the numerical density of synapses. The influence of the postmortem fixation delay on the number of synapses was investigated in five large mammals (one dog, one cow, and three pigs), the brains of which were kept under conditions similar to those under which human corpses are normally kept. The apparent decrease of 3.9% in the numerical density of synapses in the large mammals following a 2-day fixation delay was not significant. The average total number of synapses in the neocortex of five young male brains was 164 x 10(12) (CV = 0.17). An analysis of the precision of the estimate of the total number of synapses in neocortex indicates that blocks represent both the major source of variation and the largest workload. Using eight blocks per brain the imprecision of the estimate is, however, only 66% of the total variance.",
		"container-title": "Synapse (New York, N.Y.)",
		"DOI": "10.1002/syn.1083",
		"ISSN": "0887-4476",
		"issue": "3",
		"journalAbbreviation": "Synapse",
		"language": "eng",
		"note": "PMID: 11418939",
		"page": "258-273",
		"source": "PubMed",
		"title": "Total regional and global number of synapses in the human brain neocortex",
		"volume": "41",
		"author": [
			{
				"family": "Tang",
				"given": "Y."
			},
			{
				"family": "Nyengaard",
				"given": "J. R."
			},
			{
				"family": "De Groot",
				"given": "D. M."
			},
			{
				"family": "Gundersen",
				"given": "H. J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2001",
					9,
					1
				]
			]
		}
	},
	{
		"id": "llinas2004",
		"type": "book",
		"abstract": "The cerebellum is a very distinct region of the brain, occupying a position immediately behind the tectal plate and straddles the midline as a bridge over the fourth ventricle. The basic functional design of the cerebellum is that of an interaction between two sets of different neuronal elements: those of the cortex and those in the centrally located cerebellar nuclei. The cerebellar cortex receives two types of afferents, the climbing fibers and the mossy fibers, and generates a single output system, the axons of Purkinje cells. The cerebellar nuclei receive collaterals from the climbing and mossy fibers and are the main targets for the Purkinje cell axons. The cerebellum as a whole is connected to the rest of the central nervous system by three large fiber bundles, the cerebellar peduncles. This chapter discusses the general organization of the cerebellum, covering its neuronal elements, synaptic connections, basic circuit organization, intrinsic membrane properties, synaptic actions, dendritic properties, and functional circuits.",
		"ISBN": "978-0-19-986444-7",
		"language": "en_US",
		"publisher": "Oxford University Press",
		"source": "oxford.universitypressscholarship.com",
		"title": "Cerebellum",
		"URL": "https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195159561.001.1/acprof-9780195159561-chapter-7",
		"author": [
			{
				"family": "Llinás",
				"given": "Rodolfo R."
			},
			{
				"family": "Walton",
				"given": "Kerry D."
			},
			{
				"family": "Lang",
				"given": "Eric J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2004"
				]
			]
		}
	},
	{
		"id": "bartol2015",
		"type": "article-journal",
		"abstract": "Information in a computer is quantified by the number of bits that can be stored and recovered. An important question about the brain is how much information can be stored at a synapse through synaptic plasticity, which depends on the history of probabilistic synaptic activity. The strong correlation between size and efficacy of a synapse allowed us to estimate the variability of synaptic plasticity. In an EM reconstruction of hippocampal neuropil we found single axons making two or more synaptic contacts onto the same dendrites, having shared histories of presynaptic and postsynaptic activity. The spine heads and neck diameters, but not neck lengths, of these pairs were nearly identical in size. We found that there is a minimum of 26 distinguishable synaptic strengths, corresponding to storing 4.7 bits of information at each synapse. Because of stochastic variability of synaptic activation the observed precision requires averaging activity over several minutes.",
		"container-title": "eLife",
		"DOI": "10.7554/eLife.10778",
		"ISSN": "2050-084X",
		"page": "e10778",
		"source": "eLife",
		"title": "Nanoconnectomic upper bound on the variability of synaptic plasticity",
		"URL": "https://doi.org/10.7554/eLife.10778",
		"volume": "4",
		"author": [
			{
				"family": "Bartol",
				"given": "Thomas M",
				"suffix": "Jr"
			},
			{
				"family": "Bromer",
				"given": "Cailey"
			},
			{
				"family": "Kinney",
				"given": "Justin"
			},
			{
				"family": "Chirillo",
				"given": "Michael A"
			},
			{
				"family": "Bourne",
				"given": "Jennifer N"
			},
			{
				"family": "Harris",
				"given": "Kristen M"
			},
			{
				"family": "Sejnowski",
				"given": "Terrence J"
			}
		],
		"editor": [
			{
				"family": "Nelson",
				"given": "Sacha B"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					11,
					30
				]
			]
		}
	},
	{
		"id": "koch2004",
		"type": "book",
		"abstract": "Neural network research often builds on the fiction that neurons are simple linear threshold units, completely neglecting the highly dynamic and complex nature of synapses, dendrites, and voltage-dependent ionic currents.  Biophysics of Computation: Information Processing in Single Neurons challenges this notion, using richly detailed experimental and theoretical findings from cellular biophysics to explain the repertoire of computational functions available to single neurons. The author shows how individual nerve cells can multiply, integrate, or delay synaptic inputs and how information can be encoded in the voltage across the membrane, in the intracellular calcium concentration, or in the timing of individual spikes.Key topics covered include the linear cable equation; cable theory as applied to passive dendritic trees and dendritic spines; chemical and electrical synapses and how to treat them from a computational point of view; nonlinear interactions of synaptic input in passive and active dendritic trees; the Hodgkin-Huxley model of action potential generation and propagation; phase space analysis; linking stochastic ionic channels to membrane-dependent currents; calcium and potassium currents and their role in information processing; the role of diffusion, buffering and binding of calcium, and other messenger systems in information processing and storage; short- and long-term models of synaptic plasticity; simplified models of single cells; stochastic aspects of neuronal firing; the nature of the neuronal code; and unconventional models of sub-cellular computation.Biophysics of Computation: Information Processing in Single Neurons serves as an ideal text for advanced undergraduate and graduate courses in cellular biophysics, computational neuroscience, and neural networks, and will appeal to students and professionals in neuroscience, electrical and computer engineering, and physics.",
		"edition": "Illustrated edition",
		"event-place": "New York",
		"ISBN": "978-0-19-518199-9",
		"language": "English",
		"number-of-pages": "588",
		"publisher": "Oxford University Press",
		"publisher-place": "New York",
		"source": "Amazon",
		"title": "Biophysics of Computation: Information Processing in Single Neurons",
		"title-short": "Biophysics of Computation",
		"author": [
			{
				"family": "Koch",
				"given": "Christof"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2004",
					10,
					28
				]
			]
		}
	},
	{
		"id": "hsu1988",
		"type": "paper-conference",
		"container-title": "Neural Information Processing Systems",
		"publisher": "American Institute of Physics",
		"source": "Neural Information Processing Systems",
		"title": "Experimental Demonstrations of Optical Neural Computers",
		"URL": "https://proceedings.neurips.cc/paper/1987/file/8f14e45fceea167a5a36dedd4bea2543-Paper.pdf",
		"author": [
			{
				"family": "Hsu",
				"given": "Ken"
			},
			{
				"family": "Brady",
				"given": "David"
			},
			{
				"family": "Psaltis",
				"given": "Demetri"
			}
		],
		"editor": [
			{
				"family": "Anderson",
				"given": "D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1988"
				]
			]
		}
	},
	{
		"id": "mcdonnell2011",
		"type": "article-journal",
		"abstract": "Both theoretical and experimental approaches have demonstrated that noise can improve information processing, but there is substantial scope for new biologically appropriate computational hypotheses and noise sources to be investigated. McDonnell and Ward propose a unifying framework for reconciling theory with experiment.",
		"container-title": "Nature Reviews Neuroscience",
		"DOI": "10.1038/nrn3061",
		"ISSN": "1471-0048",
		"issue": "7",
		"language": "en",
		"license": "2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.",
		"page": "415-425",
		"source": "www.nature.com",
		"title": "The benefits of noise in neural systems: bridging theory and experiment",
		"title-short": "The benefits of noise in neural systems",
		"URL": "https://www.nature.com/articles/nrn3061",
		"volume": "12",
		"author": [
			{
				"family": "McDonnell",
				"given": "Mark D."
			},
			{
				"family": "Ward",
				"given": "Lawrence M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011",
					7
				]
			]
		}
	},
	{
		"id": "faisal2008",
		"type": "article-journal",
		"abstract": "Random disturbances of signals, termed ‘noise’, pose a fundamental problem for information processing and affect all aspects of nervous-system function. However, the nature, amount and impact of noise in the nervous system have only recently been addressed in a quantitative manner. Experimental and computational methods have shown that multiple noise sources contribute to cellular and behavioural trial-to-trial variability. We review the sources of noise in the nervous system, from the molecular to the behavioural level, and show how noise contributes to trial-to-trial variability. We highlight how noise affects neuronal networks and the principles the nervous system applies to counter detrimental effects of noise, and briefly discuss noise's potential benefits.",
		"container-title": "Nature reviews. Neuroscience",
		"DOI": "10.1038/nrn2258",
		"ISSN": "1471-003X",
		"issue": "4",
		"journalAbbreviation": "Nat Rev Neurosci",
		"note": "PMID: 18319728\nPMCID: PMC2631351",
		"page": "292-303",
		"source": "PubMed Central",
		"title": "Noise in the nervous system",
		"URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631351/",
		"volume": "9",
		"author": [
			{
				"family": "Faisal",
				"given": "A. Aldo"
			},
			{
				"family": "Selen",
				"given": "Luc P. J."
			},
			{
				"family": "Wolpert",
				"given": "Daniel M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2008",
					4
				]
			]
		}
	},
	{
		"id": "sinz2019",
		"type": "article-journal",
		"abstract": "Despite enormous progress in machine learning, artificial neural networks still lag behind brains in their ability to generalize to new situations. Given identical training data, differences in generalization are caused by many defining features of a learning algorithm, such as network architecture and learning rule. Their joint effect, called “inductive bias,” determines how well any learning algorithm—or brain—generalizes: robust generalization needs good inductive biases. Artificial networks use rather nonspecific biases and often latch onto patterns that are only informative about the statistics of the training data but may not generalize to different scenarios. Brains, on the other hand, generalize across comparatively drastic changes in the sensory input all the time. We highlight some shortcomings of state-of-the-art learning algorithms compared to biological brains and discuss several ideas about how neuroscience can guide the quest for better inductive biases by providing useful constraints on representations and network architecture.",
		"container-title": "Neuron",
		"DOI": "10.1016/j.neuron.2019.08.034",
		"ISSN": "0896-6273",
		"issue": "6",
		"journalAbbreviation": "Neuron",
		"language": "en",
		"page": "967-979",
		"source": "ScienceDirect",
		"title": "Engineering a Less Artificial Intelligence",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0896627319307408",
		"volume": "103",
		"author": [
			{
				"family": "Sinz",
				"given": "Fabian H."
			},
			{
				"family": "Pitkow",
				"given": "Xaq"
			},
			{
				"family": "Reimer",
				"given": "Jacob"
			},
			{
				"family": "Bethge",
				"given": "Matthias"
			},
			{
				"family": "Tolias",
				"given": "Andreas S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					9,
					25
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/6773082/items/DE5J5NA4",
		"type": "document",
		"title": "Moravec, 1988"
	},
	{
		"id": "tenenbaum2011",
		"type": "article-journal",
		"abstract": "In coming to understand the world—in learning concepts, acquiring language, and grasping causal relations—our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?",
		"container-title": "Science",
		"DOI": "10.1126/science.1192788",
		"ISSN": "0036-8075, 1095-9203",
		"issue": "6022",
		"language": "en",
		"license": "Copyright © 2011, American Association for the Advancement of Science",
		"note": "PMID: 21393536",
		"page": "1279-1285",
		"source": "science.sciencemag.org",
		"title": "How to Grow a Mind: Statistics, Structure, and Abstraction",
		"title-short": "How to Grow a Mind",
		"URL": "https://science.sciencemag.org/content/331/6022/1279",
		"volume": "331",
		"author": [
			{
				"family": "Tenenbaum",
				"given": "Joshua B."
			},
			{
				"family": "Kemp",
				"given": "Charles"
			},
			{
				"family": "Griffiths",
				"given": "Thomas L."
			},
			{
				"family": "Goodman",
				"given": "Noah D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011",
					3,
					11
				]
			]
		}
	},
	{
		"id": "xiao-donghuang2004",
		"type": "article-journal",
		"abstract": "This paper studies modeling approach of MPEG-4 VBR video traffic based on multifractal multiplicative model. Multiscale analysis reveals that the multiplier distribution is different in style on different time scales. Based on statistical characteristics of the multipliers, a composite modeling approach is proposed: Gaussian distribution is used to fit multiplier distribution at large time scales, a new statistical distribution-Symmetric Pareto distribution to fit multiplier distribution at small time scales and a linear model to model frame traffic. Simulations are performed to validate the good effect of this approach.",
		"container-title": "IEEE Transactions on Broadcasting",
		"DOI": "10.1109/TBC.2004.834013",
		"ISSN": "1557-9611",
		"issue": "3",
		"page": "323-334",
		"source": "IEEE Xplore",
		"title": "A multiscale model for MPEG-4 varied bit rate video traffic",
		"volume": "50",
		"author": [
			{
				"literal": "Xiao-dong Huang"
			},
			{
				"literal": "Yuan-hua Zhou"
			},
			{
				"literal": "Rong-fu Zhang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2004",
					9
				]
			]
		}
	},
	{
		"id": "bozek2021",
		"type": "article-journal",
		"abstract": "From cells in tissue, to bird flocks, to human crowds, living systems display a stunning variety of collective behaviors. Yet quantifying such phenomena first requires tracking a significant fraction of the group members in natural conditions, a substantial and ongoing challenge. We present a comprehensive, computational method for tracking an entire colony of the honey bee Apis mellifera using high-resolution video on a natural honeycomb background. We adapt a convolutional neural network (CNN) segmentation architecture to automatically identify bee and brood cell positions, body orientations and within-cell states. We achieve high accuracy (~10% body width error in position, ~10° error in orientation, and true positive rate > 90%) and demonstrate months-long monitoring of sociometric colony fluctuations. These fluctuations include ~24 h cycles in the counted detections, negative correlation between bee and brood, and nightly enhancement of bees inside comb cells. We combine detected positions with visual features of organism-centered images to track individuals over time and through challenging occluding events, recovering ~79% of bee trajectories from five observation hives over 5 min timespans. The trajectories reveal important individual behaviors, including waggle dances and crawling inside comb cells. Our results provide opportunities for the quantitative study of collective bee behavior and for advancing tracking techniques of crowded systems.",
		"container-title": "Nature Communications",
		"DOI": "10.1038/s41467-021-21769-1",
		"ISSN": "2041-1723",
		"issue": "1",
		"language": "en",
		"license": "2021 The Author(s)",
		"page": "1733",
		"source": "www.nature.com",
		"title": "Markerless tracking of an entire honey bee colony",
		"URL": "https://www.nature.com/articles/s41467-021-21769-1",
		"volume": "12",
		"author": [
			{
				"family": "Bozek",
				"given": "Katarzyna"
			},
			{
				"family": "Hebert",
				"given": "Laetitia"
			},
			{
				"family": "Portugal",
				"given": "Yoann"
			},
			{
				"family": "Stephens",
				"given": "Greg J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					19
				]
			]
		}
	},
	{
		"id": "schrittwieser2020",
		"type": "article-journal",
		"abstract": "Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3—the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4—the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi—canonical environments for high-performance planning—the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.",
		"container-title": "Nature",
		"DOI": "10.1038/s41586-020-03051-4",
		"ISSN": "1476-4687",
		"issue": "7839",
		"language": "en",
		"license": "2020 The Author(s), under exclusive licence to Springer Nature Limited",
		"page": "604-609",
		"source": "www.nature.com",
		"title": "Mastering Atari, Go, chess and shogi by planning with a learned model",
		"URL": "https://www.nature.com/articles/s41586-020-03051-4",
		"volume": "588",
		"author": [
			{
				"family": "Schrittwieser",
				"given": "Julian"
			},
			{
				"family": "Antonoglou",
				"given": "Ioannis"
			},
			{
				"family": "Hubert",
				"given": "Thomas"
			},
			{
				"family": "Simonyan",
				"given": "Karen"
			},
			{
				"family": "Sifre",
				"given": "Laurent"
			},
			{
				"family": "Schmitt",
				"given": "Simon"
			},
			{
				"family": "Guez",
				"given": "Arthur"
			},
			{
				"family": "Lockhart",
				"given": "Edward"
			},
			{
				"family": "Hassabis",
				"given": "Demis"
			},
			{
				"family": "Graepel",
				"given": "Thore"
			},
			{
				"family": "Lillicrap",
				"given": "Timothy"
			},
			{
				"family": "Silver",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/6773082/items/K7CPHSWN",
		"type": "document",
		"title": "Comment: 40+32 pages"
	},
	{
		"id": "brown2020a",
		"type": "article-journal",
		"abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
		"container-title": "arXiv:2005.14165 [cs]",
		"note": "arXiv: 2005.14165",
		"source": "arXiv.org",
		"title": "Language Models are Few-Shot Learners",
		"URL": "http://arxiv.org/abs/2005.14165",
		"author": [
			{
				"family": "Brown",
				"given": "Tom B."
			},
			{
				"family": "Mann",
				"given": "Benjamin"
			},
			{
				"family": "Ryder",
				"given": "Nick"
			},
			{
				"family": "Subbiah",
				"given": "Melanie"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Neelakantan",
				"given": "Arvind"
			},
			{
				"family": "Shyam",
				"given": "Pranav"
			},
			{
				"family": "Sastry",
				"given": "Girish"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Agarwal",
				"given": "Sandhini"
			},
			{
				"family": "Herbert-Voss",
				"given": "Ariel"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Ramesh",
				"given": "Aditya"
			},
			{
				"family": "Ziegler",
				"given": "Daniel M."
			},
			{
				"family": "Wu",
				"given": "Jeffrey"
			},
			{
				"family": "Winter",
				"given": "Clemens"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Chen",
				"given": "Mark"
			},
			{
				"family": "Sigler",
				"given": "Eric"
			},
			{
				"family": "Litwin",
				"given": "Mateusz"
			},
			{
				"family": "Gray",
				"given": "Scott"
			},
			{
				"family": "Chess",
				"given": "Benjamin"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Berner",
				"given": "Christopher"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					22
				]
			]
		}
	},
	{
		"id": "mccarthy2007",
		"type": "article-journal",
		"abstract": "Human-level AI will be achieved, but new ideas are almost certainly needed, so a date cannot be reliably predicted—maybe five years, maybe five hundred years. I'd be inclined to bet on this 21st century. It is not surprising that human-level AI has proved difficult and progress has been slow—though there has been important progress. The slowness and the demand to exploit what has been discovered has led many to mistakenly redefine AI, sometimes in ways that preclude human-level AI—by relegating to humans parts of the task that human-level computer programs would have to do. In the terminology of this paper, it amounts to settling for a bounded informatic situation instead of the more general common sense informatic situation. Overcoming the “brittleness” of present AI systems and reaching human-level AI requires programs that deal with the common sense informatic situation—in which the phenomena to be taken into account in achieving a goal are not fixed in advance. We discuss reaching human-level AI, emphasizing logical AI and especially emphasizing representation problems of information and of reasoning. Ideas for reasoning in the common sense informatic situation include nonmonotonic reasoning, approximate concepts, formalized contexts and introspection.",
		"collection-title": "Special Review Issue",
		"container-title": "Artificial Intelligence",
		"DOI": "10.1016/j.artint.2007.10.009",
		"ISSN": "0004-3702",
		"issue": "18",
		"journalAbbreviation": "Artificial Intelligence",
		"language": "en",
		"page": "1174-1182",
		"source": "ScienceDirect",
		"title": "From here to human-level AI",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0004370207001476",
		"volume": "171",
		"author": [
			{
				"family": "McCarthy",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					12,
					1
				]
			]
		}
	},
	{
		"id": "singularity2008",
		"type": "report",
		"collection-title": "Spectrum",
		"language": "en",
		"publisher": "IEEE",
		"title": "Special Report: The Singularity",
		"title-short": "Special Report",
		"URL": "https://spectrum.ieee.org/static/singularity",
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2008"
				]
			]
		}
	},
	{
		"id": "gur-ari2018",
		"type": "article-journal",
		"abstract": "We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.",
		"container-title": "arXiv:1812.04754 [cs, stat]",
		"note": "arXiv: 1812.04754",
		"source": "arXiv.org",
		"title": "Gradient Descent Happens in a Tiny Subspace",
		"URL": "http://arxiv.org/abs/1812.04754",
		"author": [
			{
				"family": "Gur-Ari",
				"given": "Guy"
			},
			{
				"family": "Roberts",
				"given": "Daniel A."
			},
			{
				"family": "Dyer",
				"given": "Ethan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					12,
					11
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/6773082/items/C884FPF9",
		"type": "document",
		"title": "Comment: 9 pages + appendices, 12 figures"
	},
	{
		"id": "http://zotero.org/users/6773082/items/UD4HM3SX",
		"type": "document",
		"title": "Comment: Published at ICLR2019"
	},
	{
		"id": "dehghani2019",
		"type": "article-journal",
		"abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",
		"container-title": "arXiv:1807.03819 [cs, stat]",
		"note": "arXiv: 1807.03819",
		"source": "arXiv.org",
		"title": "Universal Transformers",
		"URL": "http://arxiv.org/abs/1807.03819",
		"author": [
			{
				"family": "Dehghani",
				"given": "Mostafa"
			},
			{
				"family": "Gouws",
				"given": "Stephan"
			},
			{
				"family": "Vinyals",
				"given": "Oriol"
			},
			{
				"family": "Uszkoreit",
				"given": "Jakob"
			},
			{
				"family": "Kaiser",
				"given": "Łukasz"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					3,
					5
				]
			]
		}
	},
	{
		"id": "papyan2019",
		"type": "article-journal",
		"abstract": "We apply state-of-the-art tools in modern high-dimensional numerical linear algebra to approximate efficiently the spectrum of the Hessian of modern deepnets, with tens of millions of parameters, trained on real data. Our results corroborate previous findings, based on small-scale networks, that the Hessian exhibits \"spiked\" behavior, with several outliers isolated from a continuous bulk. We decompose the Hessian into different components and study the dynamics with training and sample size of each term individually.",
		"container-title": "arXiv:1811.07062 [cs, stat]",
		"note": "arXiv: 1811.07062",
		"source": "arXiv.org",
		"title": "The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Training and Sample Size",
		"title-short": "The Full Spectrum of Deepnet Hessians at Scale",
		"URL": "http://arxiv.org/abs/1811.07062",
		"author": [
			{
				"family": "Papyan",
				"given": "Vardan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					6,
					2
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/6773082/items/SDSHTPET",
		"type": "document",
		"title": "Comment: 21 pages, 19 figures"
	},
	{
		"id": "ghorbani2019",
		"type": "article-journal",
		"abstract": "To understand the dynamics of optimization in deep neural networks, we develop a tool to study the evolution of the entire Hessian spectrum throughout the optimization process. Using this, we study a number of hypotheses concerning smoothness, curvature, and sharpness in the deep learning literature. We then thoroughly analyze a crucial structural feature of the spectra: in non-batch normalized networks, we observe the rapid appearance of large isolated eigenvalues in the spectrum, along with a surprising concentration of the gradient in the corresponding eigenspaces. In batch normalized networks, these two effects are almost absent. We characterize these effects, and explain how they affect optimization speed through both theory and experiments. As part of this work, we adapt advanced tools from numerical linear algebra that allow scalable and accurate estimation of the entire Hessian spectrum of ImageNet-scale neural networks; this technique may be of independent interest in other applications.",
		"container-title": "arXiv:1901.10159 [cs, stat]",
		"note": "arXiv: 1901.10159",
		"source": "arXiv.org",
		"title": "An Investigation into Neural Net Optimization via Hessian Eigenvalue Density",
		"URL": "http://arxiv.org/abs/1901.10159",
		"author": [
			{
				"family": "Ghorbani",
				"given": "Behrooz"
			},
			{
				"family": "Krishnan",
				"given": "Shankar"
			},
			{
				"family": "Xiao",
				"given": "Ying"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					1,
					29
				]
			]
		}
	},
	{
		"id": "can2020",
		"type": "paper-conference",
		"abstract": "Recurrent neural networks (RNNs) are powerful dynamical models for data with complex temporal structure. However, training RNNs has traditionally proved challenging due to exploding or vanishing of...",
		"container-title": "Mathematical and Scientific Machine Learning",
		"event-title": "Mathematical and Scientific Machine Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "476-511",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Gating creates slow modes and controls phase-space complexity in GRUs and LSTMs",
		"URL": "http://proceedings.mlr.press/v107/can20a.html",
		"author": [
			{
				"family": "Can",
				"given": "Tankut"
			},
			{
				"family": "Krishnamurthy",
				"given": "Kamesh"
			},
			{
				"family": "Schwab",
				"given": "David J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					8,
					16
				]
			]
		}
	},
	{
		"id": "krishnamurthy2021",
		"type": "article-journal",
		"abstract": "Recurrent neural networks (RNNs) are powerful dynamical models, widely used in machine learning (ML) for processing sequential data, and in neuroscience, to understand the emergent properties of networks of real neurons. Prior theoretical work in understanding the properties of RNNs has focused on networks with additive interactions. However, gating -- i.e. multiplicative -- interactions are ubiquitous in real neurons, and gating is also the central feature of the best-performing RNNs in ML. Here, we study the consequences of gating for the dynamical behavior of RNNs. We show that gating leads to slow modes and a novel, marginally-stable state. The network in this marginally-stable state can function as a robust integrator, and unlike previous approaches, gating permits this function without parameter fine-tuning or special symmetries. We study the long-time behavior of the gated network using its Lyapunov spectrum, and provide a novel relation between the maximum Lyapunov exponent and the relaxation time of the dynamics. Gating is also shown to give rise to a novel, discontinuous transition to chaos, where the proliferation of critical points (topological complexity) is decoupled from the appearance of chaotic dynamics (dynamical complexity), in contrast to a seminal result for additive RNNs. The rich dynamical behavior is summarized in a phase diagram indicating critical surfaces and regions of marginal stability -- thus, providing a map for principled parameter choices to ML practitioners. Finally, we develop a field theory for gradients that arise in training, by combining the adjoint formalism from control theory with the dynamical mean-field theory. This paves the way for the use of powerful field theoretic techniques to study training and gradients in large RNNs.",
		"container-title": "arXiv:2007.14823 [cond-mat, physics:nlin, q-bio]",
		"note": "arXiv: 2007.14823",
		"source": "arXiv.org",
		"title": "Theory of gating in recurrent neural networks",
		"URL": "http://arxiv.org/abs/2007.14823",
		"author": [
			{
				"family": "Krishnamurthy",
				"given": "Kamesh"
			},
			{
				"family": "Can",
				"given": "Tankut"
			},
			{
				"family": "Schwab",
				"given": "David J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					1,
					20
				]
			]
		}
	},
	{
		"id": "sompolinsky1982",
		"type": "article-journal",
		"abstract": "Langevin equations for the relaxation of spin fluctuations in a soft-spin version of the Edwards-Anderson model are used as a starting point for the study of the dynamic and static properties of spin-glasses. An exact uniform Lagrangian for the average dynamic correlation and response functions is derived for arbitrary range of random exchange, using a functional-integral method proposed by De Dominicis. The properties of the Lagrangian are studied in the mean-field limit which is realized by considering an infinite-ranged random exchange. In this limit, the dynamics are represented by a stochastic equation of motion of a single spin with self-consistent (bare) propagator and Gaussian noise. The low-frequency and the static properties of this equation are studied both above and below Tc. Approaching Tc from above, spin fluctuations slow down with a relaxation time proportional to |T−Tc|−1 whereas at Tc the damping function vanishes as ω12. We derive a criterion for dynamic stability below Tc. It is shown that a stable solution necessarily violates the fluctuation-dissipation theorem below Tc. Consequently, the spin-glass order parameters are the time-persistent terms which appear in both the spin correlations and the local response. This is shown to invalidate the treatment of the spin-glass order parameters as purely static quantities. Instead, one has to specify the manner in which they relax in a finite system, along time scales which diverge in the thermodynamic limit. We show that the finite-time correlations decay algebraically with time as t−ν at all temperatures below Tc, with a temperature-dependent exponent ν. Near Tc, ν is given (in the Ising case) as ν(T)∼12−π−1(1−TTc)+σ(1−TTc)2. A tentative calculation of ν at T=0 K is presented. We briefly discuss the physical origin of the violation of the fluctuation-dissipation theorem.",
		"container-title": "Physical Review B",
		"DOI": "10.1103/PhysRevB.25.6860",
		"issue": "11",
		"journalAbbreviation": "Phys. Rev. B",
		"note": "publisher: American Physical Society",
		"page": "6860-6875",
		"source": "APS",
		"title": "Relaxational dynamics of the Edwards-Anderson model and the mean-field theory of spin-glasses",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevB.25.6860",
		"volume": "25",
		"author": [
			{
				"family": "Sompolinsky",
				"given": "H."
			},
			{
				"family": "Zippelius",
				"given": "Annette"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1982",
					6,
					1
				]
			]
		}
	},
	{
		"id": "sompolinsky1981",
		"type": "article-journal",
		"abstract": "Exact averaged equations for spin relaxation in infinite-ranged spin-glasses are derived. It is shown that, in general, time-persistent terms in both correlation and response functions cannot be treated as purely static order parameters. A dynamic stability criterion for static solutions is derived. In the marginally stable solution dynamic correlations decay algebraically ∼t−ν below Tc, with an exponent which decreases continuously with temperature, ν(T)=12−π−1(1−TTc)+O((1−TTc)2), in disagreement with previous predictions (ν=12).",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.47.359",
		"issue": "5",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"note": "publisher: American Physical Society",
		"page": "359-362",
		"source": "APS",
		"title": "Dynamic Theory of the Spin-Glass Phase",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevLett.47.359",
		"volume": "47",
		"author": [
			{
				"family": "Sompolinsky",
				"given": "H."
			},
			{
				"family": "Zippelius",
				"given": "Annette"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1981",
					8,
					3
				]
			]
		}
	},
	{
		"id": "schuecker2018",
		"type": "article-journal",
		"abstract": "Autonomous, randomly coupled, neural networks display a transition to chaos at a critical coupling strength. Here, we investigate the effect of a time-varying input on the onset of chaos and the resulting consequences for information processing. Dynamic mean-field theory yields the statistics of the activity, the maximum Lyapunov exponent, and the memory capacity of the network. We find an exact condition that determines the transition from stable to chaotic dynamics and the sequential memory capacity in closed form. The input suppresses chaos by a dynamic mechanism, shifting the transition to significantly larger coupling strengths than predicted by local stability analysis. Beyond linear stability, a regime of coexistent locally expansive but nonchaotic dynamics emerges that optimizes the capacity of the network to store sequential input.",
		"container-title": "Physical Review X",
		"DOI": "10.1103/PhysRevX.8.041029",
		"issue": "4",
		"journalAbbreviation": "Phys. Rev. X",
		"note": "publisher: American Physical Society",
		"page": "041029",
		"source": "APS",
		"title": "Optimal Sequence Memory in Driven Random Networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevX.8.041029",
		"volume": "8",
		"author": [
			{
				"family": "Schuecker",
				"given": "Jannis"
			},
			{
				"family": "Goedeke",
				"given": "Sven"
			},
			{
				"family": "Helias",
				"given": "Moritz"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					11,
					14
				]
			]
		}
	},
	{
		"id": "chow2015",
		"type": "article-journal",
		"abstract": "Stochastic differential equations (SDEs) have multiple applications in mathematical neuroscience and are notoriously difficult. Here, we give a self-contained pedagogical review of perturbative field theoretic and path integral methods to calculate moments of the probability density function of SDEs. The methods can be extended to high dimensional systems such as networks of coupled neurons and even deterministic systems with quenched disorder.",
		"container-title": "The Journal of Mathematical Neuroscience (JMN)",
		"DOI": "10.1186/s13408-015-0018-5",
		"ISSN": "2190-8567",
		"issue": "1",
		"journalAbbreviation": "J. Math. Neurosc.",
		"language": "en",
		"page": "8",
		"source": "Springer Link",
		"title": "Path Integral Methods for Stochastic Differential Equations",
		"URL": "https://doi.org/10.1186/s13408-015-0018-5",
		"volume": "5",
		"author": [
			{
				"family": "Chow",
				"given": "Carson C."
			},
			{
				"family": "Buice",
				"given": "Michael A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					3,
					24
				]
			]
		}
	},
	{
		"id": "tracy1996",
		"type": "article-journal",
		"abstract": "The focus of this paper is on the probability, $E_\\beta(0;J)$, that a set $J$ consisting of a finite union of intervals contains no eigenvalues for the finite $N$ Gaussian Orthogonal ($\\beta=1$) and Gaussian Symplectic ($\\beta=4$) Ensembles and their respective scaling limits both in the bulk and at the edge of the spectrum. We show how these probabilities can be expressed in terms of quantities arising in the corresponding unitary ($\\beta=2$) ensembles. Our most explicit new results concern the distribution of the largest eigenvalue in each of these ensembles. In the edge scaling limit we show that these largest eigenvalue distributions are given in terms of a particular Painlev\\'e II function.",
		"container-title": "Communications in Mathematical Physics",
		"DOI": "10.1007/BF02099545",
		"ISSN": "0010-3616, 1432-0916",
		"issue": "3",
		"journalAbbreviation": "Commun.Math. Phys.",
		"note": "arXiv: solv-int/9509007",
		"page": "727-754",
		"source": "arXiv.org",
		"title": "On Orthogonal and Symplectic Matrix Ensembles",
		"URL": "http://arxiv.org/abs/solv-int/9509007",
		"volume": "177",
		"author": [
			{
				"family": "Tracy",
				"given": "Craig A."
			},
			{
				"family": "Widom",
				"given": "Harold"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1996",
					4
				]
			]
		}
	},
	{
		"id": "martin1973",
		"type": "article-journal",
		"abstract": "The statistical dynamics of a classical random variable that satisfies a nonlinear equation of motion is recast in terms of closed self-consistent equations in which only the observable correlations at pairs of points and the exact response to infinitesimal disturbances appear. The self-consistent equations are developed by introducing a second field that does not commute with the random variable. Techniques used in the study of the interacting quantum fields can then be employed, and systematic approximations can be obtained. It is also possible to carry out a \"charge normalization\" eliminating the nonlinear coupling in favor of a dimensionless parameter which measures the deviation from Gaussian behavior. No assumptions of spatial or time homogeneity or of small deviation from equilibrium enter. It is shown that previously inferred renormalization schemes for homogeneous systems were incomplete or erroneous. The application of the method to classical microscopic systems, where it leads from first principles to a coupled-mode description is briefly indicated.",
		"container-title": "Physical Review A",
		"DOI": "10.1103/PhysRevA.8.423",
		"issue": "1",
		"journalAbbreviation": "Phys. Rev. A",
		"note": "publisher: American Physical Society",
		"page": "423-437",
		"source": "APS",
		"title": "Statistical Dynamics of Classical Systems",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevA.8.423",
		"volume": "8",
		"author": [
			{
				"family": "Martin",
				"given": "P. C."
			},
			{
				"family": "Siggia",
				"given": "E. D."
			},
			{
				"family": "Rose",
				"given": "H. A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1973",
					7,
					1
				]
			]
		}
	},
	{
		"id": "martin1973a",
		"type": "article-journal",
		"abstract": "The statistical dynamics of a classical random variable that satisfies a nonlinear equation of motion is recast in terms of closed self-consistent equations in which only the observable correlations at pairs of points and the exact response to infinitesimal disturbances appear. The self-consistent equations are developed by introducing a second field that does not commute with the random variable. Techniques used in the study of the interacting quantum fields can then be employed, and systematic approximations can be obtained. It is also possible to carry out a \"charge normalization\" eliminating the nonlinear coupling in favor of a dimensionless parameter which measures the deviation from Gaussian behavior. No assumptions of spatial or time homogeneity or of small deviation from equilibrium enter. It is shown that previously inferred renormalization schemes for homogeneous systems were incomplete or erroneous. The application of the method to classical microscopic systems, where it leads from first principles to a coupled-mode description is briefly indicated.",
		"container-title": "Physical Review A",
		"DOI": "10.1103/PhysRevA.8.423",
		"issue": "1",
		"journalAbbreviation": "Phys. Rev. A",
		"note": "publisher: American Physical Society",
		"page": "423-437",
		"source": "APS",
		"title": "Statistical Dynamics of Classical Systems",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevA.8.423",
		"volume": "8",
		"author": [
			{
				"family": "Martin",
				"given": "P. C."
			},
			{
				"family": "Siggia",
				"given": "E. D."
			},
			{
				"family": "Rose",
				"given": "H. A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1973",
					7,
					1
				]
			]
		}
	},
	{
		"id": "dedominicis1978",
		"type": "article-journal",
		"container-title": "Physical Review B",
		"DOI": "10.1103/PhysRevB.18.353",
		"ISSN": "0163-1829",
		"issue": "1",
		"journalAbbreviation": "Phys. Rev. B",
		"language": "en",
		"page": "353-376",
		"source": "DOI.org (Crossref)",
		"title": "Field-theory renormalization and critical dynamics above T c : Helium, antiferromagnets, and liquid-gas systems",
		"title-short": "Field-theory renormalization and critical dynamics above T c",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevB.18.353",
		"volume": "18",
		"author": [
			{
				"family": "De Dominicis",
				"given": "C."
			},
			{
				"family": "Peliti",
				"given": "L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1978",
					7,
					1
				]
			]
		}
	},
	{
		"id": "schuecker2016",
		"type": "article-journal",
		"abstract": "Neural networks of the brain form one of the most complex systems we know. Many qualitative features of the emerging collective phenomena, such as correlated activity, stability, response to inputs, chaotic and regular behavior, can, however, be understood in simple models that are accessible to a treatment in statistical mechanics, or, more precisely, classical statistical field theory. This tutorial presents the fundamentals behind contemporary developments in the theory of neural networks of rate units that are based on methods from statistical mechanics of classical systems with a large number of interacting degrees of freedom. In particular we will focus on a relevant class of systems that have quenched (time independent) disorder. In neural networks, the main source of disorder arises from random synaptic couplings between neurons. These systems are in many respects similar to spin glasses. The tutorial therefore also explains the methods for these disordered systems as far as they are applied in neuroscience. The presentation consists of two parts. In the first part we introduce stochastic differential equations in the Martin - Siggia - Rose - De Dominicis - Janssen path integral formalism. In the second part we employ this language to derive the dynamic mean-field theory for deterministic random networks, the basis of the seminal work by Sompolinsky, Crisanti, Sommers 1988, as well as a recent extension to stochastic dynamics.",
		"container-title": "arXiv:1605.06758 [cond-mat, q-bio]",
		"note": "arXiv: 1605.06758",
		"source": "arXiv.org",
		"title": "Functional methods for disordered neural networks",
		"URL": "http://arxiv.org/abs/1605.06758",
		"author": [
			{
				"family": "Schuecker",
				"given": "Jannis"
			},
			{
				"family": "Goedeke",
				"given": "Sven"
			},
			{
				"family": "Dahmen",
				"given": "David"
			},
			{
				"family": "Helias",
				"given": "Moritz"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					6,
					15
				]
			]
		}
	},
	{
		"id": "stamp2015",
		"type": "report",
		"collection-title": "PHYS508",
		"event-place": "Vancouver",
		"genre": "Lecture Notes",
		"publisher": "University of British Columbia",
		"publisher-place": "Vancouver",
		"title": "B1-2016.pdf",
		"URL": "https://phas.ubc.ca/~stamp/TEACHING/PHYS508/NOTES/B1-2016.pdf",
		"author": [
			{
				"family": "Stamp",
				"given": "P. C. E."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "aradi2002",
		"type": "article-journal",
		"abstract": "Interneurones are important regulators of neuronal networks. The conventional approach to interneurones is to focus on the mean values of various parameters. Here we tested the hypothesis that changes in the variance of interneuronal properties (e.g. in the degree of scattering of parameter values of individual cells around the population mean) may modify the behaviour of networks. Biophysically based multicompartmental models of principal cells and interneurones showed that changes in the variance in the electrophysiological and anatomical properties of interneurones significantly alter the input-output functions, rhythmicity and synchrony of principal cells, even if the mean values were unchanged. In most cases, increased heterogeneity in interneurones resulted in stronger inhibition of principal cell firing; however, there were parameter ranges where increased interneuronal variance decreased the inhibition of principal cells. Electrophysiological recordings showed that the variance in the resting membrane potential of CA1 stratum oriens interneurones persistently increased following experimental complex febrile seizures in developing rats, without a change in the mean resting membrane potential, indicating that lasting alterations in interneuronal heterogeneity can take place in real neuronal systems. These computational and experimental data demonstrate that modifications in interneuronal population variance influence the behaviour of neuronal networks, and suggest a physiological role for interneuronal diversity. Furthermore, the results indicate that interneuronal heterogeneity can change in neurological diseases, and raise the possibility that neuromodulators may act by regulating the variance of key parameters in interneuronal populations.",
		"container-title": "The Journal of Physiology",
		"DOI": "https://doi.org/10.1113/jphysiol.2001.013054",
		"ISSN": "1469-7793",
		"issue": "1",
		"language": "en",
		"license": "© 2002 The Journal of Physiology © 2002 The Physiological Society",
		"note": "_eprint: https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.2001.013054",
		"page": "227-251",
		"source": "Wiley Online Library",
		"title": "Modulation of network behaviour by changes in variance in interneuronal properties",
		"URL": "https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.2001.013054",
		"volume": "538",
		"author": [
			{
				"family": "Aradi",
				"given": "I."
			},
			{
				"family": "Soltesz",
				"given": "I."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					3,
					31
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2002"
				]
			]
		}
	},
	{
		"id": "adler2007",
		"type": "book",
		"call-number": "QA274.45 .A345 2007",
		"collection-number": "115",
		"collection-title": "Springer monographs in mathematics",
		"event-place": "New York",
		"ISBN": "978-0-387-48112-8",
		"language": "en",
		"number-of-pages": "448",
		"publisher": "Springer",
		"publisher-place": "New York",
		"source": "Library of Congress ISBN",
		"title": "Random fields and geometry",
		"author": [
			{
				"family": "Adler",
				"given": "Robert J."
			},
			{
				"family": "Taylor",
				"given": "Jonathan E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2007"
				]
			]
		}
	},
	{
		"id": "hoover1994",
		"type": "article-journal",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.49.1913",
		"ISSN": "1063-651X, 1095-3787",
		"issue": "3",
		"journalAbbreviation": "Phys. Rev. E",
		"language": "en",
		"page": "1913-1920",
		"source": "DOI.org (Crossref)",
		"title": "Second-law irreversibility and phase-space dimensionality loss from time-reversible nonequilibrium steady-state Lyapunov spectra",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.49.1913",
		"volume": "49",
		"author": [
			{
				"family": "Hoover",
				"given": "W. G."
			},
			{
				"family": "Posch",
				"given": "H. A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1994",
					3,
					1
				]
			]
		}
	},
	{
		"id": "daems1999",
		"type": "article-journal",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.59.4000",
		"ISSN": "1063-651X, 1095-3787",
		"issue": "4",
		"journalAbbreviation": "Phys. Rev. E",
		"language": "en",
		"page": "4000-4006",
		"source": "DOI.org (Crossref)",
		"title": "Entropy production and phase space volume contraction",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.59.4000",
		"volume": "59",
		"author": [
			{
				"family": "Daems",
				"given": "D."
			},
			{
				"family": "Nicolis",
				"given": "G."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1999",
					4,
					1
				]
			]
		}
	},
	{
		"id": "vanhandel2017",
		"type": "article-journal",
		"abstract": "Let X be a d × d symmetric random matrix with independent but non-identically distributed Gaussian entries. It has been conjectured by Latala that the spectral norm of X is always of the same order as the largest Euclidean norm of its rows. A positive resolution of this conjecture would provide a sharp understanding of the probabilistic mechanisms that control the spectral norm of inhomogeneous Gaussian random ma√trices. This paper establishes the conjecture up to a dimensional factor of order log log d. Moreover, dimensionfree bounds are developed that are optimal to leading order and that establish the conjecture in special cases. The proofs of these results shed signiﬁcant light on the geometry of the underlying Gaussian processes.",
		"container-title": "Transactions of the American Mathematical Society",
		"DOI": "10.1090/tran/6922",
		"ISSN": "0002-9947, 1088-6850",
		"issue": "11",
		"journalAbbreviation": "Trans. Amer. Math. Soc.",
		"language": "en",
		"page": "8161-8178",
		"source": "DOI.org (Crossref)",
		"title": "On the spectral norm of Gaussian random matrices",
		"URL": "https://www.ams.org/tran/2017-369-11/S0002-9947-2017-06922-1/",
		"volume": "369",
		"author": [
			{
				"family": "Handel",
				"given": "Ramon",
				"non-dropping-particle": "van"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					5,
					30
				]
			]
		}
	},
	{
		"id": "seginer2000",
		"type": "article-journal",
		"abstract": "We compare the Euclidean operator norm of a random matrix with the Euclidean norm of \nits rows and columns. In the first part of this paper, we show that if A is a random matrix \nwith i.i.d. zero mean entries, then \nE∥A∥h [les     ] Kh\n(E maxi\n∥ai[bull  ]\n∥h + E maxj\n∥aj[bull  ]\n∥h), where K is \na constant which does not depend on the dimensions or distribution of \nA (h, however, does \ndepend on the dimensions). In the second part we drop the assumption that the entries \nof A are i.i.d. We therefore consider the Euclidean operator \nnorm of a random matrix, A, \nobtained from a (non-random) matrix by randomizing the signs of the matrix's entries. \nWe show that in this case, the best inequality possible (up to a multiplicative constant) is \nE∥A∥h [les     ] (c log1/4 min \n{m, n})h\n(E maxi\n∥ai[bull  ]\n∥h + E maxj\n∥aj[bull  ]\n∥h) (m, n the dimensions of the \nmatrix and c a constant independent of m, n).",
		"container-title": "Combinatorics, Probability and Computing",
		"DOI": "10.1017/S096354830000420X",
		"ISSN": "1469-2163, 0963-5483",
		"issue": "2",
		"language": "en",
		"note": "publisher: Cambridge University Press",
		"page": "149-166",
		"source": "Cambridge University Press",
		"title": "The Expected Norm of Random Matrices",
		"URL": "https://www.cambridge.org/core/journals/combinatorics-probability-and-computing/article/expected-norm-of-random-matrices/04DDADDE1805480F8FDCD2DC379CA0AE",
		"volume": "9",
		"author": [
			{
				"family": "Seginer",
				"given": "Yoav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2000",
					3
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/6773082/items/YQNDESIV",
		"type": "document",
		"title": "ADA576100.pdf",
		"URL": "https://apps.dtic.mil/sti/pdfs/ADA576100.pdf",
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					5
				]
			]
		}
	},
	{
		"id": "zotero-619",
		"type": "webpage",
		"abstract": "Home    Computational Random Matrix TheoryBy Robert Sweeney BlancoThis is an expository textbook for the field of Random Matrix Theory. In addition t...",
		"language": "en",
		"title": "Home",
		"URL": "/Computational_Random_Matrix_Theory/YOUR%20URL/Computational_Random_Matrix_Theory/Home/Home.html",
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					5
				]
			]
		}
	},
	{
		"id": "forrester2007",
		"type": "article-journal",
		"abstract": "The real Ginibre ensemble consists of random $N \\times N$ matrices formed from i.i.d. standard Gaussian entries. By using the method of skew orthogonal polynomials, the general $n$-point correlations for the real eigenvalues, and for the complex eigenvalues, are given as $n \\times n$ Pfaffians with explicit entries. A computationally tractable formula for the cumulative probability density of the largest real eigenvalue is presented. This is relevant to May's stability analysis of biological webs.",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.99.050603",
		"ISSN": "0031-9007, 1079-7114",
		"issue": "5",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"note": "arXiv: 0706.2020",
		"page": "050603",
		"source": "arXiv.org",
		"title": "Eigenvalue statistics of the real Ginibre ensemble",
		"URL": "http://arxiv.org/abs/0706.2020",
		"volume": "99",
		"author": [
			{
				"family": "Forrester",
				"given": "Peter J."
			},
			{
				"family": "Nagao",
				"given": "Taro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					8,
					3
				]
			]
		}
	},
	{
		"id": "baik2020",
		"type": "article-journal",
		"abstract": "The real Ginibre ensemble consists of n × n real matrices X whose entries are i.i.d. standard normal random variables. In sharp contrast to the complex and quaternion Ginibre ensemble, real eigenvalues in the real Ginibre ensemble attain positive likelihood. In turn, the spectral radius Rn = max1≤j≤n |zj (X)| of the eigenvalues zj (X) ∈ C of a real Ginibre matrix X follows a diﬀerent limiting law (as n → ∞) for zj (X) ∈ R than for zj (X) ∈ C \\ R. Building on previous work by Rider, Sinclair [28] and Poplavskyi, Tribe, Zaboronski [27], we show that the limiting distribution of maxj:zj ∈R zj (X) admits a closed form expression in terms of a distinguished solution to an inverse scattering problem for the Zakharov-Shabat system. As byproducts of our analysis we also obtain a new determinantal representation for the limiting distribution of maxj:zj ∈R zj (X) and extend recent tail estimates in [27] via nonlinear steepest descent techniques.",
		"container-title": "The Annals of Applied Probability",
		"DOI": "10.1214/19-AAP1509",
		"ISSN": "1050-5164",
		"issue": "1",
		"journalAbbreviation": "Ann. Appl. Probab.",
		"language": "en",
		"source": "DOI.org (Crossref)",
		"title": "The largest real eigenvalue in the real Ginibre ensemble and its relation to the Zakharov–Shabat system",
		"URL": "https://projecteuclid.org/journals/annals-of-applied-probability/volume-30/issue-1/The-largest-real-eigenvalue-in-the-real-Ginibre-ensemble-and/10.1214/19-AAP1509.full",
		"volume": "30",
		"author": [
			{
				"family": "Baik",
				"given": "Jinho"
			},
			{
				"family": "Bothner",
				"given": "Thomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					2,
					1
				]
			]
		}
	},
	{
		"id": "rider2014",
		"type": "article-journal",
		"abstract": "The real Ginibre ensemble refers to the family of $n\\times n$ matrices in which each entry is an independent Gaussian random variable of mean zero and variance one. Our main result is that the appropriately scaled spectral radius converges in law to a Gumbel distribution as $n\\rightarrow\\infty$. This fact has been known to hold in the complex and quaternion analogues of the ensemble for some time, with simpler proofs. Along the way we establish a new form for the limit law of the largest real eigenvalue.",
		"container-title": "The Annals of Applied Probability",
		"DOI": "10.1214/13-AAP958",
		"ISSN": "1050-5164, 2168-8737",
		"issue": "4",
		"note": "publisher: Institute of Mathematical Statistics",
		"page": "1621-1651",
		"source": "Project Euclid",
		"title": "Extremal laws for the real Ginibre ensemble",
		"URL": "https://projecteuclid.org/journals/annals-of-applied-probability/volume-24/issue-4/Extremal-laws-for-the-real-Ginibre-ensemble/10.1214/13-AAP958.full",
		"volume": "24",
		"author": [
			{
				"family": "Rider",
				"given": "Brian"
			},
			{
				"family": "Sinclair",
				"given": "Christopher D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					8
				]
			]
		}
	},
	{
		"id": "zotero-630",
		"type": "webpage",
		"title": "Health Curious | Robin Laird",
		"URL": "http://localhost:3000/coach/D247AE293975FC917AB83612A61FF77B9D3945B7EA8575BFBE47EA3F7C99DABF/",
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					6
				]
			]
		}
	},
	{
		"id": "monteforte2010",
		"type": "article-journal",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.105.268104",
		"ISSN": "0031-9007, 1079-7114",
		"issue": "26",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"language": "en",
		"page": "268104",
		"source": "DOI.org (Crossref)",
		"title": "Dynamical Entropy Production in Spiking Neuron Networks in the Balanced State",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevLett.105.268104",
		"volume": "105",
		"author": [
			{
				"family": "Monteforte",
				"given": "Michael"
			},
			{
				"family": "Wolf",
				"given": "Fred"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					6
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2010",
					12,
					30
				]
			]
		}
	},
	{
		"id": "stern2014",
		"type": "article-journal",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.90.062710",
		"ISSN": "1539-3755, 1550-2376",
		"issue": "6",
		"journalAbbreviation": "Phys. Rev. E",
		"language": "en",
		"page": "062710",
		"source": "DOI.org (Crossref)",
		"title": "Dynamics of random neural networks with bistable units",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.90.062710",
		"volume": "90",
		"author": [
			{
				"family": "Stern",
				"given": "M."
			},
			{
				"family": "Sompolinsky",
				"given": "H."
			},
			{
				"family": "Abbott",
				"given": "L. F."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					6
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					12,
					16
				]
			]
		}
	},
	{
		"id": "martin1973b",
		"type": "article-journal",
		"container-title": "Physical Review A",
		"DOI": "10.1103/PhysRevA.8.423",
		"ISSN": "0556-2791",
		"issue": "1",
		"journalAbbreviation": "Phys. Rev. A",
		"language": "en",
		"page": "423-437",
		"source": "DOI.org (Crossref)",
		"title": "Statistical Dynamics of Classical Systems",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevA.8.423",
		"volume": "8",
		"author": [
			{
				"family": "Martin",
				"given": "P. C."
			},
			{
				"family": "Siggia",
				"given": "E. D."
			},
			{
				"family": "Rose",
				"given": "H. A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					13
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1973",
					7,
					1
				]
			]
		}
	},
	{
		"id": "sompolinsky1982a",
		"type": "article-journal",
		"container-title": "Physical Review B",
		"DOI": "10.1103/PhysRevB.25.6860",
		"ISSN": "0163-1829",
		"issue": "11",
		"journalAbbreviation": "Phys. Rev. B",
		"language": "en",
		"page": "6860-6875",
		"source": "DOI.org (Crossref)",
		"title": "Relaxational dynamics of the Edwards-Anderson model and the mean-field theory of spin-glasses",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevB.25.6860",
		"volume": "25",
		"author": [
			{
				"family": "Sompolinsky",
				"given": "H."
			},
			{
				"family": "Zippelius",
				"given": "Annette"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					13
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1982",
					6,
					1
				]
			]
		}
	},
	{
		"id": "orourke2015",
		"type": "article-journal",
		"abstract": "For fixed $$m > 1$$, we study the product of $$m$$independent $$N \\times N$$elliptic random matrices as $$N$$tends to infinity. Our main result shows that the empirical spectral distribution of the product converges, with probability $$1$$, to the $$m$$-th power of the circular law, regardless of the joint distribution of the mirror entries in each matrix. This leads to a new kind of universality phenomenon: the limit law for the product of independent random matrices is independent of the limit laws for the individual matrices themselves. Our result also generalizes earlier results of Götze–Tikhomirov (On the asymptotic spectrum of products of independent random matrices, available at http://arxiv.org/abs/1012.2710) and O’Rourke–Soshnikov (J Probab 16(81):2219–2245, 2011) concerning the product of independent iid random matrices.",
		"container-title": "Journal of Statistical Physics",
		"DOI": "10.1007/s10955-015-1246-5",
		"ISSN": "1572-9613",
		"issue": "1",
		"journalAbbreviation": "J Stat Phys",
		"language": "en",
		"page": "89-119",
		"source": "Springer Link",
		"title": "Products of Independent Elliptic Random Matrices",
		"URL": "https://doi.org/10.1007/s10955-015-1246-5",
		"volume": "160",
		"author": [
			{
				"family": "O’Rourke",
				"given": "Sean"
			},
			{
				"family": "Renfrew",
				"given": "David"
			},
			{
				"family": "Soshnikov",
				"given": "Alexander"
			},
			{
				"family": "Vu",
				"given": "Van"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					7,
					1
				]
			]
		}
	},
	{
		"id": "arous2020",
		"type": "article-journal",
		"abstract": "We consider a nonlinear autonomous system of $N\\gg 1$ degrees of freedom randomly coupled by both relaxational ('gradient') and non-relaxational ('solenoidal') random interactions. We show that with increased interaction strength such systems generically undergo an abrupt transition from a trivial phase portrait with a single stable equilibrium into a topologically non-trivial regime of 'absolute instability' where equilibria are on average exponentially abundant, but typically all of them are unstable, unless the dynamics is purely gradient. When interactions increase even further the stable equilibria eventually become on average exponentially abundant unless the interaction is purely solenoidal. We further calculate the mean proportion of equilibria which have a fixed fraction of unstable directions.",
		"container-title": "arXiv:2008.00690 [cond-mat, physics:math-ph, physics:nlin]",
		"note": "arXiv: 2008.00690",
		"source": "arXiv.org",
		"title": "Counting equilibria of large complex systems by instability index",
		"URL": "http://arxiv.org/abs/2008.00690",
		"author": [
			{
				"family": "Arous",
				"given": "Gérard Ben"
			},
			{
				"family": "Fyodorov",
				"given": "Yan V."
			},
			{
				"family": "Khoruzhenko",
				"given": "Boris A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					8,
					3
				]
			]
		}
	},
	{
		"id": "mora2011",
		"type": "article-journal",
		"abstract": "Many of life’s most fascinating phenomena emerge from interactions among many elements—many amino acids determine the structure of a single protein, many genes determine the fate of a cell, many neurons are involved in shaping our thoughts and memories. Physicists have long hoped that these collective behaviors could be described using the ideas and methods of statistical mechanics. In the past few years, new, larger scale experiments have made it possible to construct statistical mechanics models of biological systems directly from real data. We review the surprising successes of this “inverse” approach, using examples from families of proteins, networks of neurons, and flocks of birds. Remarkably, in all these cases the models that emerge from the data are poised near a very special point in their parameter space—a critical point. This suggests there may be some deeper theoretical principle behind the behavior of these diverse systems.",
		"container-title": "Journal of Statistical Physics",
		"DOI": "10.1007/s10955-011-0229-4",
		"ISSN": "1572-9613",
		"issue": "2",
		"journalAbbreviation": "J Stat Phys",
		"language": "en",
		"page": "268-302",
		"source": "Springer Link",
		"title": "Are Biological Systems Poised at Criticality?",
		"URL": "https://doi.org/10.1007/s10955-011-0229-4",
		"volume": "144",
		"author": [
			{
				"family": "Mora",
				"given": "Thierry"
			},
			{
				"family": "Bialek",
				"given": "William"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011",
					7,
					1
				]
			]
		}
	},
	{
		"id": "beggs2012",
		"type": "article-journal",
		"abstract": "Relatively recent work has reported that networks of neurons can produce avalanches of activity whose sizes follow a power law distribution. This suggests that these networks may be operating near a critical point, poised between a phase where activity rapidly dies out and a phase where activity is amplified over time. The hypothesis that the electrical activity of neural networks in the brain is critical is potentially important, as many simulations suggest that information processing functions would be optimized at the critical point. This hypothesis, however, is still controversial. Here we will explain the concept of criticality and review the substantial objections to the criticality hypothesis raised by skeptics. Points and counter points are presented in dialogue form.",
		"container-title": "Frontiers in Physiology",
		"DOI": "10.3389/fphys.2012.00163",
		"ISSN": "1664-042X",
		"journalAbbreviation": "Front. Physiol.",
		"language": "English",
		"note": "publisher: Frontiers",
		"source": "Frontiers",
		"title": "Being Critical of Criticality in the Brain",
		"URL": "https://www.frontiersin.org/articles/10.3389/fphys.2012.00163/full",
		"volume": "3",
		"author": [
			{
				"family": "Beggs",
				"given": "John M."
			},
			{
				"family": "Timme",
				"given": "Nicholas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "cuntz2012",
		"type": "article-journal",
		"abstract": "The wide diversity of dendritic trees is one of the most striking features of neural circuits. Here we develop a general quantitative theory relating the total length of dendritic wiring to the number of branch points and synapses. We show that optimal wiring predicts a 2/3 power law between these measures. We demonstrate that the theory is consistent with data from a wide variety of neurons across many different species and helps define the computational compartments in dendritic trees. Our results imply fundamentally distinct design principles for dendritic arbors compared with vascular, bronchial, and botanical trees.",
		"container-title": "Proceedings of the National Academy of Sciences",
		"DOI": "10.1073/pnas.1200430109",
		"ISSN": "0027-8424, 1091-6490",
		"issue": "27",
		"journalAbbreviation": "PNAS",
		"language": "en",
		"note": "publisher: National Academy of Sciences\nsection: Biological Sciences\nPMID: 22715290",
		"page": "11014-11018",
		"source": "www.pnas.org",
		"title": "A scaling law derived from optimal dendritic wiring",
		"URL": "https://www.pnas.org/content/109/27/11014",
		"volume": "109",
		"author": [
			{
				"family": "Cuntz",
				"given": "Hermann"
			},
			{
				"family": "Mathy",
				"given": "Alexandre"
			},
			{
				"family": "Häusser",
				"given": "Michael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					7,
					3
				]
			]
		}
	},
	{
		"id": "katz2009",
		"type": "article-journal",
		"abstract": "Competing models have been proposed to explain how neurons integrate the thousands of inputs distributed throughout their dendritic trees. In a simple global integration model, inputs from all locations sum in the axon. In a two-stage integration model, inputs contribute directly to dendritic spikes, and outputs from multiple branches sum in the axon. These two models yield opposite predictions of how synapses at different dendritic locations should be scaled if they are to contribute equally to neuronal output. We used serial-section electron microscopy to reconstruct individual apical oblique dendritic branches of CA1 pyramidal neurons and observe a synapse distribution consistent with the two-stage integration model. Computational modeling suggests that the observed synapse distribution enhances the contribution of each dendritic branch to neuronal output.",
		"container-title": "Neuron",
		"DOI": "10.1016/j.neuron.2009.06.023",
		"ISSN": "0896-6273",
		"issue": "2",
		"journalAbbreviation": "Neuron",
		"note": "PMID: 19640476\nPMCID: PMC2921807",
		"page": "171-177",
		"source": "PubMed Central",
		"title": "Synapse Distribution Suggests a Two-Stage Model of Dendritic Integration in CA1 Pyramidal Neurons",
		"URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2921807/",
		"volume": "63",
		"author": [
			{
				"family": "Katz",
				"given": "Yael"
			},
			{
				"family": "Menon",
				"given": "Vilas"
			},
			{
				"family": "Nicholson",
				"given": "Daniel A."
			},
			{
				"family": "Geinisman",
				"given": "Yuri"
			},
			{
				"family": "Kath",
				"given": "William L."
			},
			{
				"family": "Spruston",
				"given": "Nelson"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009",
					7,
					30
				]
			]
		}
	},
	{
		"id": "barbour2007",
		"type": "article-journal",
		"abstract": "Much research effort into synaptic plasticity has been motivated by the idea that modifications of synaptic weights (or strengths or efficacies) underlie learning and memory. Here, we examine the possibility of exploiting the statistics of experimentally measured synaptic weights to deduce information about the learning process. Analysing distributions of synaptic weights requires a theoretical framework to interpret the experimental measurements, but the results can be unexpectedly powerful, yielding strong constraints on possible learning theories as well as information that is difficult to obtain by other means, such as the information storage capacity of a cell. We review the available experimental and theoretical techniques as well as important open issues.",
		"container-title": "Trends in Neurosciences",
		"DOI": "10.1016/j.tins.2007.09.005",
		"ISSN": "0166-2236",
		"issue": "12",
		"journalAbbreviation": "Trends in Neurosciences",
		"language": "en",
		"page": "622-629",
		"source": "ScienceDirect",
		"title": "What can we learn from synaptic weight distributions?",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0166223607002615",
		"volume": "30",
		"author": [
			{
				"family": "Barbour",
				"given": "Boris"
			},
			{
				"family": "Brunel",
				"given": "Nicolas"
			},
			{
				"family": "Hakim",
				"given": "Vincent"
			},
			{
				"family": "Nadal",
				"given": "Jean-Pierre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					12,
					1
				]
			]
		}
	},
	{
		"id": "grace2018",
		"type": "article-journal",
		"container-title": "Journal of Artificial Intelligence Research",
		"DOI": "10.1613/jair.1.11222",
		"ISSN": "1076-9757",
		"language": "en",
		"license": "Copyright (c) 2018",
		"page": "729-754",
		"source": "jair.org",
		"title": "Viewpoint: When Will AI Exceed Human Performance? Evidence from AI Experts",
		"title-short": "Viewpoint",
		"URL": "https://jair.org/index.php/jair/article/view/11222",
		"volume": "62",
		"author": [
			{
				"family": "Grace",
				"given": "Katja"
			},
			{
				"family": "Salvatier",
				"given": "John"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Zhang",
				"given": "Baobao"
			},
			{
				"family": "Evans",
				"given": "Owain"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					7,
					31
				]
			]
		}
	},
	{
		"id": "reinsel2020",
		"type": "report",
		"abstract": "IDC examines consumer markets by devices, applications, networks, and services to provide complete solutions for succeeding in these expanding markets.",
		"language": "en",
		"title": "Global DataSphere",
		"URL": "https://www.idc.com/getdoc.jsp?containerId=IDC_P38353",
		"author": [
			{
				"family": "Reinsel",
				"given": "David"
			},
			{
				"family": "Rydning",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wang2020",
		"type": "report",
		"abstract": "Based on the pace of AI training cost declines, AI is in very early days. ARK believes AI will scale from $1 trillion to $30 trillion by 2037.",
		"language": "en-US",
		"title": "AI Training Costs Are Improving at 50x the Speed of Moore’s Law",
		"URL": "https://ark-invest.com/articles/analyst-research/ai-training/",
		"author": [
			{
				"family": "Wang",
				"given": "James"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					5,
					6
				]
			]
		}
	},
	{
		"id": "kaplan2020",
		"type": "post-weblog",
		"abstract": "About a year ago, when we were very much in stealth, we did a small user study of an early build of our app. Our goal was to test our capabilities in finding movies, which meant that the build only had movies. Segue to me sitting with a user who",
		"container-title": "MeetKai Blog",
		"language": "en",
		"title": "Catastrophic Failure: The Out Of Domain problem (Part 1)",
		"title-short": "Catastrophic Failure",
		"URL": "https://www.thekaihq.com/the-out-of-domain-problem-part-1/",
		"author": [
			{
				"family": "Kaplan",
				"given": "James"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					6,
					22
				]
			]
		}
	},
	{
		"id": "cho2014",
		"type": "article-journal",
		"abstract": "In this paper, we propose a novel neural network model called RNN Encoder–Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a ﬁxedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
		"container-title": "arXiv:1406.1078 [cs, stat]",
		"language": "en",
		"note": "arXiv: 1406.1078",
		"source": "arXiv.org",
		"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
		"URL": "http://arxiv.org/abs/1406.1078",
		"author": [
			{
				"family": "Cho",
				"given": "Kyunghyun"
			},
			{
				"family": "Merrienboer",
				"given": "Bart",
				"non-dropping-particle": "van"
			},
			{
				"family": "Gulcehre",
				"given": "Caglar"
			},
			{
				"family": "Bahdanau",
				"given": "Dzmitry"
			},
			{
				"family": "Bougares",
				"given": "Fethi"
			},
			{
				"family": "Schwenk",
				"given": "Holger"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					9,
					2
				]
			]
		}
	},
	{
		"id": "hochreiter2001",
		"type": "article-journal",
		"language": "en",
		"page": "15",
		"source": "Zotero",
		"title": "Gradient Flow in Recurrent Nets: the Diﬃculty of Learning Long-Term Dependencies",
		"author": [
			{
				"family": "Hochreiter",
				"given": "Sepp"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Frasconi",
				"given": "Paolo"
			},
			{
				"family": "Schmidhuber",
				"given": "Jurgen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2001"
				]
			]
		}
	},
	{
		"id": "katz2003",
		"type": "article-journal",
		"abstract": "Synaptic gating is normally thought to be a mechanism for excluding synaptic input, but three recent studies show how the resting membrane potential interacts with integrative properties to act as a permissive synaptic gate.",
		"container-title": "Current Biology",
		"DOI": "10.1016/S0960-9822(03)00471-8",
		"ISSN": "0960-9822",
		"issue": "14",
		"journalAbbreviation": "Current Biology",
		"language": "en",
		"page": "R554-R556",
		"source": "ScienceDirect",
		"title": "Synaptic Gating: The Potential to Open Closed Doors",
		"title-short": "Synaptic Gating",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0960982203004718",
		"volume": "13",
		"author": [
			{
				"family": "Katz",
				"given": "Paul S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2003",
					7,
					15
				]
			]
		}
	},
	{
		"id": "marti2018",
		"type": "article-journal",
		"abstract": "Networks of randomly connected neurons are among the most popular models in theoretical neuroscience. The connectivity between neurons in the cortex is however not fully random, the simplest and most prominent deviation from randomness found in experimental data being the overrepresentation of bidirectional connections among pyramidal cells. Using numerical and analytical methods, we investigate the effects of partially symmetric connectivity on the dynamics in networks of rate units. We consider the two dynamical regimes exhibited by random neural networks: the weak-coupling regime, where the firing activity decays to a single fixed point unless the network is stimulated, and the strong-coupling or chaotic regime, characterized by internally generated fluctuating firing rates. In the weak-coupling regime, we compute analytically, for an arbitrary degree of symmetry, the autocorrelation of network activity in the presence of external noise. In the chaotic regime, we perform simulations to determine the timescale of the intrinsic fluctuations. In both cases, symmetry increases the characteristic asymptotic decay time of the autocorrelation function and therefore slows down the dynamics in the network.",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.97.062314",
		"issue": "6",
		"journalAbbreviation": "Phys. Rev. E",
		"note": "publisher: American Physical Society",
		"page": "062314",
		"source": "APS",
		"title": "Correlations between synapses in pairs of neurons slow down dynamics in randomly connected neural networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.97.062314",
		"volume": "97",
		"author": [
			{
				"family": "Martí",
				"given": "Daniel"
			},
			{
				"family": "Brunel",
				"given": "Nicolas"
			},
			{
				"family": "Ostojic",
				"given": "Srdjan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6,
					26
				]
			]
		}
	},
	{
		"id": "mastrogiuseppe2018",
		"type": "article-journal",
		"abstract": "Large-scale neural recordings have established that the transformation of sensory stimuli into motor outputs relies on low-dimensional dynamics at the population level, while individual neurons exhibit complex selectivity. Understanding how low-dimensional computations on mixed, distributed representations emerge from the structure of the recurrent connectivity and inputs to cortical networks is a major challenge. Here, we study a class of recurrent network models in which the connectivity is a sum of a random part and a minimal, low-dimensional structure. We show that, in such networks, the dynamics are low dimensional and can be directly inferred from connectivity using a geometrical approach. We exploit this understanding to determine minimal connectivity required to implement specific computations and find that the dynamical range and computational capacity quickly increase with the dimensionality of the connectivity structure. This framework produces testable experimental predictions for the relationship between connectivity, low-dimensional dynamics, and computational features of recorded neurons.",
		"container-title": "Neuron",
		"DOI": "10.1016/j.neuron.2018.07.003",
		"ISSN": "0896-6273",
		"issue": "3",
		"journalAbbreviation": "Neuron",
		"language": "en",
		"page": "609-623.e29",
		"source": "ScienceDirect",
		"title": "Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0896627318305439",
		"volume": "99",
		"author": [
			{
				"family": "Mastrogiuseppe",
				"given": "Francesca"
			},
			{
				"family": "Ostojic",
				"given": "Srdjan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					8,
					8
				]
			]
		}
	},
	{
		"id": "mastrogiuseppe2017",
		"type": "article-journal",
		"abstract": "Recurrent networks of non-linear units display a variety of dynamical regimes depending on the structure of their synaptic connectivity. A particularly remarkable phenomenon is the appearance of strongly fluctuating, chaotic activity in networks of deterministic, but randomly connected rate units. How this type of intrinsically generated fluctuations appears in more realistic networks of spiking neurons has been a long standing question. To ease the comparison between rate and spiking networks, recent works investigated the dynamical regimes of randomly-connected rate networks with segregated excitatory and inhibitory populations, and firing rates constrained to be positive. These works derived general dynamical mean field (DMF) equations describing the fluctuating dynamics, but solved these equations only in the case of purely inhibitory networks. Using a simplified excitatory-inhibitory architecture in which DMF equations are more easily tractable, here we show that the presence of excitation qualitatively modifies the fluctuating activity compared to purely inhibitory networks. In presence of excitation, intrinsically generated fluctuations induce a strong increase in mean firing rates, a phenomenon that is much weaker in purely inhibitory networks. Excitation moreover induces two different fluctuating regimes: for moderate overall coupling, recurrent inhibition is sufficient to stabilize fluctuations; for strong coupling, firing rates are stabilized solely by the upper bound imposed on activity, even if inhibition is stronger than excitation. These results extend to more general network architectures, and to rate networks receiving noisy inputs mimicking spiking activity. Finally, we show that signatures of the second dynamical regime appear in networks of integrate-and-fire neurons.",
		"container-title": "PLOS Computational Biology",
		"DOI": "10.1371/journal.pcbi.1005498",
		"ISSN": "1553-7358",
		"issue": "4",
		"journalAbbreviation": "PLOS Computational Biology",
		"language": "en",
		"note": "publisher: Public Library of Science",
		"page": "e1005498",
		"source": "PLoS Journals",
		"title": "Intrinsically-generated fluctuating activity in excitatory-inhibitory networks",
		"URL": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005498",
		"volume": "13",
		"author": [
			{
				"family": "Mastrogiuseppe",
				"given": "Francesca"
			},
			{
				"family": "Ostojic",
				"given": "Srdjan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					4,
					24
				]
			]
		}
	},
	{
		"id": "dong",
		"type": "article-journal",
		"abstract": "Reservoir Computing is a class of simple yet efﬁcient Recurrent Neural Networks where internal weights are ﬁxed at random and only a linear output layer is trained. In the large size limit, such random neural networks have a deep connection with kernel methods. Our contributions are threefold: a) We rigorously establish the recurrent kernel limit of Reservoir Computing and prove its convergence. b) We test our models on chaotic time series prediction, a classic but challenging benchmark in Reservoir Computing, and show how the Recurrent Kernel is competitive and computationally efﬁcient when the number of data points remains moderate. c) When the number of samples is too large, we leverage the success of structured Random Features for kernel approximation by introducing Structured Reservoir Computing. The two proposed methods, Recurrent Kernel and Structured Reservoir Computing, turn out to be much faster and more memory-efﬁcient than conventional Reservoir Computing.",
		"language": "en",
		"page": "12",
		"source": "Zotero",
		"title": "Reservoir Computing meets Recurrent Kernels and Structured Transforms",
		"author": [
			{
				"family": "Dong",
				"given": "Jonathan"
			},
			{
				"family": "Ohana",
				"given": "Ruben"
			},
			{
				"family": "Rafayelyan",
				"given": "Mushegh"
			},
			{
				"family": "Krzakala",
				"given": "Florent"
			}
		]
	},
	{
		"id": "mitchell2021",
		"type": "article-journal",
		"abstract": "Since its beginning in the 1950s, the field of artificial intelligence has cycled several times between periods of optimistic predictions and massive investment (\"AI spring\") and periods of disappointment, loss of confidence, and reduced funding (\"AI winter\"). Even with today's seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. One reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself. In this paper I describe four fallacies in common assumptions made by AI researchers, which can lead to overconfident predictions about the field. I conclude by discussing the open questions spurred by these fallacies, including the age-old challenge of imbuing machines with humanlike common sense.",
		"container-title": "arXiv:2104.12871 [cs]",
		"note": "arXiv: 2104.12871",
		"source": "arXiv.org",
		"title": "Why AI is Harder Than We Think",
		"URL": "http://arxiv.org/abs/2104.12871",
		"author": [
			{
				"family": "Mitchell",
				"given": "Melanie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					4,
					26
				]
			]
		}
	},
	{
		"id": "recanatesi2019",
		"type": "article-journal",
		"abstract": "The dimensionality of a network’s collective activity is of increasing interest in neuroscience. This is because dimensionality provides a compact measure of how coordinated network-wide activity is, in terms of the number of modes (or degrees of freedom) that it can independently explore. A low number of modes suggests a compressed low dimensional neural code and reveals interpretable dynamics [1], while findings of high dimension may suggest flexible computations [2, 3]. Here, we address the fundamental question of how dimensionality is related to connectivity, in both autonomous and stimulus-driven networks. Working with a simple spiking network model, we derive three main findings. First, the dimensionality of global activity patterns can be strongly, and systematically, regulated by local connectivity structures. Second, the dimensionality is a better indicator than average correlations in determining how constrained neural activity is. Third, stimulus evoked neural activity interacts systematically with neural connectivity patterns, leading to network responses of either greater or lesser dimensionality than the stimulus.",
		"container-title": "PLOS Computational Biology",
		"DOI": "10.1371/journal.pcbi.1006446",
		"ISSN": "1553-7358",
		"issue": "7",
		"journalAbbreviation": "PLOS Computational Biology",
		"language": "en",
		"note": "publisher: Public Library of Science",
		"page": "e1006446",
		"source": "PLoS Journals",
		"title": "Dimensionality in recurrent spiking networks: Global trends in activity and local origins in connectivity",
		"title-short": "Dimensionality in recurrent spiking networks",
		"URL": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006446",
		"volume": "15",
		"author": [
			{
				"family": "Recanatesi",
				"given": "Stefano"
			},
			{
				"family": "Ocker",
				"given": "Gabriel Koch"
			},
			{
				"family": "Buice",
				"given": "Michael A."
			},
			{
				"family": "Shea-Brown",
				"given": "Eric"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					7,
					12
				]
			]
		}
	},
	{
		"id": "ahmad2019",
		"type": "article-journal",
		"abstract": "Most artificial networks today rely on dense representations, whereas biological networks rely on sparse representations. In this paper we show how sparse representations can be more robust to noise and interference, as long as the underlying dimensionality is sufficiently high. A key intuition that we develop is that the ratio of the operable volume around a sparse vector divided by the volume of the representational space decreases exponentially with dimensionality. We then analyze computationally efficient sparse networks containing both sparse weights and activations. Simulations on MNIST and the Google Speech Command Dataset show that such networks demonstrate significantly improved robustness and stability compared to dense networks, while maintaining competitive accuracy. We discuss the potential benefits of sparsity on accuracy, noise robustness, hyperparameter tuning, learning speed, computational efficiency, and power requirements.",
		"container-title": "arXiv:1903.11257 [cs, stat]",
		"note": "arXiv: 1903.11257",
		"source": "arXiv.org",
		"title": "How Can We Be So Dense? The Benefits of Using Highly Sparse Representations",
		"title-short": "How Can We Be So Dense?",
		"URL": "http://arxiv.org/abs/1903.11257",
		"author": [
			{
				"family": "Ahmad",
				"given": "Subutai"
			},
			{
				"family": "Scheinkman",
				"given": "Luiz"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					4,
					2
				]
			]
		}
	},
	{
		"id": "zotero-696",
		"type": "webpage",
		"abstract": "Neuroscientist and tech entrepreneur Jeff Hawkins claims he knows how intelligence works and he wants every AI lab in the world to read his book.",
		"container-title": "MIT Technology Review",
		"language": "en",
		"title": "We first need to understand how the brain works if we want true AI",
		"URL": "https://www.technologyreview.com/2021/03/03/1020247/artificial-intelligence-brain-neuroscience-jeff-hawkins/",
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					28
				]
			]
		}
	},
	{
		"id": "russo2014",
		"type": "article-journal",
		"abstract": "The human brain exhibits a complex structure made of scale-free highly connected modules loosely interconnected by weaker links to form a small-world network. These features appear in healthy patients whereas neurological diseases often modify this structure. An important open question concerns the role of brain modularity in sustaining the critical behaviour of spontaneous activity. Here we analyse the neuronal activity of a model, successful in reproducing on non-modular networks the scaling behaviour observed in experimental data, on a modular network implementing the main statistical features measured in human brain. We show that on a modular network, regardless the strength of the synaptic connections or the modular size and number, activity is never fully scale-free. Neuronal avalanches can invade different modules which results in an activity depression, hindering further avalanche propagation. Critical behaviour is solely recovered if inter-module connections are added, modifying the modular into a more random structure.",
		"container-title": "Scientific Reports",
		"DOI": "10.1038/srep04312",
		"ISSN": "2045-2322",
		"issue": "1",
		"language": "en",
		"license": "2014 The Author(s)",
		"note": "number: 1\npublisher: Nature Publishing Group",
		"page": "4312",
		"source": "www.nature.com",
		"title": "Brain modularity controls the critical behavior of spontaneous activity",
		"URL": "https://www.nature.com/articles/srep04312",
		"volume": "4",
		"author": [
			{
				"family": "Russo",
				"given": "R."
			},
			{
				"family": "Herrmann",
				"given": "H. J."
			},
			{
				"family": "Arcangelis",
				"given": "L.",
				"non-dropping-particle": "de"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					3,
					13
				]
			]
		}
	},
	{
		"id": "eguiluz2005",
		"type": "article-journal",
		"abstract": "Functional magnetic resonance imaging is used to extract functional networks connecting correlated human brain sites. Analysis of the resulting networks in different tasks shows that (a) the distribution of functional connections, and the probability of finding a link versus distance are both scale-free, (b) the characteristic path length is small and comparable with those of equivalent random networks, and (c) the clustering coefficient is orders of magnitude larger than those of equivalent random networks. All these properties, typical of scale-free small-world networks, reflect important functional information about brain states.",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.94.018102",
		"issue": "1",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"note": "publisher: American Physical Society",
		"page": "018102",
		"source": "APS",
		"title": "Scale-Free Brain Functional Networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevLett.94.018102",
		"volume": "94",
		"author": [
			{
				"family": "Eguíluz",
				"given": "Victor M."
			},
			{
				"family": "Chialvo",
				"given": "Dante R."
			},
			{
				"family": "Cecchi",
				"given": "Guillermo A."
			},
			{
				"family": "Baliki",
				"given": "Marwan"
			},
			{
				"family": "Apkarian",
				"given": "A. Vania"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2005",
					1,
					6
				]
			]
		}
	},
	{
		"id": "hinton2012",
		"type": "article-journal",
		"abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",
		"container-title": "arXiv:1207.0580 [cs]",
		"note": "arXiv: 1207.0580",
		"source": "arXiv.org",
		"title": "Improving neural networks by preventing co-adaptation of feature detectors",
		"URL": "http://arxiv.org/abs/1207.0580",
		"author": [
			{
				"family": "Hinton",
				"given": "Geoffrey E."
			},
			{
				"family": "Srivastava",
				"given": "Nitish"
			},
			{
				"family": "Krizhevsky",
				"given": "Alex"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Salakhutdinov",
				"given": "Ruslan R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					7,
					3
				]
			]
		}
	},
	{
		"id": "cipolloni2020",
		"type": "article-journal",
		"abstract": "We consider the non-Hermitian analogue of the celebrated Wigner-Dyson-Mehta bulk universality phenomenon, i.e. that in the bulk the local eigenvalue statistics of a large random matrix with independent, identically distributed centred entries are universal, in particular they asymptotically coincide with those of the Ginibre ensemble in the corresponding symmetry class. In this paper we reduce this problem to understanding a certain microscopic regime for the Hermitized resolvent in Girko's formula by showing that all other regimes are negligible.",
		"container-title": "arXiv:1909.06350 [math-ph]",
		"note": "arXiv: 1909.06350",
		"source": "arXiv.org",
		"title": "Towards the bulk universality of non-Hermitian random matrices",
		"URL": "http://arxiv.org/abs/1909.06350",
		"author": [
			{
				"family": "Cipolloni",
				"given": "Giorgio"
			},
			{
				"family": "Erdős",
				"given": "László"
			},
			{
				"family": "Schröder",
				"given": "Dominik"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					9,
					15
				]
			]
		}
	},
	{
		"id": "huang2015",
		"type": "article-journal",
		"abstract": "We consider the adjacency matrix of the ensemble of Erdős-Rényi random graphs which consists of graphs on N vertices in which each edge occurs independently with probability p. We prove that in the regime pN ≫ 1, these matrices exhibit bulk universality in the sense that both the averaged n-point correlation functions and distribution of a single eigenvalue gap coincide with those of the GOE. Our methods extend to a class of random matrices which includes sparse ensembles whose entries have different variances.",
		"container-title": "Journal of Mathematical Physics",
		"DOI": "10.1063/1.4936139",
		"ISSN": "0022-2488",
		"issue": "12",
		"journalAbbreviation": "Journal of Mathematical Physics",
		"note": "publisher: American Institute of Physics",
		"page": "123301",
		"source": "aip.scitation.org (Atypon)",
		"title": "Bulk universality of sparse random matrices",
		"URL": "https://aip.scitation.org/doi/full/10.1063/1.4936139",
		"volume": "56",
		"author": [
			{
				"family": "Huang",
				"given": "Jiaoyang"
			},
			{
				"family": "Landon",
				"given": "Benjamin"
			},
			{
				"family": "Yau",
				"given": "Horng-Tzer"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					12,
					1
				]
			]
		}
	},
	{
		"id": "goh2001",
		"type": "article-journal",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.64.051903",
		"ISSN": "1063-651X, 1095-3787",
		"issue": "5",
		"journalAbbreviation": "Phys. Rev. E",
		"language": "en",
		"page": "051903",
		"source": "DOI.org (Crossref)",
		"title": "Spectra and eigenvectors of scale-free networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.64.051903",
		"volume": "64",
		"author": [
			{
				"family": "Goh",
				"given": "K.-I."
			},
			{
				"family": "Kahng",
				"given": "B."
			},
			{
				"family": "Kim",
				"given": "D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					4,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2001",
					10,
					15
				]
			]
		}
	},
	{
		"id": "jabr2013",
		"type": "article-magazine",
		"abstract": "Research on naps, meditation, nature walks and the habits of exceptional artists and athletes reveals how mental breaks increase productivity, replenish attention, solidify memories and encourage creativity",
		"container-title": "Scientific American",
		"language": "en",
		"title": "Why Your Brain Needs More Downtime",
		"URL": "https://www.scientificamerican.com/article/mental-downtime/",
		"author": [
			{
				"family": "Jabr",
				"given": "Ferris"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		}
	},
	{
		"id": "thiel2014",
		"type": "book",
		"abstract": "If you want to build a better future, you must believe in secrets.  The great secret of our time is that there are still uncharted fronti...",
		"title": "Zero to One",
		"URL": "https://www.goodreads.com/work/best_book/25332940-zero-to-one-notes-on-start-ups-or-how-to-build-the-future",
		"author": [
			{
				"family": "Thiel",
				"given": "Peter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "isaacson2017",
		"type": "book",
		"title": "Leonardo da Vinci by Walter Isaacson | Goodreads",
		"URL": "https://www.goodreads.com/book/show/34684622-leonardo-da-vinci?from_search=true&from_srp=true&qid=RgsYIgzbtI&rank=2",
		"author": [
			{
				"family": "Isaacson",
				"given": "Walter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "kahneman2011",
		"type": "book",
		"abstract": "In the highly anticipated Thinking, Fast and Slow, Kahneman takes us on a groundbreaking tour of the mind and explains the two systems th...",
		"title": "Thinking, Fast and Slow",
		"URL": "https://www.goodreads.com/work/best_book/16402639-thinking-fast-and-slow",
		"author": [
			{
				"family": "Kahneman",
				"given": "Daniel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		}
	},
	{
		"id": "dehghani2016",
		"type": "article-journal",
		"abstract": "Balance of excitation and inhibition is a fundamental feature of in vivo network activity and is important for its computations. However, its presence in the neocortex of higher mammals is not well established. We investigated the dynamics of excitation and inhibition using dense multielectrode recordings in humans and monkeys. We found that in all states of the wake-sleep cycle, excitatory and inhibitory ensembles are well balanced and co-fluctuate with slight instantaneous deviations from perfect balance, mostly in slow-wave sleep. Remarkably, these correlated fluctuations are seen for many different temporal scales. The similarity of these computational features with a network model of self-generated balanced states suggests that such balanced activity is essentially generated by recurrent activity in the local network and is not due to external inputs. Finally, we find that this balance breaks down during seizures, where the temporal correlation of excitatory and inhibitory populations is disrupted. These results show that balanced activity is a feature of normal brain activity and break down of the balance could be an important factor to define pathological states.",
		"container-title": "Scientific Reports",
		"DOI": "10.1038/srep23176",
		"ISSN": "2045-2322",
		"issue": "1",
		"language": "en",
		"license": "2016 The Author(s)",
		"note": "number: 1\npublisher: Nature Publishing Group",
		"page": "23176",
		"source": "www.nature.com",
		"title": "Dynamic Balance of Excitation and Inhibition in Human and Monkey Neocortex",
		"URL": "https://www.nature.com/articles/srep23176",
		"volume": "6",
		"author": [
			{
				"family": "Dehghani",
				"given": "Nima"
			},
			{
				"family": "Peyrache",
				"given": "Adrien"
			},
			{
				"family": "Telenczuk",
				"given": "Bartosz"
			},
			{
				"family": "Le Van Quyen",
				"given": "Michel"
			},
			{
				"family": "Halgren",
				"given": "Eric"
			},
			{
				"family": "Cash",
				"given": "Sydney S."
			},
			{
				"family": "Hatsopoulos",
				"given": "Nicholas G."
			},
			{
				"family": "Destexhe",
				"given": "Alain"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					4
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					3,
					16
				]
			]
		}
	},
	{
		"id": "helias2020",
		"type": "book",
		"collection-title": "Lecture Notes in Physics",
		"event-place": "Cham",
		"ISBN": "978-3-030-46443-1",
		"language": "en",
		"note": "DOI: 10.1007/978-3-030-46444-8",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Statistical Field Theory for Neural Networks",
		"URL": "http://link.springer.com/10.1007/978-3-030-46444-8",
		"volume": "970",
		"author": [
			{
				"family": "Helias",
				"given": "Moritz"
			},
			{
				"family": "Dahmen",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "domingos2020",
		"type": "article-journal",
		"abstract": "Deep learning's successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for prediction via a similarity function (the kernel). This greatly enhances the interpretability of deep network weights, by elucidating that they are effectively a superposition of the training examples. The network architecture incorporates knowledge of the target function into the kernel. This improved understanding should lead to better learning algorithms.",
		"container-title": "arXiv:2012.00152 [cs, stat]",
		"note": "arXiv: 2012.00152",
		"source": "arXiv.org",
		"title": "Every Model Learned by Gradient Descent Is Approximately a Kernel Machine",
		"URL": "http://arxiv.org/abs/2012.00152",
		"author": [
			{
				"family": "Domingos",
				"given": "Pedro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					6
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					11,
					30
				]
			]
		}
	},
	{
		"id": "schwartz-ziv2017",
		"type": "article-journal",
		"abstract": "Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \\textit{Information Plane}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on {\\emph compression} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.",
		"container-title": "arXiv:1703.00810 [cs]",
		"note": "arXiv: 1703.00810",
		"source": "arXiv.org",
		"title": "Opening the Black Box of Deep Neural Networks via Information",
		"URL": "http://arxiv.org/abs/1703.00810",
		"author": [
			{
				"family": "Schwartz-Ziv",
				"given": "Ravid"
			},
			{
				"family": "Tishby",
				"given": "Naftali"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					6
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					4,
					29
				]
			]
		}
	},
	{
		"id": "bellec2019",
		"type": "article-journal",
		"abstract": "The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.",
		"container-title": "arXiv:1901.09049 [cs]",
		"note": "arXiv: 1901.09049",
		"source": "arXiv.org",
		"title": "Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets",
		"URL": "http://arxiv.org/abs/1901.09049",
		"author": [
			{
				"family": "Bellec",
				"given": "Guillaume"
			},
			{
				"family": "Scherr",
				"given": "Franz"
			},
			{
				"family": "Hajek",
				"given": "Elias"
			},
			{
				"family": "Salaj",
				"given": "Darjan"
			},
			{
				"family": "Legenstein",
				"given": "Robert"
			},
			{
				"family": "Maass",
				"given": "Wolfgang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					6
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					2,
					21
				]
			]
		}
	},
	{
		"id": "vaswani2017",
		"type": "article-journal",
		"abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
		"container-title": "arXiv:1706.03762 [cs]",
		"note": "arXiv: 1706.03762\nversion: 5",
		"source": "arXiv.org",
		"title": "Attention Is All You Need",
		"URL": "http://arxiv.org/abs/1706.03762",
		"author": [
			{
				"family": "Vaswani",
				"given": "Ashish"
			},
			{
				"family": "Shazeer",
				"given": "Noam"
			},
			{
				"family": "Parmar",
				"given": "Niki"
			},
			{
				"family": "Uszkoreit",
				"given": "Jakob"
			},
			{
				"family": "Jones",
				"given": "Llion"
			},
			{
				"family": "Gomez",
				"given": "Aidan N."
			},
			{
				"family": "Kaiser",
				"given": "Lukasz"
			},
			{
				"family": "Polosukhin",
				"given": "Illia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					10
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					12,
					5
				]
			]
		}
	},
	{
		"id": "quinn2019",
		"type": "book",
		"abstract": "The leading experts in system change and learning, with their school-based partners around the world, have created this essential companion to their runaway best-seller, Deep Learning: Engage the World Change the World. This hands-on guide provides a roadmap for building capacity in teachers, schools, districts, and systems to design deep learning, measure progress, and assess conditions needed to activate and sustain innovation.   Dive Into Deep Learning: Tools for Engagement is rich with resources educators need to construct and drive meaningful deep learning experiences in order to develop the kind of mindset and know-how that is crucial to becoming a problem-solving change agent in our global society. Designed in full color, this easy-to-use guide is loaded with tools, tips, protocols, and real-world examples. It includes:  • A framework for deep learning that provides a pathway to develop the six global competencies needed to flourish in a complex world — character, citizenship, collaboration, communication, creativity, and critical thinking. • Learning progressions to help educators analyze student work and measure progress. • Learning design rubrics, templates and examples for incorporating the four elements of learning design: learning partnerships, pedagogical practices, learning environments, and leveraging digital.  • Conditions rubrics, teacher self-assessment tools, and planning guides to help educators build, mobilize, and sustain deep learning in schools and districts.   Learn about, improve, and expand your world of learning. Put the joy back into learning for students and adults alike. Dive into deep learning to create learning experiences that give purpose, unleash student potential, and transform not only learning, but life itself.",
		"ISBN": "978-1-5443-8540-2",
		"language": "en",
		"note": "Google-Books-ID: eaCgDwAAQBAJ",
		"number-of-pages": "297",
		"publisher": "Corwin Press",
		"source": "Google Books",
		"title": "Dive Into Deep Learning: Tools for Engagement",
		"title-short": "Dive Into Deep Learning",
		"author": [
			{
				"family": "Quinn",
				"given": "Joanne"
			},
			{
				"family": "McEachen",
				"given": "Joanne"
			},
			{
				"family": "Fullan",
				"given": "Michael"
			},
			{
				"family": "Gardner",
				"given": "Mag"
			},
			{
				"family": "Drummy",
				"given": "Max"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019",
					7,
					15
				]
			]
		}
	},
	{
		"id": "mccormick1985",
		"type": "article-journal",
		"abstract": "Slices of sensorimotor and anterior cingulate cortex from guinea pigs were maintained in vitro and bathed in a normal physiological medium. Electrophysiological properties of neurons were assessed with intracellular recording techniques. Some neurons were identified morphologically by intracellular injection of the fluorescent dye Lucifer yellow CH. Three distinct neuronal classes of electrophysiological behavior were observed; these were termed regular spiking, bursting, and fast spiking. The physiological properties of neurons from sensorimotor and anterior cingulate areas did not differ significantly. Regular-spiking cells were characterized by action potentials with a mean duration of 0.80 ms at one-half amplitude, a ratio of maximum rate of spike rise to maximum rate of fall of 4.12, and a prominent afterhyperpolarization following a train of spikes. The primary slope of initial spike frequency versus injected current intensity was 241 Hz/nA. During prolonged suprathreshold current pulses the frequency of firing adapted strongly. When local synaptic pathways were activated, all cells were transiently excited and then strongly inhibited. Bursting cells were distinguished by their ability to generate endogenous, all-or-none bursts of three to five action potentials. Their properties were otherwise very similar to regular-spiking cells. The ability to generate a burst was eliminated when the membrane was depolarized to near the firing threshold with tonic current. By contrast, hyperpolarization of regular-spiking (i.e., nonbursting) cells did not uncover latent bursting tendencies. The action potentials of fast-spiking cells were much briefer (mean of 0.32 ms) than those of the other cell types.(ABSTRACT TRUNCATED AT 250 WORDS)",
		"container-title": "Journal of Neurophysiology",
		"DOI": "10.1152/jn.1985.54.4.782",
		"ISSN": "0022-3077",
		"issue": "4",
		"note": "publisher: American Physiological Society",
		"page": "782-806",
		"source": "journals.physiology.org (Atypon)",
		"title": "Comparative electrophysiology of pyramidal and sparsely spiny stellate neurons of the neocortex",
		"URL": "https://journals.physiology.org/doi/abs/10.1152/jn.1985.54.4.782",
		"volume": "54",
		"author": [
			{
				"family": "McCormick",
				"given": "D. A."
			},
			{
				"family": "Connors",
				"given": "B. W."
			},
			{
				"family": "Lighthall",
				"given": "J. W."
			},
			{
				"family": "Prince",
				"given": "D. A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					10
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1985",
					10,
					1
				]
			]
		}
	},
	{
		"id": "klinshov2015",
		"type": "article-journal",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.92.062813",
		"ISSN": "1539-3755, 1550-2376",
		"issue": "6",
		"journalAbbreviation": "Phys. Rev. E",
		"language": "en",
		"page": "062813",
		"source": "DOI.org (Crossref)",
		"title": "Mean-field dynamics of a random neural network with noise",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.92.062813",
		"volume": "92",
		"author": [
			{
				"family": "Klinshov",
				"given": "Vladimir"
			},
			{
				"family": "Franović",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					10
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					12,
					10
				]
			]
		}
	},
	{
		"id": "bressloff2012",
		"type": "article-journal",
		"abstract": "We survey recent analytical approaches to studying the spatiotemporal dynamics of continuum neural ﬁelds. Neural ﬁelds model the large-scale dynamics of spatially structured biological neural networks in terms of nonlinear integrodifferential equations whose associated integral kernels represent the spatial distribution of neuronal synaptic connections. They provide an important example of spatially extended excitable systems with nonlocal interactions and exhibit a wide range of spatially coherent dynamics including traveling waves oscillations and Turing-like patterns.",
		"container-title": "Journal of Physics A: Mathematical and Theoretical",
		"DOI": "10.1088/1751-8113/45/3/033001",
		"ISSN": "1751-8113, 1751-8121",
		"issue": "3",
		"journalAbbreviation": "J. Phys. A: Math. Theor.",
		"language": "en",
		"page": "033001",
		"source": "DOI.org (Crossref)",
		"title": "Spatiotemporal dynamics of continuum neural fields",
		"URL": "https://iopscience.iop.org/article/10.1088/1751-8113/45/3/033001",
		"volume": "45",
		"author": [
			{
				"family": "Bressloff",
				"given": "Paul C"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					1,
					27
				]
			]
		}
	},
	{
		"id": "bressloff2010",
		"type": "article-journal",
		"container-title": "SIAM Journal on Applied Mathematics",
		"DOI": "10.1137/090756971",
		"ISSN": "0036-1399, 1095-712X",
		"issue": "5",
		"journalAbbreviation": "SIAM J. Appl. Math.",
		"language": "en",
		"page": "1488-1521",
		"source": "DOI.org (Crossref)",
		"title": "Stochastic Neural Field Theory and the System-Size Expansion",
		"URL": "http://epubs.siam.org/doi/10.1137/090756971",
		"volume": "70",
		"author": [
			{
				"family": "Bressloff",
				"given": "Paul C."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2010",
					1
				]
			]
		}
	},
	{
		"id": "bressloff",
		"type": "article-journal",
		"abstract": "We analyze a master equation formulation of stochastic neurodynamics for a network of synaptically coupled homogeneous neuronal populations each consisting of N identical neurons. The state of the network is speciﬁed by the fraction of active or spiking neurons in each population, and transition rates are chosen so that in the thermodynamic or deterministic limit (N → ∞) we recover standard activity-based or voltage-based rate models. We derive the lowest order corrections to these rate equations for large but ﬁnite N using two diﬀerent approximation schemes, one based on the Van Kampen system-size expansion and the other based on path integral methods. Both methods yield the same series expansion of the moment equations, which at O(1/N ) can be truncated to form a closed system of equations for the ﬁrst- and second-order moments. Taking a continuum limit of the moment equations while keeping the system size N ﬁxed generates a system of integrodiﬀerential equations for the mean and covariance of the corresponding stochastic neural ﬁeld model. We also show how the path integral approach can be used to study large deviation or rare event statistics underlying escape from the basin of attraction of a stable ﬁxed point of the mean-ﬁeld dynamics; such an analysis is not possible using the system-size expansion since the latter cannot accurately determine exponentially small transitions.",
		"language": "en",
		"page": "34",
		"source": "Zotero",
		"title": "STOCHASTIC NEURAL FIELD THEORY AND THE SYSTEM-SIZE EXPANSION",
		"author": [
			{
				"family": "Bressloff",
				"given": "Paul C"
			}
		]
	},
	{
		"id": "wilson1972",
		"type": "article-journal",
		"container-title": "Biophysical Journal",
		"DOI": "10.1016/S0006-3495(72)86068-5",
		"ISSN": "00063495",
		"issue": "1",
		"journalAbbreviation": "Biophysical Journal",
		"language": "en",
		"page": "1-24",
		"source": "DOI.org (Crossref)",
		"title": "Excitatory and Inhibitory Interactions in Localized Populations of Model Neurons",
		"URL": "https://linkinghub.elsevier.com/retrieve/pii/S0006349572860685",
		"volume": "12",
		"author": [
			{
				"family": "Wilson",
				"given": "Hugh R."
			},
			{
				"family": "Cowan",
				"given": "Jack D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1972",
					1
				]
			]
		}
	},
	{
		"id": "hasegawa2007",
		"type": "article-journal",
		"abstract": "We have proposed a generalized Langevin-type rate-code model subjected to multiplicative noise, in order to study stationary and dynamical properties of an ensemble containing a finite number N of neurons. Calculations using the Fokker-Planck equation have shown that, owing to the multiplicative noise, our rate model yields various kinds of stationary non-Gaussian distributions such as Γ, inverse-Gaussian-like, and log-normal-like distributions, which have been experimentally observed. The dynamical properties of the rate model have been studied with the use of the augmented moment method (AMM), which was previously proposed by the author from a macroscopic point of view for finite-unit stochastic systems. In the AMM, the original N-dimensional stochastic differential equations (DEs) are transformed into three-dimensional deterministic DEs for the means and fluctuations of local and global variables. The dynamical responses of the neuron ensemble to pulse and sinusoidal inputs calculated by the AMM are in good agreement with those obtained by direct simulation. The synchronization in the neuronal ensemble is discussed. The variabilities of the firing rate and of the interspike interval are shown to increase with increasing magnitude of multiplicative noise, which may be a conceivable origin of the observed large variability in cortical neurons.",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.75.051904",
		"issue": "5",
		"journalAbbreviation": "Phys. Rev. E",
		"note": "publisher: American Physical Society",
		"page": "051904",
		"source": "APS",
		"title": "Generalized rate-code model for neuron ensembles with finite populations",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.75.051904",
		"volume": "75",
		"author": [
			{
				"family": "Hasegawa",
				"given": "Hideo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					5,
					8
				]
			]
		}
	},
	{
		"id": "buice2007",
		"type": "article-journal",
		"abstract": "A well-defined stochastic theory for neural activity, which permits the calculation of arbitrary statistical moments and equations governing them, is a potentially valuable tool for theoretical neuroscience. We produce such a theory by analyzing the dynamics of neural activity using field theoretic methods for nonequilibrium statistical processes. Assuming that neural network activity is Markovian, we construct the effective spike model, which describes both neural fluctuations and response. This analysis leads to a systematic expansion of corrections to mean field theory, which for the effective spike model is a simple version of the Wilson-Cowan equation. We argue that neural activity governed by this model exhibits a dynamical phase transition which is in the universality class of directed percolation. More general models (which may incorporate refractoriness) can exhibit other universality classes, such as dynamic isotropic percolation. Because of the extremely high connectivity in typical networks, it is expected that higher-order terms in the systematic expansion are small for experimentally accessible measurements, and thus, consistent with measurements in neocortical slice preparations, we expect mean field exponents for the transition. We provide a quantitative criterion for the relative magnitude of each term in the systematic expansion, analogous to the Ginsburg criterion. Experimental identification of dynamic universality classes in vivo is an outstanding and important question for neuroscience.",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.75.051919",
		"issue": "5",
		"journalAbbreviation": "Phys. Rev. E",
		"note": "publisher: American Physical Society",
		"page": "051919",
		"source": "APS",
		"title": "Field-theoretic approach to fluctuation effects in neural networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.75.051919",
		"volume": "75",
		"author": [
			{
				"family": "Buice",
				"given": "Michael A."
			},
			{
				"family": "Cowan",
				"given": "Jack D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					5,
					29
				]
			]
		}
	},
	{
		"id": "eldredge1972",
		"type": "article-journal",
		"title": "Punctuated Equilibria: An Alternative to Phyletic Gradualism",
		"URL": "http://www.blackwellpublishing.com/ridley/classictexts/eldredge.pdf",
		"author": [
			{
				"family": "Eldredge",
				"given": "Niles"
			},
			{
				"family": "Gould",
				"given": "Stephen J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1972"
				]
			]
		}
	},
	{
		"id": "eldredge1971",
		"type": "article-journal",
		"abstract": "Emphasis on time and gradualistic transformation has led to the dominance of a strictly phyletic model of species transformation in most paleontological thought. Even documented cases of lineage-splitting are often interpreted by recourse to a gradual (morphological) divergence model in which a strong element of phyletic thinking is incorporated. Allopatric speciation, not a strict alternative to gradual divergence, seems to fit the common pattern of non-intergrading species within a lineage as typically preserved in Paleozoic epeiric sediments. The majority of species preserved in epeiric sediments show no change in species-specific characters throughout the interval of their stratigraphic occurrence, and the phyletic model is inapplicable to most of these elements of the fossil record. Instead, change in, or development of, species-specific characters are envisioned as occurring relatively rapidly in peripheral isolates. Morphological stability of epeiric species is attributed to stabilizing selection. The Devonian trilobite Phacops rana illustrates, at two different times in its history, the origin of a new character state in peripheral isolates. Most new Paleozoic invertebrate taxa probably arose in geosynclines (marginal seas) bordering the cratons; though these origination events themselves were probably not related to marine regressions, invasion of new taxa into the epeiric seas seems directly related to periods of transgression.",
		"container-title": "Evolution",
		"DOI": "10.2307/2406508",
		"ISSN": "0014-3820",
		"issue": "1",
		"note": "publisher: [Society for the Study of Evolution, Wiley]",
		"page": "156-167",
		"source": "JSTOR",
		"title": "The Allopatric Model and Phylogeny in Paleozoic Invertebrates",
		"URL": "https://www.jstor.org/stable/2406508",
		"volume": "25",
		"author": [
			{
				"family": "Eldredge",
				"given": "Niles"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1971"
				]
			]
		}
	},
	{
		"id": "mayr1954",
		"type": "article-journal",
		"title": "Change of Genetic Environment and Evolution",
		"URL": "http://www.blackwellpublishing.com/ridley/classictexts/mayr.pdf",
		"author": [
			{
				"family": "Mayr",
				"given": "Ernst"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1954"
				]
			]
		}
	},
	{
		"id": "sterelny2007",
		"type": "book",
		"publisher": "Icon Books Cambridge",
		"title": "Dawkins vs. Gould: survival of the fittest",
		"author": [
			{
				"family": "Sterelny",
				"given": "Kim"
			},
			{
				"family": "Turney",
				"given": "Jon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2007"
				]
			]
		}
	},
	{
		"id": "rohrlich1989",
		"type": "book",
		"abstract": "This book discusses, in clear non technical language, the two major theories of twentieth-century physics: relativity and quantum mechanics. They are discussed conceptually and philosophically, rather than using mathematics, and the philosophical issues raised pertain to much of science, not only physics. The book is based on successful courses taught by the author, who shows how new discoveries forced physicists to accept often strange and unconventional notions. He aims to remove the mystery and misrepresentation that often surround the ideas of modern physics and to show how modern scientists construct theories. In this way, the reader can appreciate their successes and failures and understand problems which are as yet unsolved.",
		"ISBN": "978-0-521-37605-1",
		"language": "en",
		"note": "Google-Books-ID: 3TqA1394OVcC",
		"number-of-pages": "244",
		"publisher": "Cambridge University Press",
		"source": "Google Books",
		"title": "From Paradox to Reality: Our Basic Concepts of the Physical World",
		"title-short": "From Paradox to Reality",
		"author": [
			{
				"family": "Rohrlich",
				"given": "Fritz"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1989",
					8,
					25
				]
			]
		}
	},
	{
		"id": "faraday",
		"type": "webpage",
		"abstract": "Biographical information on Michael Faraday and his experiments which led directly to the modern electric motor generator and transformer. Also includes his famous work on electromagnetic induction.",
		"container-title": "IET: The Institution of Engineering and Technology",
		"language": "en-US",
		"title": "Archives Biographies: Michael Faraday",
		"title-short": "Michael Faraday",
		"URL": "https://www.theiet.org/membership/library-archives/the-iet-archives/biographies/michael-faraday/",
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					21
				]
			]
		}
	},
	{
		"id": "grove",
		"type": "webpage",
		"abstract": "Sir William Robert Grove, British physicist and a justice of Britain’s High Court (from 1880), who built the first fuel cell in 1842 and first offered proof of the thermal dissociation of atoms within a molecule. Grove was educated by private tutors and then at Brasenose College, Oxford, and also",
		"container-title": "Encyclopedia Britannica",
		"language": "en",
		"title": "Sir William Robert Grove",
		"URL": "https://www.britannica.com/biography/William-Robert-Grove",
		"author": [
			{
				"family": "Britannica",
				"given": "The Editors of Encyclopaedia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mclaughlin1997",
		"type": "article-journal",
		"abstract": "<section class=\"abstract\"><p>Carcinization, or the process of becoming a crab, has been, and continues to be, a focal point of anomuran evolutionary hypotheses. Traditional examples of carcinization in the Anomura are most celebrated among hermit crabs but certainly are not limited to this group. Carcinization, if it has occurred, has done so independently in all major anomuran taxa. </p><p>In this critique, the traditional examples of carcinization in the Anomura are reviewed and more modern variations on the theme assessed. Potential pathways of carcinization are examined from perspectives of adult morphology in the Paguroidea, Galatheoidea, Hippoidea and Lomoidea, with emphasis on the Paguroidea. Specific attention is given to the theoretical transformation of a hermit crab-like body form into a “king crab”- like lithodid crab. Resulting coercive evidence indicates: (1) that while the evolution of a crab-like body form certainly occurs, the traditional applications, based on inadequate and often inaccurate data, are flawed; and (2) that lithodid crabs did not arise from a hermit crab predecessor through the process of carcinization.</p></section>",
		"container-title": "Contributions to Zoology",
		"DOI": "10.1163/18759866-06702001",
		"ISSN": "1875-9866, 1383-4517",
		"issue": "2",
		"language": "en",
		"note": "publisher: Brill\nsection: Contributions to Zoology",
		"page": "79-123",
		"source": "brill.com",
		"title": "Carcinization in the Anomura - fact or fiction? I. Evidence from adult morphology",
		"title-short": "Carcinization in the Anomura - fact or fiction?",
		"URL": "https://brill.com/view/journals/ctoz/67/2/article-p79_1.xml",
		"volume": "67",
		"author": [
			{
				"family": "McLaughlin",
				"given": "Patsy A."
			},
			{
				"family": "Lemaitre",
				"given": "Rafael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1997",
					1,
					1
				]
			]
		}
	},
	{
		"id": "keiler2017",
		"type": "article-journal",
		"container-title": "Biological Journal of the Linnean Society",
		"title": "One hundred years of carcinization – the evolution of the crab-like habitus in Anomura (Arthropoda: Crustacea)",
		"URL": "https://academic.oup.com/biolinnean/article/121/1/200/3089703",
		"author": [
			{
				"family": "Keiler",
				"given": "Jonas"
			},
			{
				"family": "Wirkner",
				"given": "Christian S."
			},
			{
				"family": "Richter",
				"given": "Stefan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "lens2013",
		"type": "article-journal",
		"abstract": "Premise of research. One of the most conspicuous aspects of island floras is the relatively high proportion of woody species. Often, but not always, these woody species have developed wood on the islands and have evolved from herbaceous continental ancestors, a phenomenon known as insular woodiness. Shifts from herbaceousness toward increased woodiness also occur on continents (the broader term “secondary woodiness” is more appropriate here and includes insular woodiness), but comprehensive worldwide knowledge about secondary woodiness within angiosperms remains lacking. We update hypotheses regarding the herbaceous ancestry of woody Canarian lineages in a molecular phylogenetic context and investigate the possible link of secondary woodiness and paedomorphic wood features in the Carlquistian sense.Methodology. We have assembled available literature data from molecular phylogenetic studies, wood anatomical descriptions, floras, and taxonomic revisions to identify the native secondarily woody taxa.Pivotal results. In total, at least 220 native Canary Island species of flowering plants, from 34 genera representing 15 families, are truly insular woody. This represents a significant portion of the native nonmonocot angiosperm species on the Canaries, and all of the insular woody species have paedomorphic wood features in the Carlquistian sense, although this wood anatomical syndrome might be more related to particular life forms. The majority of these insular woody groups typically grow in the markedly dry lowland regions, suggesting a possible link between secondary woodiness and increased drought resistance.Conclusions. The Canary Island flora is characterized by at least 38 independent shifts toward insular woodiness, representing an important portion of the endemic angiosperms on the archipelago. These convergent evolutionary events emphasize the remarkable lability in growth forms between herbaceous and woody lineages, but it remains puzzling which environmental variables trigger these shifts and how these independent shifts are regulated genetically.",
		"container-title": "International Journal of Plant Sciences",
		"DOI": "10.1086/670259",
		"ISSN": "1058-5893",
		"issue": "7",
		"note": "publisher: The University of Chicago Press",
		"page": "992-1013",
		"source": "journals.uchicago.edu (Atypon)",
		"title": "Insular Woodiness on the Canary Islands: A Remarkable Case of Convergent Evolution",
		"title-short": "Insular Woodiness on the Canary Islands",
		"URL": "https://www.journals.uchicago.edu/doi/10.1086/670259",
		"volume": "174",
		"author": [
			{
				"family": "Lens",
				"given": "Frederic"
			},
			{
				"family": "Davin",
				"given": "Nicolas"
			},
			{
				"family": "Smets",
				"given": "Erik"
			},
			{
				"family": "Arco",
				"given": "Marcelino",
				"non-dropping-particle": "del"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013",
					9,
					1
				]
			]
		}
	},
	{
		"id": "wulf2015",
		"type": "book",
		"abstract": "The acclaimed author of Founding Gardeners reveals the forgotten life of Alexander von Humboldt, the visionary German naturalist whose id...",
		"title": "The Invention of Nature",
		"URL": "https://www.goodreads.com/work/best_book/43595986-the-invention-of-nature-alexander-von-humboldt-s-new-world",
		"author": [
			{
				"family": "Wulf",
				"given": "Andrea"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "isaacson2014",
		"type": "book",
		"abstract": "The computer and the internet are among the most important innovations of our era, but few people know who created them. They were not co...",
		"publisher": "Simon & Schuster",
		"title": "The Innovators",
		"URL": "https://www.goodreads.com/work/best_book/41129225-the-innovators-how-a-group-of-inventors-hackers-geniuses-and-geeks-cr",
		"author": [
			{
				"family": "Isaacson",
				"given": "Walter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "hadzigeorgiou2014a",
		"type": "article-journal",
		"abstract": "The unique contributions of romanticism and romantic science have been generally ignored or undervalued in history and philosophy of science studies and science education. Although more recent research in history of science has come to delineate the value of both topics for the development of modern science, their merit for the educational field has not been explored. Romanticism was not only an obvious historical period, but a particular state of mind with its own extraordinary emotional sensitivity towards nature. It is especially the latter which we hope to revisit and reclaim for science education. After discussing several key historical contributions, we describe nine characteristics of ‘Romantic Science’ in order to focus on six ideas/possibilities that we believe hold much value for transforming current science education: (1) the emotional sensitivity toward nature, (2) the centrality of sense experience, (3) the importance of “holistic experience”, (4) the importance of the notions of mystery and wonder, (5) the power of science to transform people’s outlook on the natural world, and (6) the importance of the relationship between science and philosophy. It is argued that in view of a pragmatist/utilitarian conception of school science prevalent today the aforementioned ideas (especially the notion of wonder and the poetic/non-analytical mode of knowledge), can provide food for thought for both science teachers and researchers seeking to work out an aesthetic conception, one that complements current approaches such as inquiry science and conceptual change.",
		"container-title": "Science & Education",
		"DOI": "10.1007/s11191-014-9711-0",
		"ISSN": "1573-1901",
		"issue": "10",
		"journalAbbreviation": "Sci & Educ",
		"language": "en",
		"page": "1963-2006",
		"source": "Springer Link",
		"title": "Romanticism and Romantic Science: Their Contribution to Science Education",
		"title-short": "Romanticism and Romantic Science",
		"URL": "https://doi.org/10.1007/s11191-014-9711-0",
		"volume": "23",
		"author": [
			{
				"family": "Hadzigeorgiou",
				"given": "Yannis"
			},
			{
				"family": "Schulz",
				"given": "Roland"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					10,
					1
				]
			]
		}
	},
	{
		"id": "longrich2016",
		"type": "article-journal",
		"abstract": "The end-Cretaceous mass extinction ranks among the most severe extinctions of all time; however, patterns of extinction and recovery remain incompletely understood. In particular, it is unclear how severe the extinction was, how rapid the recovery was and how sampling biases might affect our understanding of these processes. To better understand terrestrial extinction and recovery and how sampling influences these patterns, we collected data on the occurrence and abundance of fossil mammals to examine mammalian diversity across the K-Pg boundary in North America. Our data show that the extinction was more severe and the recovery more rapid than previously thought. Extinction rates are markedly higher than previously estimated: of 59 species, four survived (93% species extinction, 86% of genera). Survival is correlated with geographic range size and abundance, with widespread, common species tending to survive. This creates a sampling artefact in which rare species are both more vulnerable to extinction and less likely to be recovered, such that the fossil record is inherently biased towards the survivors. The recovery was remarkably rapid. Within 300 000 years, local diversity recovered and regional diversity rose to twice Cretaceous levels, driven by increased endemicity; morphological disparity increased above levels observed in the Cretaceous. The speed of the recovery tends to be obscured by sampling effects; faunas show increased endemicity, such that a rapid, regional increase in diversity and disparity is not seen in geographically restricted studies. Sampling biases that operate against rare taxa appear to obscure the severity of extinction and the pace of recovery across the K-Pg boundary, and similar biases may operate during other extinction events.",
		"container-title": "Journal of Evolutionary Biology",
		"DOI": "https://doi.org/10.1111/jeb.12882",
		"ISSN": "1420-9101",
		"issue": "8",
		"language": "en",
		"license": "© 2016 European Society For Evolutionary Biology. Journal of Evolutionary Biology © 2016 European Society For Evolutionary Biology",
		"note": "_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jeb.12882",
		"page": "1495-1512",
		"source": "Wiley Online Library",
		"title": "Severe extinction and rapid recovery of mammals across the Cretaceous–Palaeogene boundary, and the effects of rarity on patterns of extinction and recovery",
		"URL": "https://onlinelibrary.wiley.com/doi/abs/10.1111/jeb.12882",
		"volume": "29",
		"author": [
			{
				"family": "Longrich",
				"given": "N. R."
			},
			{
				"family": "Scriberas",
				"given": "J."
			},
			{
				"family": "Wills",
				"given": "M. A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "kepler1618",
		"type": "book",
		"abstract": "Book 2 has half-title: Epitomes astronomiae copernicanae liber secundus, de sphaera & circulis eius; [Pt. 2] has imprint: Lentiis ad Danubium; Impensis G. Tampachii, excudebat I. Plancus, 1622; [pt. 3]: Francofvrti, Sumptibus G. Tampachij, 1621; [pars 1] Liber 1-3. De doctrina sphaericâ.- [pars 2] Liber 4. Physica coelestis.- [pars 3] Liber 5-7. Doctrina theorica; Errors in paging: nos. 401-408, 781-784 omitted, 767-768, 776-777 duplicated; 32; NEW",
		"call-number": "gal 00228",
		"language": "lat",
		"number-of-pages": "446",
		"publisher": "Lentijs ad Danubium, Excudebat J. Plancus",
		"source": "Internet Archive",
		"title": "Epitome astronomiae copernicanae, usitatâ formâ quaestionum & responsionum conscripta, inq; VII. libros digesta ..",
		"URL": "http://archive.org/details/epitomeastronomi01kepl",
		"author": [
			{
				"family": "Kepler",
				"given": "Johannes"
			}
		],
		"contributor": [
			{
				"literal": "Fisher - University of Toronto"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1618"
				]
			]
		}
	},
	{
		"id": "bruno1584",
		"type": "book",
		"language": "it",
		"source": "Zotero",
		"title": "De l’infinito, universo e mondi",
		"author": [
			{
				"family": "Bruno",
				"given": "Giordano"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1584"
				]
			]
		}
	},
	{
		"id": "newton1687",
		"type": "book",
		"abstract": "Also available in digital form on the Library of Congress Web site. Signatures: [A]-Z⁴, (H₂ lettered G₂), Aa-Zz⁴, ***⁴, Aaa-Ooo⁴. The second and third books were apparently printed by different printers as indicated by the different type in the headings and the break in paging between the two books. Imperfect: folded plate wanting. Sig. Fff reinforced with crepeline and badly discolored. \"E libris Collegii Episcopalis a Societate de propagando apud exteros evangelio prope Calcuttam fundati.\" ESTC R5707 LAC scc 2018-11-09 update (1 card) LAC kcc 2018-12-06 review LC Review of LAC Completed/Approved 2018-12-10 LAC ael 2021-05-13 update (1 card)0",
		"title": "Philosophiae naturalis principia mathematica.",
		"URL": "https://www.loc.gov/item/28020872/",
		"author": [
			{
				"family": "Newton",
				"given": "Isaac"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1687"
				]
			]
		}
	},
	{
		"id": "buchholz2004",
		"type": "article-journal",
		"abstract": "The aim of this review was to evaluate data regarding potential thermodynamic mechanisms for increased rates of weight loss in subjects consuming diets high in protein and/or low in carbohydrate. Studies that compared weight loss and energy expenditure in adults consuming diets high in protein and/or low in carbohydrate with those in adults consuming diets low in fat were reviewed. In addition, studies that measured the metabolizable energy of proteins, fats, and carbohydrates were reviewed. Diets high in protein and/or low in carbohydrate produced an ≈2.5-kg greater weight loss after 12 wk of treatment. Neither macronutrient-specific differences in the availability of dietary energy nor changes in energy expenditure could explain these differences in weight loss. Thermodynamics dictate that a calorie is a calorie regardless of the macronutrient composition of the diet. Further research on differences in the composition of weight loss and on the influence of satiety on compliance with energy-restricted diets is needed to explain the observed increase in weight loss with diets high in protein and/or low in carbohydrate.",
		"container-title": "The American Journal of Clinical Nutrition",
		"DOI": "10.1093/ajcn/79.5.899S",
		"ISSN": "0002-9165",
		"issue": "5",
		"journalAbbreviation": "The American Journal of Clinical Nutrition",
		"page": "899S-906S",
		"source": "Silverchair",
		"title": "Is a calorie a calorie?",
		"URL": "https://doi.org/10.1093/ajcn/79.5.899S",
		"volume": "79",
		"author": [
			{
				"family": "Buchholz",
				"given": "Andrea C"
			},
			{
				"family": "Schoeller",
				"given": "Dale A"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2004",
					5,
					1
				]
			]
		}
	},
	{
		"id": "dyson1999",
		"type": "book",
		"abstract": "Includes bibliographical references (p. [119]-124)",
		"ISBN": "978-0-19-512942-7",
		"language": "eng",
		"number-of-pages": "154",
		"publisher": "New York : New York Public Library : Oxford University Press",
		"source": "Internet Archive",
		"title": "The sun, the genome & the Internet : tools of scientific revolutions",
		"title-short": "The sun, the genome & the Internet",
		"URL": "http://archive.org/details/sungenomeinterne00dyso",
		"author": [
			{
				"family": "Dyson",
				"given": "Freeman J."
			}
		],
		"contributor": [
			{
				"literal": "Internet Archive"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1999"
				]
			]
		}
	},
	{
		"id": "humboldt1807",
		"type": "graphic",
		"source": "Zentralbibliothek Zürich",
		"title": "Ein Naturgemälde der Anden",
		"URL": "https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/Zentralbibliothek_Z%C3%BCrich_-_Ideen_zu_einer_Geographie_der_Pflanzen_nebst_einem_Naturgem%C3%A4lde_der_Tropenl%C3%A4nder_-_000012142.jpg/2560px-Zentralbibliothek_Z%C3%BCrich_-_Ideen_zu_einer_Geographie_der_Pflanzen_nebst_einem_Naturgem%C3%A4lde_der_Tropenl%C3%A4nder_-_000012142.jpg",
		"author": [
			{
				"family": "Humboldt",
				"given": "Alexander",
				"dropping-particle": "von"
			}
		],
		"contributor": [
			{
				"family": "Bonpland",
				"given": "Aimé",
				"suffix": "Arzt"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1807"
				]
			]
		}
	},
	{
		"id": "oersted",
		"type": "entry-encyclopedia",
		"abstract": "Hans Christian Ørsted, Danish physicist and chemist who discovered that electric current in a wire can deflect a magnetized compass needle, a phenomenon the importance of which was rapidly recognized and which inspired the development of electromagnetic theory. In 1806 Ørsted became a professor at",
		"container-title": "Encyclopedia Britannica",
		"language": "en",
		"note": "Citation-Key: oersted",
		"title": "Hans Christian Ørsted",
		"URL": "https://www.britannica.com/biography/Hans-Christian-Orsted",
		"author": [
			{
				"family": "Britannica",
				"given": "The Editors of Encyclopaedia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					22
				]
			]
		}
	},
	{
		"id": "darwin1859",
		"type": "book",
		"event-place": "London",
		"publisher": "John Murray",
		"publisher-place": "London",
		"title": "On the Origin of Species",
		"author": [
			{
				"family": "Darwin",
				"given": "Charles"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1859"
				]
			]
		}
	},
	{
		"id": "mendeleev1867",
		"type": "book",
		"number-of-volumes": "4",
		"title": "The principles of chemistry",
		"URL": "https://archive.org/details/principlesofchem00menduoft",
		"volume": "1",
		"author": [
			{
				"family": "Mendeleev",
				"given": "Dmitrii Ivanovich",
				"suffix": ""
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1867"
				]
			]
		}
	},
	{
		"id": "dirac1928",
		"type": "article-journal",
		"container-title": "The Royal Society",
		"issue": "778",
		"page": "610–624",
		"title": "The quantum theory of the electron",
		"volume": "117",
		"author": [
			{
				"family": "Dirac",
				"given": "Paul Adrien Maurice"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1928"
				]
			]
		}
	},
	{
		"id": "carithers1995",
		"type": "article-magazine",
		"container-title": "SLAC National Accelerator Laboratory",
		"title": "Discovery of the top quark",
		"URL": "https://www.slac.stanford.edu/pubs/beamline/25/3/25-3-carithers.pdf",
		"author": [
			{
				"family": "Carithers",
				"given": "Bill"
			},
			{
				"family": "Grannis",
				"given": "Paul"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1995"
				]
			]
		}
	},
	{
		"id": "bernoulli1738",
		"type": "book",
		"language": "la",
		"note": "Google-Books-ID: 7zEVAAAAQAAJ",
		"number-of-pages": "356",
		"source": "Google Books",
		"title": "Hydrodynamica: sive de viribus et motibus fluidorum commentarii",
		"title-short": "Hydrodynamica",
		"author": [
			{
				"family": "Bernoulli",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1738"
				]
			]
		}
	},
	{
		"id": "sepkoski1993",
		"type": "article-journal",
		"abstract": "A comparison is made between compilations of times of origination and extinction of fossil marine animal families published in 1982 and 1992. As a result of ten years of library research, half of the information in the compendia has changed: families have been added and deleted, low-resolution stratigraphic data have been improved, and intervals of origination and extinction have been altered. Despite these changes, apparent macroevolutionary patterns for the entire marine fauna have remained constant. Diversity curves compiled from the two data bases are very similar, with a goodness-of-fit of 99%; the principal difference is that the 1992 curve averages 13% higher than the older curve. Both numbers and percentages of origination and extinction also match well, with fits ranging from 83% to 95%. All major events of radiation and extinction are identical. Therefore, errors in large paleontological data bases and arbitrariness of included taxa are not necessarily impediments to the analysis of pattern in the fossil record, so long as the data are sufficiently numerous.",
		"container-title": "Paleobiology",
		"ISSN": "0094-8373",
		"issue": "1",
		"note": "publisher: Paleontological Society",
		"page": "43-51",
		"source": "JSTOR",
		"title": "Ten Years in the Library: New Data Confirm Paleontological Patterns",
		"title-short": "Ten Years in the Library",
		"URL": "https://www.jstor.org/stable/2400770",
		"volume": "19",
		"author": [
			{
				"family": "Sepkoski",
				"given": "J. John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1993"
				]
			]
		}
	},
	{
		"id": "hardin1960",
		"type": "article-journal",
		"container-title": "Science",
		"title": "The Competitive Exclusion Principle",
		"URL": "https://www.esf.edu/efb/schulz/seminars/hardin.pdf",
		"volume": "131",
		"author": [
			{
				"family": "Hardin",
				"given": "Garrett"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1960"
				]
			]
		}
	},
	{
		"id": "leading-killers",
		"type": "report",
		"number": "LCWK9",
		"publisher": "CDC",
		"title": "Deaths, Percent of Total Deaths, and Death Rates for the 15 Leading Causes of Death: United States and Each State, 2015",
		"URL": "https://www.cdc.gov/nchs/data/dvs/LCWK9_2015.pdf",
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "lung-cancer",
		"type": "webpage",
		"abstract": "Get the American Cancer Society's latest statistics on lung cancer. Learn how common lung cancer is and what your lifetime chance of getting it may be.",
		"container-title": "American Cancer Society",
		"language": "en",
		"title": "Key Statistics for Lung Cancer",
		"URL": "https://www.cancer.org/cancer/lung-cancer/about/key-statistics.html",
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					1,
					12
				]
			]
		}
	},
	{
		"id": "martin2021",
		"type": "article-journal",
		"container-title": "Health Affairs",
		"DOI": "10.1377/hlthaff.2020.02022",
		"ISSN": "0278-2715, 1544-5208",
		"issue": "1",
		"journalAbbreviation": "Health Affairs",
		"language": "en",
		"page": "14-24",
		"source": "DOI.org (Crossref)",
		"title": "National Health Care Spending In 2019: Steady Growth For The Fourth Consecutive Year: Study examines national health care spending for 2019.",
		"title-short": "National Health Care Spending In 2019",
		"URL": "http://www.healthaffairs.org/doi/10.1377/hlthaff.2020.02022",
		"volume": "40",
		"author": [
			{
				"family": "Martin",
				"given": "Anne B."
			},
			{
				"family": "Hartman",
				"given": "Micah"
			},
			{
				"family": "Lassman",
				"given": "David"
			},
			{
				"family": "Catlin",
				"given": "Aaron"
			},
			{
				"literal": "The National Health Expenditure Accounts Team"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					1,
					1
				]
			]
		}
	},
	{
		"id": "life-expectancy",
		"type": "report",
		"abstract": "Search for trend data in Health, United States by health topic and population subgroup.",
		"language": "en-us",
		"number": "004",
		"publisher": "CDC",
		"title": "Life expectancy at birth, at age 65, and at age 75, by sex, race, and Hispanic origin: United States, selected years 1900–2018",
		"URL": "https://www.cdc.gov/nchs/hus/contents2019.htm",
		"accessed": {
			"date-parts": [
				[
					"2021",
					5,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "puzziferri2014",
		"type": "article-journal",
		"abstract": "Bariatric surgery is an accepted treatment for obesity. Despite extensive literature, few studies report long-term follow-up in cohorts with adequate retention rates.To assess the quality of evidence and treatment effectiveness 2 years after bariatric procedures for weight loss, type 2 diabetes, hypertension, and hyperlipidemia in severely obese adults.MEDLINE and Cochrane databases were searched from 1946 through May 15, 2014. Search terms included bariatric surgery, individual bariatric procedures, and obesity. Studies were included if they described outcomes for gastric bypass, gastric band, or sleeve gastrectomy performed on patients with a body mass index of 35 or greater, had more than 2 years of outcome information, and had follow-up measures for at least 80% of the initial cohort. Two investigators reviewed each study and a third resolved study inclusion disagreements.Of 7371 clinical studies reviewed, 29 studies (0.4%, 7971 patients) met inclusion criteria. All gastric bypass studies (6 prospective cohorts, 5 retrospective cohorts) and sleeve gastrectomy studies (2 retrospective cohorts) had 95% confidence intervals for the reported mean, median, or both exceeding 50% excess weight loss. This amount of excess weight loss occurred in 31% of gastric band studies (9 prospective cohorts, 5 retrospective cohorts). The mean sample-size–weighted percentage of excess weight loss for gastric bypass was 65.7% (n = 3544) vs 45.0% (n = 4109) for gastric band. Nine studies measured comorbidity improvement. For type 2 diabetes (glycated hemoglobin &lt;6.5% without medication), sample-size–weighted remission rates were 66.7% for gastric bypass (n = 428) and 28.6% for gastric band (n = 96). For hypertension (blood pressure &lt;140/90 mm Hg without medication), remission rates were 38.2% for gastric bypass ( n = 808) and 17.4% for gastric band (n = 247). For hyperlipidemia (cholesterol &lt;200 mg/dL, high-density lipoprotein &gt;40 mg/dL, low-density lipoprotein &lt;160 mg/dL, and triglycerides &lt;200 mg/dL), remission rates were 60.4% for gastric bypass (n = 477) and 22.7% for gastric band (n = 97).Very few bariatric surgery studies report long-term results with sufficient patient follow-up to minimize biased results. Gastric bypass has better outcomes than gastric band procedures for long-term weight loss, type 2 diabetes control and remission, hypertension, and hyperlipidemia. Insufficient evidence exists regarding long-term outcomes for gastric sleeve resections.",
		"container-title": "JAMA",
		"DOI": "10.1001/jama.2014.10706",
		"ISSN": "0098-7484",
		"issue": "9",
		"journalAbbreviation": "JAMA",
		"page": "934-942",
		"source": "Silverchair",
		"title": "Long-term Follow-up After Bariatric Surgery: A Systematic Review",
		"title-short": "Long-term Follow-up After Bariatric Surgery",
		"URL": "https://doi.org/10.1001/jama.2014.10706",
		"volume": "312",
		"author": [
			{
				"family": "Puzziferri",
				"given": "Nancy"
			},
			{
				"family": "Roshek",
				"given": "Thomas B.",
				"suffix": "III"
			},
			{
				"family": "Mayo",
				"given": "Helen G."
			},
			{
				"family": "Gallagher",
				"given": "Ryan"
			},
			{
				"family": "Belle",
				"given": "Steven H."
			},
			{
				"family": "Livingston",
				"given": "Edward H."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					9,
					3
				]
			]
		}
	},
	{
		"id": "buchwald2004",
		"type": "article-journal",
		"abstract": "ContextAbout 5% of the US population is morbidly obese. This disease remains largely refractory to diet and drug therapy, but generally responds well to\nbariatric surgery.ObjectiveTo determine the impact of bariatric surgery on weight loss, operative\nmortality outcome, and 4 obesity comorbidities (diabetes, hyperlipidemia,\nhypertension, and obstructive sleep apnea).Data Sources and Study SelectionElectronic literature search of MEDLINE, Current Contents, and the Cochrane\nLibrary databases plus manual reference checks of all articles on bariatric\nsurgery published in the English language between 1990 and 2003. Two levels\nof screening were used on 2738 citations.Data ExtractionA total of 136 fully extracted studies, which included 91 overlapping\npatient populations (kin studies), were included for a total of 22 094\npatients. Nineteen percent of the patients were men and 72.6% were women,\nwith a mean age of 39 years (range, 16-64 years). Sex was not reported for\n1537 patients (8%). The baseline mean body mass index for 16 944 patients\nwas 46.9 (range, 32.3-68.8).Data SynthesisA random effects model was used in the meta-analysis. The mean (95%\nconfidence interval) percentage of excess weight loss was 61.2% (58.1%-64.4%)\nfor all patients; 47.5% (40.7%-54.2%) for patients who underwent gastric banding;\n61.6% (56.7%-66.5%), gastric bypass; 68.2% (61.5%-74.8%), gastroplasty; and\n70.1% (66.3%-73.9%), biliopancreatic diversion or duodenal switch. Operative\nmortality (≤30 days) in the extracted studies was 0.1% for the purely restrictive\nprocedures, 0.5% for gastric bypass, and 1.1% for biliopancreatic diversion\nor duodenal switch. Diabetes was completely resolved in 76.8% of patients\nand resolved or improved in 86.0%. Hyperlipidemia improved in 70% or more\nof patients. Hypertension was resolved in 61.7% of patients and resolved or\nimproved in 78.5%. Obstructive sleep apnea was resolved in 85.7% of patients\nand was resolved or improved in 83.6% of patients.ConclusionsEffective weight loss was achieved in morbidly obese patients after\nundergoing bariatric surgery. A substantial majority of patients with diabetes,\nhyperlipidemia, hypertension, and obstructive sleep apnea experienced complete\nresolution or improvement.",
		"container-title": "JAMA",
		"DOI": "10.1001/jama.292.14.1724",
		"ISSN": "0098-7484",
		"issue": "14",
		"journalAbbreviation": "JAMA",
		"page": "1724-1737",
		"source": "Silverchair",
		"title": "Bariatric SurgeryA Systematic Review and Meta-analysis",
		"URL": "https://doi.org/10.1001/jama.292.14.1724",
		"volume": "292",
		"author": [
			{
				"family": "Buchwald",
				"given": "Henry"
			},
			{
				"family": "Avidor",
				"given": "Yoav"
			},
			{
				"family": "Braunwald",
				"given": "Eugene"
			},
			{
				"family": "Jensen",
				"given": "Michael D."
			},
			{
				"family": "Pories",
				"given": "Walter"
			},
			{
				"family": "Fahrbach",
				"given": "Kyle"
			},
			{
				"family": "Schoelles",
				"given": "Karen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2004",
					10,
					13
				]
			]
		}
	},
	{
		"id": "livingston2005",
		"type": "article-journal",
		"abstract": "The rise in bariatric operations has been exponential, with a greater acceptance for\nthese procedures [1,2]. Although complication rates are relatively low (major complications\noccur in approximately 10% of procedures), they can result in formidable disability\n[1,3–6]. Adverse outcomes also result in medical malpractice claims that are particularly\nproblematic for bariatric surgery practices. For these reasons, surgeons performing\nthese operations must be knowledgeable and must possess the technical skills required\nfor managing complications when they occur.",
		"container-title": "Surgical Clinics",
		"DOI": "10.1016/j.suc.2005.04.007",
		"ISSN": "0039-6109, 1558-3171",
		"issue": "4",
		"journalAbbreviation": "Surgical Clinics",
		"language": "English",
		"note": "publisher: Elsevier\nPMID: 16061090",
		"page": "853-868",
		"source": "www.surgical.theclinics.com",
		"title": "Complications of Bariatric Surgery",
		"URL": "https://www.surgical.theclinics.com/article/S0039-6109(05)00070-8/abstract",
		"volume": "85",
		"author": [
			{
				"family": "Livingston",
				"given": "Edward H."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2005",
					8,
					1
				]
			]
		}
	},
	{
		"id": "xu2020",
		"type": "article-journal",
		"abstract": "<h3>Abstract</h3> <p>The neural circuits responsible for behavior remain largely unknown. Previous efforts have reconstructed the complete circuits of small animals, with hundreds of neurons, and selected circuits for larger animals. Here we (the FlyEM project at Janelia and collaborators at Google) summarize new methods and present the complete circuitry of a large fraction of the brain of a much more complex animal, the fruit fly <i>Drosophila melanogaster</i>. Improved methods include new procedures to prepare, image, align, segment, find synapses, and proofread such large data sets; new methods that define cell types based on connectivity in addition to morphology; and new methods to simplify access to a large and evolving data set. From the resulting data we derive a better definition of computational compartments and their connections; an exhaustive atlas of cell examples and types, many of them novel; detailed circuits for most of the central brain; and exploration of the statistics and structure of different brain compartments, and the brain as a whole. We make the data public, with a web site and resources specifically designed to make it easy to explore, for all levels of expertise from the expert to the merely curious. The public availability of these data, and the simplified means to access it, dramatically reduces the effort needed to answer typical circuit questions, such as the identity of upstream and downstream neural partners, the circuitry of brain regions, and to link the neurons defined by our analysis with genetic reagents that can be used to study their functions.</p><p>Note: In the next few weeks, we will release a series of papers with more involved discussions. One paper will detail the hemibrain reconstruction with more extensive analysis and interpretation made possible by this dense connectome. Another paper will explore the central complex, a brain region involved in navigation, motor control, and sleep. A final paper will present insights from the mushroom body, a center of multimodal associative learning in the fly brain.</p>",
		"container-title": "bioRxiv",
		"DOI": "10.1101/2020.01.21.911859",
		"language": "en",
		"license": "© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/",
		"note": "publisher: Cold Spring Harbor Laboratory\nsection: New Results",
		"page": "2020.01.21.911859",
		"source": "www.biorxiv.org",
		"title": "A Connectome of the Adult Drosophila Central Brain",
		"URL": "https://www.biorxiv.org/content/10.1101/2020.01.21.911859v1",
		"author": [
			{
				"family": "Xu",
				"given": "C. Shan"
			},
			{
				"family": "Januszewski",
				"given": "Michal"
			},
			{
				"family": "Lu",
				"given": "Zhiyuan"
			},
			{
				"family": "Takemura",
				"given": "Shin-ya"
			},
			{
				"family": "Hayworth",
				"given": "Kenneth J."
			},
			{
				"family": "Huang",
				"given": "Gary"
			},
			{
				"family": "Shinomiya",
				"given": "Kazunori"
			},
			{
				"family": "Maitin-Shepard",
				"given": "Jeremy"
			},
			{
				"family": "Ackerman",
				"given": "David"
			},
			{
				"family": "Berg",
				"given": "Stuart"
			},
			{
				"family": "Blakely",
				"given": "Tim"
			},
			{
				"family": "Bogovic",
				"given": "John"
			},
			{
				"family": "Clements",
				"given": "Jody"
			},
			{
				"family": "Dolafi",
				"given": "Tom"
			},
			{
				"family": "Hubbard",
				"given": "Philip"
			},
			{
				"family": "Kainmueller",
				"given": "Dagmar"
			},
			{
				"family": "Katz",
				"given": "William"
			},
			{
				"family": "Kawase",
				"given": "Takashi"
			},
			{
				"family": "Khairy",
				"given": "Khaled A."
			},
			{
				"family": "Leavitt",
				"given": "Laramie"
			},
			{
				"family": "Li",
				"given": "Peter H."
			},
			{
				"family": "Lindsey",
				"given": "Larry"
			},
			{
				"family": "Neubarth",
				"given": "Nicole"
			},
			{
				"family": "Olbris",
				"given": "Donald J."
			},
			{
				"family": "Otsuna",
				"given": "Hideo"
			},
			{
				"family": "Troutman",
				"given": "Eric T."
			},
			{
				"family": "Umayam",
				"given": "Lowell"
			},
			{
				"family": "Zhao",
				"given": "Ting"
			},
			{
				"family": "Ito",
				"given": "Masayoshi"
			},
			{
				"family": "Goldammer",
				"given": "Jens"
			},
			{
				"family": "Wolff",
				"given": "Tanya"
			},
			{
				"family": "Svirskas",
				"given": "Robert"
			},
			{
				"family": "Schlegel",
				"given": "Philipp"
			},
			{
				"family": "Neace",
				"given": "Erika R."
			},
			{
				"family": "Knecht",
				"given": "Christopher J."
			},
			{
				"family": "Alvarado",
				"given": "Chelsea X."
			},
			{
				"family": "Bailey",
				"given": "Dennis A."
			},
			{
				"family": "Ballinger",
				"given": "Samantha"
			},
			{
				"family": "Borycz",
				"given": "Jolanta A."
			},
			{
				"family": "Canino",
				"given": "Brandon S."
			},
			{
				"family": "Cheatham",
				"given": "Natasha"
			},
			{
				"family": "Cook",
				"given": "Michael"
			},
			{
				"family": "Dreher",
				"given": "Marisa"
			},
			{
				"family": "Duclos",
				"given": "Octave"
			},
			{
				"family": "Eubanks",
				"given": "Bryon"
			},
			{
				"family": "Fairbanks",
				"given": "Kelli"
			},
			{
				"family": "Finley",
				"given": "Samantha"
			},
			{
				"family": "Forknall",
				"given": "Nora"
			},
			{
				"family": "Francis",
				"given": "Audrey"
			},
			{
				"family": "Hopkins",
				"given": "Gary Patrick"
			},
			{
				"family": "Joyce",
				"given": "Emily M."
			},
			{
				"family": "Kim",
				"given": "SungJin"
			},
			{
				"family": "Kirk",
				"given": "Nicole A."
			},
			{
				"family": "Kovalyak",
				"given": "Julie"
			},
			{
				"family": "Lauchie",
				"given": "Shirley A."
			},
			{
				"family": "Lohff",
				"given": "Alanna"
			},
			{
				"family": "Maldonado",
				"given": "Charli"
			},
			{
				"family": "Manley",
				"given": "Emily A."
			},
			{
				"family": "McLin",
				"given": "Sari"
			},
			{
				"family": "Mooney",
				"given": "Caroline"
			},
			{
				"family": "Ndama",
				"given": "Miatta"
			},
			{
				"family": "Ogundeyi",
				"given": "Omotara"
			},
			{
				"family": "Okeoma",
				"given": "Nneoma"
			},
			{
				"family": "Ordish",
				"given": "Christopher"
			},
			{
				"family": "Padilla",
				"given": "Nicholas"
			},
			{
				"family": "Patrick",
				"given": "Christopher"
			},
			{
				"family": "Paterson",
				"given": "Tyler"
			},
			{
				"family": "Phillips",
				"given": "Elliott E."
			},
			{
				"family": "Phillips",
				"given": "Emily M."
			},
			{
				"family": "Rampally",
				"given": "Neha"
			},
			{
				"family": "Ribeiro",
				"given": "Caitlin"
			},
			{
				"family": "Robertson",
				"given": "Madelaine K."
			},
			{
				"family": "Rymer",
				"given": "Jon Thomson"
			},
			{
				"family": "Ryan",
				"given": "Sean M."
			},
			{
				"family": "Sammons",
				"given": "Megan"
			},
			{
				"family": "Scott",
				"given": "Anne K."
			},
			{
				"family": "Scott",
				"given": "Ashley L."
			},
			{
				"family": "Shinomiya",
				"given": "Aya"
			},
			{
				"family": "Smith",
				"given": "Claire"
			},
			{
				"family": "Smith",
				"given": "Kelsey"
			},
			{
				"family": "Smith",
				"given": "Natalie L."
			},
			{
				"family": "Sobeski",
				"given": "Margaret A."
			},
			{
				"family": "Suleiman",
				"given": "Alia"
			},
			{
				"family": "Swift",
				"given": "Jackie"
			},
			{
				"family": "Takemura",
				"given": "Satoko"
			},
			{
				"family": "Talebi",
				"given": "Iris"
			},
			{
				"family": "Tarnogorska",
				"given": "Dorota"
			},
			{
				"family": "Tenshaw",
				"given": "Emily"
			},
			{
				"family": "Tokhi",
				"given": "Temour"
			},
			{
				"family": "Walsh",
				"given": "John J."
			},
			{
				"family": "Yang",
				"given": "Tansy"
			},
			{
				"family": "Horne",
				"given": "Jane Anne"
			},
			{
				"family": "Li",
				"given": "Feng"
			},
			{
				"family": "Parekh",
				"given": "Ruchi"
			},
			{
				"family": "Rivlin",
				"given": "Patricia K."
			},
			{
				"family": "Jayaraman",
				"given": "Vivek"
			},
			{
				"family": "Ito",
				"given": "Kei"
			},
			{
				"family": "Saalfeld",
				"given": "Stephan"
			},
			{
				"family": "George",
				"given": "Reed"
			},
			{
				"family": "Meinertzhagen",
				"given": "Ian"
			},
			{
				"family": "Rubin",
				"given": "Gerald M."
			},
			{
				"family": "Hess",
				"given": "Harald F."
			},
			{
				"family": "Scheffer",
				"given": "Louis K."
			},
			{
				"family": "Jain",
				"given": "Viren"
			},
			{
				"family": "Plaza",
				"given": "Stephen M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					1,
					21
				]
			]
		}
	},
	{
		"id": "floridi2020",
		"type": "article-journal",
		"container-title": "Philosophy & Technology",
		"DOI": "10.1007/s13347-020-00396-6",
		"ISSN": "2210-5441",
		"issue": "1",
		"journalAbbreviation": "Philos. Technol.",
		"language": "en",
		"page": "1-3",
		"source": "Springer Link",
		"title": "AI and Its New Winter: from Myths to Realities",
		"title-short": "AI and Its New Winter",
		"URL": "https://doi.org/10.1007/s13347-020-00396-6",
		"volume": "33",
		"author": [
			{
				"family": "Floridi",
				"given": "Luciano"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					3,
					1
				]
			]
		}
	},
	{
		"id": "chalmers1995",
		"type": "webpage",
		"abstract": "To make progress on the problem of consciousness, we have to confront it directly. In this paper, I first isolate the truly hard part of the problem, separating it from more tractable parts and giving an account of why it is so difficult to explain. I critique some recent work that uses reductive methods to address consciousness, and argue that such methods inevitably fail to come to grips with the hardest part of the problem. Once this failure is recognized, the door to further progress is opened. In the second half of the paper, I argue that if we move to a new kind of nonreductive explanation, a naturalistic account of consciousness can be given. I put forward my own candidate for such an account: a nonreductive theory based on principles of structural coherence and organizational invariance, and a double-aspect theory of information.",
		"container-title": "Journal of Consciousness Studies",
		"genre": "Journal (Paginated)",
		"note": "issue: 3\nnumber: 3\npage: 200-219\nvolume: 2",
		"title": "Facing Up to the Problem of Consciousness",
		"URL": "http://cogprints.org/316/",
		"author": [
			{
				"family": "Chalmers",
				"given": "David J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1995"
				]
			]
		}
	},
	{
		"id": "friston2010",
		"type": "article-journal",
		"abstract": "Adaptive agents must occupy a limited repertoire of states and therefore minimize the long-term average of surprise associated with sensory exchanges with the world. Minimizing surprise enables them to resist a natural tendency to disorder.Surprise rests on predictions about sensations, which depend on an internal generative model of the world. Although surprise cannot be measured directly, a free-energy bound on surprise can be, suggesting that agents minimize free energy by changing their predictions (perception) or by changing the predicted sensory inputs (action).Perception optimizes predictions by minimizing free energy with respect to synaptic activity (perceptual inference), efficacy (learning and memory) and gain (attention and salience). This furnishes Bayes-optimal (probabilistic) representations of what caused sensations (providing a link to the Bayesian brain hypothesis).Bayes-optimal perception is mathematically equivalent to predictive coding and maximizing the mutual information between sensations and the representations of their causes. This is a probabilistic generalization of the principle of efficient coding (the infomax principle) or the minimum-redundancy principle.Learning under the free-energy principle can be formulated in terms of optimizing the connection strengths in hierarchical models of the sensorium. This rests on associative plasticity to encode causal regularities and appeals to the same synaptic mechanisms as those underlying cell assembly formation.Action under the free-energy principle reduces to suppressing sensory prediction errors that depend on predicted (expected or desired) movement trajectories. This provides a simple account of motor control, in which action is enslaved by perceptual (proprioceptive) predictions.Perceptual predictions rest on prior expectations about the trajectory or movement through the agent's state space. These priors can be acquired (as empirical priors during hierarchical inference) or they can be innate (epigenetic) and therefore subject to selective pressure.Predicted motion or state transitions realized by action correspond to policies in optimal control theory and reinforcement learning. In this context, value is inversely proportional to surprise (and implicitly free energy), and rewards correspond to innate priors that constrain policies.",
		"container-title": "Nature Reviews Neuroscience",
		"DOI": "10.1038/nrn2787",
		"ISSN": "1471-0048",
		"issue": "2",
		"journalAbbreviation": "Nat Rev Neurosci",
		"language": "en",
		"license": "2010 Nature Publishing Group",
		"note": "number: 2\npublisher: Nature Publishing Group",
		"page": "127-138",
		"source": "www.nature.com",
		"title": "The free-energy principle: a unified brain theory?",
		"title-short": "The free-energy principle",
		"URL": "https://www.nature.com/articles/nrn2787/boxes/bx1",
		"volume": "11",
		"author": [
			{
				"family": "Friston",
				"given": "Karl"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2010",
					2
				]
			]
		}
	},
	{
		"id": "lloyd2002",
		"type": "article-journal",
		"abstract": "Functional brain imaging offers new opportunities for the study of that most pervasive of cognitive conditions, human consciousness. Since consciousness is attendant to so much of human cognitive life, its study requires secondary analysis of multiple experimental datasets. Here, four preprocessed datasets from the National fMRI Data Center are considered: Hazeltine et al., Neural activation during response competition; Ishai et al., The representation of objects in the human occipital and temporal cortex; Mechelli et al., The effects of presentation rate during word and pseudoword reading; and Postle et al., Activity in human frontal cortex associated with spatial working memory and saccadic behavior. The study of consciousness also draws from multiple disciplines. In this article, the philosophical subdiscipline of phenomenology provides initial characterization of phenomenal structures conceptually necessary for an analysis of consciousness. These structures include phenomenal intentionality, phenomenal superposition, and experienced temporality. The empirical predictions arising from these structures require new interpretive methods for their confirmation. These methods begin with single-subject (preprocessed) scan series, and consider the patterns of all voxels as potential multivariate encodings of phenomenal information. Twenty-seven subjects from the four studies were analyzed with multivariate methods, revealing analogues of phenomenal structures, particularly the structures of temporality. In a second interpretive approach, artificial neural networks were used to detect a more explicit prediction from phenomenology, namely, that present experience contains and is inflected by past states of awareness and anticipated events. In all of 21 subjects in this analysis, nets were successfully trained to extract aspects of relative past and future brain states, in comparison with statistically similar controls. This exploratory study thus concludes that the proposed methods for “neurophenomenology” warrant further application, including the exploration of individual differences, multivariate differences between cognitive task conditions, and exploration of specific brain regions possibly contributing to the observations. All of these attractive questions, however, must be reserved for future research.",
		"container-title": "Journal of Cognitive Neuroscience",
		"DOI": "10.1162/089892902760191027",
		"ISSN": "0898-929X",
		"issue": "6",
		"journalAbbreviation": "Journal of Cognitive Neuroscience",
		"page": "818-831",
		"source": "Silverchair",
		"title": "Functional MRI and the Study of Human Consciousness",
		"URL": "https://doi.org/10.1162/089892902760191027",
		"volume": "14",
		"author": [
			{
				"family": "Lloyd",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2002",
					8,
					15
				]
			]
		}
	},
	{
		"id": "ali2020",
		"type": "paper-conference",
		"abstract": "We study the implicit regularization of mini-batch stochastic gradient descent, when applied to the fundamental problem of least squares regression. We leverage a continuous-time stochastic differe...",
		"container-title": "International Conference on Machine Learning",
		"event-title": "International Conference on Machine Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "233-244",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "The Implicit Regularization of Stochastic Gradient Flow for Least Squares",
		"URL": "http://proceedings.mlr.press/v119/ali20a.html",
		"author": [
			{
				"family": "Ali",
				"given": "Alnur"
			},
			{
				"family": "Dobriban",
				"given": "Edgar"
			},
			{
				"family": "Tibshirani",
				"given": "Ryan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					11,
					21
				]
			]
		}
	},
	{
		"id": "dayan2001",
		"type": "book",
		"call-number": "QP363.3 .D39 2001",
		"collection-title": "Computational neuroscience",
		"event-place": "Cambridge, Mass",
		"ISBN": "978-0-262-04199-7",
		"number-of-pages": "460",
		"publisher": "Massachusetts Institute of Technology Press",
		"publisher-place": "Cambridge, Mass",
		"source": "Library of Congress ISBN",
		"title": "Theoretical neuroscience: computational and mathematical modeling of neural systems",
		"title-short": "Theoretical neuroscience",
		"author": [
			{
				"family": "Dayan",
				"given": "Peter"
			},
			{
				"family": "Abbott",
				"given": "L. F."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2001"
				]
			]
		}
	},
	{
		"id": "gerstner2014",
		"type": "book",
		"call-number": "QP363 .G474 2014",
		"event-place": "Cambridge, United Kingdom",
		"ISBN": "978-1-107-06083-8",
		"number-of-pages": "577",
		"publisher": "Cambridge University Press",
		"publisher-place": "Cambridge, United Kingdom",
		"source": "Library of Congress ISBN",
		"title": "Neuronal dynamics: from single neurons to networks and models of cognition",
		"title-short": "Neuronal dynamics",
		"author": [
			{
				"family": "Gerstner",
				"given": "Wulfram"
			},
			{
				"family": "Kistler",
				"given": "Werner M."
			},
			{
				"family": "Naud",
				"given": "Richard"
			},
			{
				"family": "Paninski",
				"given": "Liam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "gerstner2002",
		"type": "book",
		"call-number": "QP363 .G475 2002",
		"event-place": "Cambridge, U.K. ; New York",
		"ISBN": "978-0-521-81384-6",
		"number-of-pages": "480",
		"publisher": "Cambridge University Press",
		"publisher-place": "Cambridge, U.K. ; New York",
		"source": "Library of Congress ISBN",
		"title": "Spiking neuron models: single neurons, populations, plasticity",
		"title-short": "Spiking neuron models",
		"author": [
			{
				"family": "Gerstner",
				"given": "Wulfram"
			},
			{
				"family": "Kistler",
				"given": "Werner M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2002"
				]
			]
		}
	},
	{
		"id": "sherrington1975",
		"type": "article-journal",
		"abstract": "We consider an Ising model in which the spins are coupled by infinite-ranged random interactions independently distributed with a Gaussian probability density. Both \"spinglass\" and ferromagnetic phases occur. The competition between the phases and the type of order present in each are studied.",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.35.1792",
		"issue": "26",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"note": "publisher: American Physical Society",
		"page": "1792-1796",
		"source": "APS",
		"title": "Solvable Model of a Spin-Glass",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevLett.35.1792",
		"volume": "35",
		"author": [
			{
				"family": "Sherrington",
				"given": "David"
			},
			{
				"family": "Kirkpatrick",
				"given": "Scott"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1975",
					12,
					29
				]
			]
		}
	},
	{
		"id": "banach1922",
		"type": "article-journal",
		"container-title": "Fundamenta Mathematicae",
		"DOI": "10.4064/fm-3-1-133-181",
		"ISSN": "0016-2736, 1730-6329",
		"journalAbbreviation": "Fund. Math.",
		"language": "en",
		"page": "133-181",
		"source": "DOI.org (Crossref)",
		"title": "Sur les opérations dans les ensembles abstraits et leur application aux équations intégrales",
		"URL": "http://www.impan.pl/get/doi/10.4064/fm-3-1-133-181",
		"volume": "3",
		"author": [
			{
				"family": "Banach",
				"given": "Stefan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1922"
				]
			]
		}
	},
	{
		"id": "hartman1960",
		"type": "article-journal",
		"container-title": "Proceedings of the American Mathematical Society",
		"DOI": "10.1090/S0002-9939-1960-0121542-7",
		"ISSN": "0002-9939",
		"issue": "4",
		"journalAbbreviation": "Proc. Amer. Math. Soc.",
		"language": "en",
		"page": "610-610",
		"source": "DOI.org (Crossref)",
		"title": "A lemma in the theory of structural stability of differential equations",
		"URL": "http://www.ams.org/jourcgi/jour-getitem?pii=S0002-9939-1960-0121542-7",
		"volume": "11",
		"author": [
			{
				"family": "Hartman",
				"given": "Philip"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1960",
					4,
					1
				]
			]
		}
	},
	{
		"id": "schneidman2006",
		"type": "article-journal",
		"abstract": "Biological networks have so many possible states that exhaustive sampling is impossible. Successful analysis thus depends on simplifying hypotheses, but experiments on many systems hint that complicated, higher-order interactions among large groups of elements have an important role. Here we show, in the vertebrate retina, that weak correlations between pairs of neurons coexist with strongly collective behaviour in the responses of ten or more neurons. We find that this collective behaviour is described quantitatively by models that capture the observed pairwise correlations but assume no higher-order interactions. These maximum entropy models are equivalent to Ising models, and predict that larger networks are completely dominated by correlation effects. This suggests that the neural code has associative or error-correcting properties, and we provide preliminary evidence for such behaviour. As a first test for the generality of these ideas, we show that similar results are obtained from networks of cultured cortical neurons.",
		"container-title": "Nature",
		"DOI": "10.1038/nature04701",
		"ISSN": "1476-4687",
		"issue": "7087",
		"language": "en",
		"license": "2006 Nature Publishing Group",
		"note": "Bandiera_abtest: a\nCg_type: Nature Research Journals\nnumber: 7087\nPrimary_atype: Research\npublisher: Nature Publishing Group",
		"page": "1007-1012",
		"source": "www.nature.com",
		"title": "Weak pairwise correlations imply strongly correlated network states in a neural population",
		"URL": "https://www.nature.com/articles/nature04701",
		"volume": "440",
		"author": [
			{
				"family": "Schneidman",
				"given": "Elad"
			},
			{
				"family": "Berry",
				"given": "Michael J."
			},
			{
				"family": "Segev",
				"given": "Ronen"
			},
			{
				"family": "Bialek",
				"given": "William"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2006",
					4
				]
			]
		}
	},
	{
		"id": "ovchinnikov2016",
		"type": "article-journal",
		"container-title": "Entropy",
		"DOI": "10.3390/e18040108",
		"ISSN": "1099-4300",
		"issue": "4",
		"journalAbbreviation": "Entropy",
		"language": "en",
		"page": "108",
		"source": "DOI.org (Crossref)",
		"title": "Introduction to Supersymmetric Theory of Stochastics",
		"URL": "http://www.mdpi.com/1099-4300/18/4/108",
		"volume": "18",
		"author": [
			{
				"family": "Ovchinnikov",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					3,
					28
				]
			]
		}
	},
	{
		"id": "magnitskii2012",
		"type": "chapter",
		"abstract": "Nonlinearity, Bifurcation and Chaos - Theory and Application is an edited book focused on introducing both theoretical and application oriented approaches in science and engineering. It contains 12 chapters, and is recommended for university teachers, scientists, researchers, engineers, as well as graduate and post-graduate students either working or interested in the field of nonlinearity, bifurcation and chaos.",
		"container-title": "Nonlinearity, Bifurcation and Chaos: Theory and Applications",
		"ISBN": "978-953-51-0816-0",
		"language": "en",
		"note": "Google-Books-ID: IuuPDwAAQBAJ",
		"publisher": "BoD – Books on Demand",
		"source": "Google Books",
		"title": "Universality of Transition to Chaos in All Kinds of Nonlinear Differential Equations",
		"container-author": [
			{
				"family": "Awrejcewicz",
				"given": "Jan"
			},
			{
				"family": "Hagedorn",
				"given": "Peter"
			}
		],
		"author": [
			{
				"family": "Magnitskii",
				"given": "Nikolai A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2012",
					10,
					24
				]
			]
		}
	},
	{
		"id": "harish2015",
		"type": "article-journal",
		"abstract": "The brain exhibits temporally complex patterns of activity with features similar to those of chaotic systems. Theoretical studies over the last twenty years have described various computational advantages for such regimes in neuronal systems. Nevertheless, it still remains unclear whether chaos requires specific cellular properties or network architectures, or whether it is a generic property of neuronal circuits. We investigate the dynamics of networks of excitatory-inhibitory (EI) spiking neurons with random sparse connectivity operating in the regime of balance of excitation and inhibition. Combining Dynamical Mean-Field Theory with numerical simulations, we show that chaotic, asynchronous firing rate fluctuations emerge generically for sufficiently strong synapses. Two different mechanisms can lead to these chaotic fluctuations. One mechanism relies on slow I-I inhibition which gives rise to slow subthreshold voltage and rate fluctuations. The decorrelation time of these fluctuations is proportional to the time constant of the inhibition. The second mechanism relies on the recurrent E-I-E feedback loop. It requires slow excitation but the inhibition can be fast. In the corresponding dynamical regime all neurons exhibit rate fluctuations on the time scale of the excitation. Another feature of this regime is that the population-averaged firing rate is substantially smaller in the excitatory population than in the inhibitory population. This is not necessarily the case in the I-I mechanism. Finally, we discuss the neurophysiological and computational significance of our results.",
		"container-title": "PLOS Computational Biology",
		"DOI": "10.1371/journal.pcbi.1004266",
		"ISSN": "1553-7358",
		"issue": "7",
		"journalAbbreviation": "PLOS Computational Biology",
		"language": "en",
		"note": "publisher: Public Library of Science",
		"page": "e1004266",
		"source": "PLoS Journals",
		"title": "Asynchronous Rate Chaos in Spiking Neuronal Circuits",
		"URL": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004266",
		"volume": "11",
		"author": [
			{
				"family": "Harish",
				"given": "Omri"
			},
			{
				"family": "Hansel",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					7,
					31
				]
			]
		}
	},
	{
		"id": "muir2015",
		"type": "article-journal",
		"abstract": "The eigenvalue spectrum of the matrix of directed weights defining a neural network model is informative of several stability and dynamical properties of network activity. Existing results for eigenspectra of sparse asymmetric random matrices neglect spatial or other constraints in determining entries in these matrices, and so are of partial applicability to cortical-like architectures. Here we examine a parameterized class of networks that are defined by sparse connectivity, with connection weighting modulated by physical proximity (i.e., asymmetric Euclidean random matrices), modular network partitioning, and functional specificity within the excitatory population. We present a set of analytical constraints that apply to the eigenvalue spectra of associated weight matrices, highlighting the relationship between connectivity rules and classes of network dynamics.",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.91.042808",
		"issue": "4",
		"journalAbbreviation": "Phys. Rev. E",
		"note": "publisher: American Physical Society",
		"page": "042808",
		"source": "APS",
		"title": "Eigenspectrum bounds for semirandom matrices with modular and spatial structure for neural networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.91.042808",
		"volume": "91",
		"author": [
			{
				"family": "Muir",
				"given": "Dylan R."
			},
			{
				"family": "Mrsic-Flogel",
				"given": "Thomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					4,
					24
				]
			]
		}
	},
	{
		"id": "muir2015a",
		"type": "article-journal",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.91.042808",
		"ISSN": "1539-3755, 1550-2376",
		"issue": "4",
		"journalAbbreviation": "Phys. Rev. E",
		"language": "en",
		"page": "042808",
		"source": "DOI.org (Crossref)",
		"title": "Eigenspectrum bounds for semirandom matrices with modular and spatial structure for neural networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.91.042808",
		"volume": "91",
		"author": [
			{
				"family": "Muir",
				"given": "Dylan R."
			},
			{
				"family": "Mrsic-Flogel",
				"given": "Thomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					4,
					24
				]
			]
		}
	},
	{
		"id": "zotero-869",
		"type": "webpage",
		"title": "Phys. Rev. E 95, 042323 (2017) - Mean-field equations for neuronal networks with arbitrary degree distributions",
		"URL": "https://journals-aps-org.proxy.uba.uva.nl/pre/abstract/10.1103/PhysRevE.95.042323",
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					22
				]
			]
		}
	},
	{
		"id": "bugeon2021",
		"type": "article-journal",
		"abstract": "Obtaining a consistent taxonomy of neuron types is challenging mainly because of the high dimensionality of the datasets. Coupled autoencoders are a step forward in achieving this goal.",
		"container-title": "Nature Computational Science",
		"DOI": "10.1038/s43588-021-00027-w",
		"ISSN": "2662-8457",
		"issue": "2",
		"journalAbbreviation": "Nat Comput Sci",
		"language": "en",
		"license": "2021 Springer Nature America, Inc.",
		"note": "Bandiera_abtest: a\nCg_type: Nature Research Journals\nnumber: 2\nPrimary_atype: News & Views\npublisher: Nature Publishing Group\nSubject_term: Cellular neuroscience;Computational neuroscience;Machine learning\nSubject_term_id: cellular-neuroscience;computational-neuroscience;machine-learning",
		"page": "100-101",
		"source": "www.nature.com",
		"title": "Towards a consistent neuron classification",
		"URL": "https://www.nature.com/articles/s43588-021-00027-w",
		"volume": "1",
		"author": [
			{
				"family": "Bugeon",
				"given": "Stephane"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					2
				]
			]
		}
	},
	{
		"id": "albeverio2008",
		"type": "book",
		"collection-title": "Lecture Notes in Mathematics",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-540-76954-5",
		"note": "DOI: 10.1007/978-3-540-76956-9",
		"publisher": "Springer Berlin Heidelberg",
		"publisher-place": "Berlin, Heidelberg",
		"source": "DOI.org (Crossref)",
		"title": "Mathematical Theory of Feynman Path Integrals",
		"URL": "http://link.springer.com/10.1007/978-3-540-76956-9",
		"volume": "523",
		"author": [
			{
				"family": "Albeverio",
				"given": "Sergio A."
			},
			{
				"family": "Høegh-Krohn",
				"given": "Raphael J."
			},
			{
				"family": "Mazzucchi",
				"given": "Sonia"
			}
		],
		"collection-editor": [
			{
				"family": "Morel",
				"given": "J. -M."
			},
			{
				"family": "Takens",
				"given": "F."
			},
			{
				"family": "Teissier",
				"given": "B."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2008"
				]
			]
		}
	},
	{
		"id": "yau2013",
		"type": "article-journal",
		"issue": "1",
		"language": "en",
		"page": "4",
		"source": "Zotero",
		"title": "The Wigner-Dyson-Gaudin-Mehta Conjecture",
		"volume": "1",
		"author": [
			{
				"family": "Yau",
				"given": "Horng-Tzer"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		}
	},
	{
		"id": "tao2008",
		"type": "article-journal",
		"abstract": "Let $\\a$ be a complex random variable with mean zero and bounded variance $\\sigma^{2}$. Let $N_{n}$ be a random matrix of order $n$ with entries being i.i.d. copies of $\\a$. Let $\\lambda_{1}, ..., \\lambda_{n}$ be the eigenvalues of $\\frac{1}{\\sigma \\sqrt n}N_{n}$. Define the empirical spectral distribution $\\mu_{n}$ of $N_{n}$ by the formula $$ \\mu_n(s,t) := \\frac{1}{n} # \\{k \\leq n| \\Re(\\lambda_k) \\leq s; \\Im(\\lambda_k) \\leq t \\}.$$ The Circular law conjecture asserts that $\\mu_{n}$ converges to the uniform distribution $\\mu_\\infty$ over the unit disk as $n$ tends to infinity. We prove this conjecture under the slightly stronger assumption that the $(2+\\eta)\\th$-moment of $\\a$ is bounded, for any $\\eta >0$. Our method builds and improves upon earlier work of Girko, Bai, G\\\"otze-Tikhomirov, and Pan-Zhou, and also applies for sparse random matrices. The new key ingredient in the paper is a general result about the least singular value of random matrices, which was obtained using tools and ideas from additive combinatorics.",
		"container-title": "arXiv:0708.2895 [math]",
		"note": "arXiv: 0708.2895",
		"source": "arXiv.org",
		"title": "Random Matrices: The circular Law",
		"title-short": "Random Matrices",
		"URL": "http://arxiv.org/abs/0708.2895",
		"author": [
			{
				"family": "Tao",
				"given": "Terence"
			},
			{
				"family": "Vu",
				"given": "Van"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2008",
					2,
					28
				]
			]
		}
	},
	{
		"id": "bandyopadhyay2007",
		"type": "article-journal",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.76.026109",
		"ISSN": "1539-3755, 1550-2376",
		"issue": "2",
		"journalAbbreviation": "Phys. Rev. E",
		"language": "en",
		"page": "026109",
		"source": "DOI.org (Crossref)",
		"title": "Universality in complex networks: Random matrix analysis",
		"title-short": "Universality in complex networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevE.76.026109",
		"volume": "76",
		"author": [
			{
				"family": "Bandyopadhyay",
				"given": "Jayendra N."
			},
			{
				"family": "Jalan",
				"given": "Sarika"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					8,
					20
				]
			]
		}
	},
	{
		"id": "roxin2011",
		"type": "article-journal",
		"container-title": "Journal of Neuroscience",
		"issue": "45",
		"journalAbbreviation": "J. Neurosci.",
		"page": "16217-16226",
		"title": "On the Distribution of Firing Rates in Networks of Cortical Neurons",
		"volume": "31",
		"author": [
			{
				"family": "Roxin",
				"given": "Alex"
			},
			{
				"family": "Brunel",
				"given": "Nicolas"
			},
			{
				"family": "Hansel",
				"given": "David"
			},
			{
				"family": "Mongillo",
				"given": "Gianluigi"
			},
			{
				"family": "Vreeswijk",
				"given": "Carl",
				"non-dropping-particle": "van"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2011",
					11,
					9
				]
			]
		}
	},
	{
		"id": "hebb1949",
		"type": "paper-conference",
		"abstract": "Contents: Introduction. The Problem and the Line of Attack. Summation and Learning in Perception. Field Theory and Equipotentiality. The First Stage of Perception: Growth of the Assembly. Perception of a Complex: The Phase Sequence. Development of the Learning Capacity. Higher and Lower Processes Related to Learning. Problems of Motivation: Pain and Hunger. The Problem of Motivational Drift. Emotional Disturbances. The Growth and Decline of Intelligence.",
		"DOI": "10.2307/1418888",
		"source": "Semantic Scholar",
		"title": "The Organization of Behavior: A Neuropsychological Theory",
		"title-short": "The Organization of Behavior",
		"author": [
			{
				"family": "Hebb",
				"given": "D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"1949"
				]
			]
		}
	},
	{
		"id": "edelman1993",
		"type": "article-journal",
		"abstract": "Variation and selection within neural populations play key roles in the development and function of the brain. In this article, I review a population theory of the nervous system aimed at understanding the significance of these processes. Since its original formulation in 1978, considerable evidence has accumulated to support this theory of neuronal group selection. Extensive neural modeling based on the theory has provided useful insights into several outstanding neurobiological problems including those concerned with integration of cortical function, sensorimotor control, and perceptually based behavior.",
		"container-title": "Neuron",
		"DOI": "10.1016/0896-6273(93)90304-A",
		"ISSN": "0896-6273",
		"issue": "2",
		"journalAbbreviation": "Neuron",
		"language": "en",
		"page": "115-125",
		"source": "ScienceDirect",
		"title": "Neural Darwinism: Selection and reentrant signaling in higher brain function",
		"title-short": "Neural Darwinism",
		"URL": "https://www.sciencedirect.com/science/article/pii/089662739390304A",
		"volume": "10",
		"author": [
			{
				"family": "Edelman",
				"given": "Gerald M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1993",
					2,
					1
				]
			]
		}
	},
	{
		"id": "martin2018",
		"type": "article-journal",
		"abstract": "Random Matrix Theory (RMT) is applied to analyze weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet. Empirical and theoretical results clearly indicate that the DNN training process itself implicitly implements a form of Self-Regularization. The empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of explicit regularization. Building on relatively recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization. These phases can be observed during the training process as well as in the final learned DNNs. For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a \"size scale\" separating signal from noise. For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems. This results from correlations arising at all size scales, which arises implicitly due to the training process itself. This implicit Self-Regularization can depend strongly on the many knobs of the training process. By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size. This demonstrates that---all else being equal---DNN optimization with larger batch sizes leads to less-well implicitly-regularized models, and it provides an explanation for the generalization gap phenomena.",
		"container-title": "arXiv:1810.01075 [cs, stat]",
		"note": "arXiv: 1810.01075",
		"source": "arXiv.org",
		"title": "Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning",
		"title-short": "Implicit Self-Regularization in Deep Neural Networks",
		"URL": "http://arxiv.org/abs/1810.01075",
		"author": [
			{
				"family": "Martin",
				"given": "Charles H."
			},
			{
				"family": "Mahoney",
				"given": "Michael W."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					2
				]
			]
		}
	},
	{
		"id": "scarselli1998",
		"type": "article-journal",
		"abstract": "In this paper, we present a review of some recent works on approximation by feedforward neural networks. A particular emphasis is placed on the computational aspects of the problem, i.e. we discuss the possibility of realizing a feedforward neural network which achieves a prescribed degree of accuracy of approximation, and the determination of the number of hidden layer neurons required to achieve this accuracy. Furthermore, a unifying framework is introduced to understand existing approaches to investigate the universal approximation problem using feedforward neural networks. Some new results are also presented. Finally, two training algorithms are introduced which can determine the weights of feedforward neural networks, with sigmoidal activation neurons, to any degree of prescribed accuracy. These training algorithms are designed so that they do not suffer from the problems of local minima which commonly affect neural network learning algorithms.",
		"container-title": "Neural Networks",
		"DOI": "10.1016/S0893-6080(97)00097-X",
		"ISSN": "0893-6080",
		"issue": "1",
		"journalAbbreviation": "Neural Networks",
		"language": "en",
		"page": "15-37",
		"source": "ScienceDirect",
		"title": "Universal Approximation Using Feedforward Neural Networks: A Survey of Some Existing Methods, and Some New Results",
		"title-short": "Universal Approximation Using Feedforward Neural Networks",
		"URL": "https://www.sciencedirect.com/science/article/pii/S089360809700097X",
		"volume": "11",
		"author": [
			{
				"family": "Scarselli",
				"given": "Franco"
			},
			{
				"family": "Chung Tsoi",
				"given": "Ah"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1998",
					1,
					1
				]
			]
		}
	},
	{
		"id": "albeverio2008a",
		"type": "book",
		"collection-title": "Lecture Notes in Mathematics",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-540-76954-5",
		"language": "en",
		"note": "DOI: 10.1007/978-3-540-76956-9",
		"publisher": "Springer Berlin Heidelberg",
		"publisher-place": "Berlin, Heidelberg",
		"source": "DOI.org (Crossref)",
		"title": "Mathematical Theory of Feynman Path Integrals",
		"URL": "http://link.springer.com/10.1007/978-3-540-76956-9",
		"volume": "523",
		"author": [
			{
				"family": "Albeverio",
				"given": "Sergio A."
			},
			{
				"family": "Høegh-Krohn",
				"given": "Raphael J."
			},
			{
				"family": "Mazzucchi",
				"given": "Sonia"
			}
		],
		"collection-editor": [
			{
				"family": "Morel",
				"given": "J. -M."
			},
			{
				"family": "Takens",
				"given": "F."
			},
			{
				"family": "Teissier",
				"given": "B."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2008"
				]
			]
		}
	},
	{
		"id": "hochreiter1997",
		"type": "article-journal",
		"abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
		"container-title": "Neural Computation",
		"DOI": "10.1162/neco.1997.9.8.1735",
		"ISSN": "0899-7667, 1530-888X",
		"issue": "8",
		"journalAbbreviation": "Neural Computation",
		"language": "en",
		"page": "1735-1780",
		"source": "DOI.org (Crossref)",
		"title": "Long Short-Term Memory",
		"URL": "https://direct.mit.edu/neco/article/9/8/1735-1780/6109",
		"volume": "9",
		"author": [
			{
				"family": "Hochreiter",
				"given": "Sepp"
			},
			{
				"family": "Schmidhuber",
				"given": "Jürgen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1997",
					11,
					1
				]
			]
		}
	},
	{
		"id": "boltyanskiy1962",
		"type": "article-journal",
		"title": "Mathematical theory of optimal processes",
		"author": [
			{
				"family": "Boltyanskiy",
				"given": "VG"
			},
			{
				"family": "Gamkrelidze",
				"given": "Revaz V"
			},
			{
				"family": "Mishchenko",
				"given": "YEF"
			},
			{
				"family": "Pontryagin",
				"given": "LS"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1962"
				]
			]
		}
	},
	{
		"id": "boedecker2012",
		"type": "article-journal",
		"container-title": "Theory in Biosciences",
		"DOI": "10.1007/s12064-011-0146-8",
		"ISSN": "1431-7613, 1611-7530",
		"issue": "3",
		"journalAbbreviation": "Theory Biosci.",
		"language": "en",
		"page": "205-213",
		"source": "DOI.org (Crossref)",
		"title": "Information processing in echo state networks at the edge of chaos",
		"URL": "http://link.springer.com/10.1007/s12064-011-0146-8",
		"volume": "131",
		"author": [
			{
				"family": "Boedecker",
				"given": "Joschka"
			},
			{
				"family": "Obst",
				"given": "Oliver"
			},
			{
				"family": "Lizier",
				"given": "Joseph T."
			},
			{
				"family": "Mayer",
				"given": "N. Michael"
			},
			{
				"family": "Asada",
				"given": "Minoru"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					9
				]
			]
		}
	},
	{
		"id": "rajan2006",
		"type": "article-journal",
		"abstract": "The dynamics of neural networks is influenced strongly by the spectrum of eigenvalues of the matrix describing their synaptic connectivity. In large networks, elements of the synaptic connectivity matrix can be chosen randomly from appropriate distributions, making results from random matrix theory highly relevant. Unfortunately, classic results on the eigenvalue spectra of random matrices do not apply to synaptic connectivity matrices because of the constraint that individual neurons are either excitatory or inhibitory. Therefore, we compute eigenvalue spectra of large random matrices with excitatory and inhibitory columns drawn from distributions with different means and equal or different variances.",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.97.188104",
		"issue": "18",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"note": "publisher: American Physical Society",
		"page": "188104",
		"source": "APS",
		"title": "Eigenvalue Spectra of Random Matrices for Neural Networks",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevLett.97.188104",
		"volume": "97",
		"author": [
			{
				"family": "Rajan",
				"given": "Kanaka"
			},
			{
				"family": "Abbott",
				"given": "L. F."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2006",
					11,
					2
				]
			]
		}
	},
	{
		"id": "scharfman2007",
		"type": "article-journal",
		"container-title": "Current Neurology and Neuroscience Reports",
		"DOI": "10.1007/s11910-007-0053-z",
		"ISSN": "1528-4042, 1534-6293",
		"issue": "4",
		"journalAbbreviation": "Curr Neurol Neurosci Rep",
		"language": "en",
		"page": "348-354",
		"source": "DOI.org (Crossref)",
		"title": "The neurobiology of epilepsy",
		"URL": "http://link.springer.com/10.1007/s11910-007-0053-z",
		"volume": "7",
		"author": [
			{
				"family": "Scharfman",
				"given": "Helen E."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					7
				]
			]
		}
	},
	{
		"id": "tang2001a",
		"type": "article-journal",
		"abstract": "An estimator of the total number of synapses in neocortex of human autopsy brains based on unbiased stereological principles is described. Each randomly chosen cerebral hemisphere was stratified into the four major neocortical regions. Uniform sampling with a varying sampling fraction in each region of neocortex was performed. The total volume of each neocortical region was estimated using point counting according to Cavalieri's principle. The ethanolic phosphotungstic acid staining technique was modified for synapses in human autopsy brains. The numerical density of synapses in each neocortical region studied was estimated using the disector at the electron microscopical level. The total number of neocortical synapses in each region was estimated as the product of the total volume of neocortex and the numerical density of synapses. The influence of the postmortem fixation delay on the number of synapses was investigated in five large mammals (one dog, one cow, and three pigs), the brains of which were kept under conditions similar to those under which human corpses are normally kept. The apparent decrease of 3.9% in the numerical density of synapses in the large mammals following a 2-day fixation delay was not significant. The average total number of synapses in the neocortex of five young male brains was 164 × 1012 (CV = 0.17). An analysis of the precision of the estimate of the total number of synapses in neocortex indicates that blocks represent both the major source of variation and the largest workload. Using eight blocks per brain the imprecision of the estimate is, however, only 66% of the total variance. Synapse 41:258–273, 2001. © 2001 Wiley-Liss, Inc.",
		"container-title": "Synapse",
		"DOI": "10.1002/syn.1083",
		"ISSN": "1098-2396",
		"issue": "3",
		"language": "en",
		"license": "Copyright © 2001 Wiley-Liss, Inc.",
		"note": "_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/syn.1083",
		"page": "258-273",
		"source": "Wiley Online Library",
		"title": "Total regional and global number of synapses in the human brain neocortex",
		"URL": "https://onlinelibrary.wiley.com/doi/abs/10.1002/syn.1083",
		"volume": "41",
		"author": [
			{
				"family": "Tang",
				"given": "Yong"
			},
			{
				"family": "Nyengaard",
				"given": "Jens R."
			},
			{
				"family": "Groot",
				"given": "Didima M. G. De"
			},
			{
				"family": "Gundersen",
				"given": "Hans Jørgen G."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2001"
				]
			]
		}
	},
	{
		"id": "albert2002",
		"type": "article-journal",
		"abstract": "Complex networks describe a wide range of systems in nature and society. Frequently cited examples include the cell, a network of chemicals linked by chemical reactions, and the Internet, a network of routers and computers connected by physical links. While traditionally these systems have been modeled as random graphs, it is increasingly recognized that the topology and evolution of real networks are governed by robust organizing principles. This article reviews the recent advances in the field of complex networks, focusing on the statistical mechanics of network topology and dynamics. After reviewing the empirical data that motivated the recent interest in networks, the authors discuss the main models and analytical tools, covering random graphs, small-world and scale-free networks, the emerging theory of evolving networks, and the interplay between topology and the network’s robustness against failures and attacks.",
		"container-title": "Reviews of Modern Physics",
		"DOI": "10.1103/RevModPhys.74.47",
		"issue": "1",
		"journalAbbreviation": "Rev. Mod. Phys.",
		"note": "publisher: American Physical Society",
		"page": "47-97",
		"source": "APS",
		"title": "Statistical mechanics of complex networks",
		"URL": "https://link.aps.org/doi/10.1103/RevModPhys.74.47",
		"volume": "74",
		"author": [
			{
				"family": "Albert",
				"given": "Réka"
			},
			{
				"family": "Barabási",
				"given": "Albert-László"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2002",
					1,
					30
				]
			]
		}
	},
	{
		"id": "sutskever2014",
		"type": "article-journal",
		"abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
		"container-title": "arXiv:1409.3215 [cs]",
		"note": "arXiv: 1409.3215",
		"source": "arXiv.org",
		"title": "Sequence to Sequence Learning with Neural Networks",
		"URL": "http://arxiv.org/abs/1409.3215",
		"author": [
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Vinyals",
				"given": "Oriol"
			},
			{
				"family": "Le",
				"given": "Quoc V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					12,
					14
				]
			]
		}
	},
	{
		"id": "majumdar2009",
		"type": "article-journal",
		"abstract": "We present a Coulomb gas method to calculate analytically the probability of rare events where the maximum eigenvalue of a random matrix is much larger than its typical value. The large deviation function that characterizes this probability is computed explicitly for Wishart and Gaussian ensembles. The method is general and applies to other related problems, e.g., the joint large deviation function for large fluctuations of top eigenvalues. Our results are relevant to widely employed data compression techniques, namely, the principal components analysis. Analytical predictions are verified by extensive numerical simulations.",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.102.060601",
		"issue": "6",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"note": "publisher: American Physical Society",
		"page": "060601",
		"source": "APS",
		"title": "Large Deviations of the Maximum Eigenvalue for Wishart and Gaussian Random Matrices",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevLett.102.060601",
		"volume": "102",
		"author": [
			{
				"family": "Majumdar",
				"given": "Satya N."
			},
			{
				"family": "Vergassola",
				"given": "Massimo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009",
					2,
					12
				]
			]
		}
	},
	{
		"id": "poplavskyi2017",
		"type": "article-journal",
		"abstract": "Let $\\sqrt{N}+\\lambda_{\\max}$ be the largest real eigenvalue of a random $N\\times N$ matrix with independent $N(0,1)$ entries (the “real Ginibre matrix”). We study the large deviations behaviour of the limiting $N\\rightarrow \\infty $ distribution $\\mathbb{P}[\\lambda_{\\max}<t]$ of the shifted maximal real eigenvalue $\\lambda_{\\max}$. In particular, we prove that the right tail of this distribution is Gaussian: for $t>0$, \\begin{equation*}\\mathbb{P}[\\lambda_{\\max}<t]=1-\\frac{1}{4}\\operatorname{erfc}(t)+O(e^{-2t^{2}}).\\end{equation*} This is a rigorous confirmation of the corresponding result of [Phys. Rev. Lett. 99 (2007) 050603]. We also prove that the left tail is exponential, with correct asymptotics up to $O(1)$: for $t<0$, \\begin{equation*}\\mathbb{P}[\\lambda_{\\max}<t]=e^{\\frac{1}{2\\sqrt{2\\pi }}\\zeta (\\frac{3}{2})t+O(1)},\\end{equation*} where $\\zeta $ is the Riemann zeta-function. Our results have implications for interacting particle systems. The edge scaling limit of the law of real eigenvalues for the real Ginibre ensemble is a rescaling of a fixed time distribution of annihilating Brownian motions (ABMs) with the step initial condition; see [Garrod, Poplavskyi, Tribe and Zaboronski (2015)]. Therefore, the tail behaviour of the distribution of $X_{s}^{(\\max)}$—the position of the rightmost annihilating particle at fixed time $s>0$—can be read off from the corresponding answers for $\\lambda_{\\max}$ using $X_{s}^{(\\max)}\\stackrel{D}{=}\\sqrt{4s}\\lambda_{\\max}$.",
		"container-title": "The Annals of Applied Probability",
		"DOI": "10.1214/16-AAP1233",
		"ISSN": "1050-5164, 2168-8737",
		"issue": "3",
		"note": "publisher: Institute of Mathematical Statistics",
		"page": "1395-1413",
		"source": "Project Euclid",
		"title": "On the distribution of the largest real eigenvalue for the real Ginibre ensemble",
		"URL": "https://projecteuclid.org/journals/annals-of-applied-probability/volume-27/issue-3/On-the-distribution-of-the-largest-real-eigenvalue-for-the/10.1214/16-AAP1233.full",
		"volume": "27",
		"author": [
			{
				"family": "Poplavskyi",
				"given": "Mihail"
			},
			{
				"family": "Tribe",
				"given": "Roger"
			},
			{
				"family": "Zaboronski",
				"given": "Oleg"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					6
				]
			]
		}
	},
	{
		"id": "deneve2016",
		"type": "article-journal",
		"abstract": "Despite representing a minority of cortical cells, inhibitory neurons deeply shape cortical responses. Inhibitory currents closely track excitatory currents, opening only brief windows of opportunity for a neuron to fire. This explains the variability of cortical spike trains, but may also, paradoxically, render a spiking network maximally efficient and precise.",
		"container-title": "Nature Neuroscience",
		"DOI": "10.1038/nn.4243",
		"ISSN": "1546-1726",
		"issue": "3",
		"journalAbbreviation": "Nat Neurosci",
		"language": "en",
		"license": "2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.",
		"note": "Bandiera_abtest: a\nCg_type: Nature Research Journals\nnumber: 3\nPrimary_atype: Reviews\npublisher: Nature Publishing Group\nSubject_term: Network models;Neural circuits;Neural encoding\nSubject_term_id: network-models;neural-circuit;neural-encoding",
		"page": "375-382",
		"source": "www.nature.com",
		"title": "Efficient codes and balanced networks",
		"URL": "https://www.nature.com/articles/nn.4243",
		"volume": "19",
		"author": [
			{
				"family": "Denève",
				"given": "Sophie"
			},
			{
				"family": "Machens",
				"given": "Christian K."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					3
				]
			]
		}
	},
	{
		"id": "baldi2013",
		"type": "article-journal",
		"container-title": "Advances in neural information processing systems",
		"page": "2814-2822",
		"title": "Understanding dropout",
		"volume": "26",
		"author": [
			{
				"family": "Baldi",
				"given": "Pierre"
			},
			{
				"family": "Sadowski",
				"given": "Peter J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		}
	},
	{
		"id": "rougier2007",
		"type": "graphic",
		"archive": "Own work",
		"license": "Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled GNU Free Documentation License.http://www.gnu.org/copyleft/fdl.htmlGFDLGNU Free Documentation Licensetruetrue",
		"source": "Wikimedia Commons",
		"title": "Biological neuron schema",
		"URL": "https://commons.wikimedia.org/wiki/File:Neuron-figure-notext.svg",
		"author": [
			{
				"family": "Rougier",
				"given": "Nicolas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					6,
					2
				]
			]
		}
	},
	{
		"id": "zotero-946",
		"type": "webpage",
		"language": "en",
		"note": "ISSN: 0375-9601\nDOI: 10.1016/0375-9601(90)90136-C",
		"title": "PII: 0375-9601(90)90136-C | Elsevier Enhanced Reader",
		"title-short": "PII",
		"URL": "https://reader.elsevier.com/reader/sd/pii/037596019090136C?token=DBDADE239190512A6C93F93133BF91DFA32719F79506CCF2864F88B0C444BCAB866A7ED83A6758C640DC9010300C71A3&originRegion=us-east-1&originCreation=20210625155232",
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					25
				]
			]
		}
	},
	{
		"id": "wigner1955",
		"type": "article-journal",
		"container-title": "Annals of Mathematics",
		"DOI": "10.2307/1970079",
		"ISSN": "0003-486X",
		"issue": "3",
		"note": "publisher: Annals of Mathematics",
		"page": "548-564",
		"source": "JSTOR",
		"title": "Characteristic Vectors of Bordered Matrices With Infinite Dimensions",
		"URL": "https://www.jstor.org/stable/1970079",
		"volume": "62",
		"author": [
			{
				"family": "Wigner",
				"given": "Eugene P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1955"
				]
			]
		}
	},
	{
		"id": "forrester2007a",
		"type": "article-journal",
		"abstract": "The real Ginibre ensemble consists of random N x N matrices formed from independent and identically distributed standard Gaussian entries. By using the method of skew orthogonal polynomials, the general n-point correlations for the real eigenvalues, and for the complex eigenvalues, are given as n x n Pfaffians with explicit entries. A computationally tractable formula for the cumulative probability density of the largest real eigenvalue is presented. This is relevant to May's stability analysis of biological webs.",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.99.050603",
		"ISSN": "1079-7114",
		"issue": "5",
		"journalAbbreviation": "Phys Rev Lett",
		"language": "eng",
		"note": "PMID: 17930739",
		"page": "050603",
		"source": "PubMed",
		"title": "Eigenvalue statistics of the real Ginibre ensemble",
		"volume": "99",
		"author": [
			{
				"family": "Forrester",
				"given": "Peter J."
			},
			{
				"family": "Nagao",
				"given": "Taro"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2007",
					8,
					3
				]
			]
		}
	},
	{
		"id": "zotero-951",
		"type": "webpage",
		"title": "AMS :: Bulletin of the American Mathematical Society",
		"URL": "https://www.ams.org/journals/bull/2012-49-03/S0273-0979-2012-01372-1/home.html",
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					26
				]
			]
		}
	},
	{
		"id": "wishart1928",
		"type": "article-journal",
		"container-title": "Biometrika",
		"DOI": "10.2307/2331939",
		"ISSN": "0006-3444",
		"issue": "1/2",
		"note": "publisher: [Oxford University Press, Biometrika Trust]",
		"page": "32-52",
		"source": "JSTOR",
		"title": "The Generalised Product Moment Distribution in Samples from a Normal Multivariate Population",
		"URL": "https://www.jstor.org/stable/2331939",
		"volume": "20A",
		"author": [
			{
				"family": "Wishart",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1928"
				]
			]
		}
	},
	{
		"id": "erdos2012",
		"type": "article-journal",
		"abstract": "The Wigner-Dyson-Gaudin-Mehta conjecture asserts that the local eigenvalue statistics of large random matrices exhibit universal behavior depending only on the symmetry class of the matrix ensemble. For invariant matrix models, the eigenvalue distributions are given by a log-gas with potential V and inverse temperature β = 1, 2, 4, corresponding to the orthogonal, unitary and symplectic ensembles. For β ∈/ {1, 2, 4}, there is no natural random matrix ensemble behind this model, but the statistical physics interpretation of the log-gas is still valid for all β > 0. The universality conjecture for invariant ensembles asserts that the local eigenvalue statistics are independent of V . In this article, we review our recent solution to the universality conjecture for both invariant and non-invariant ensembles. We will also demonstrate that the local ergodicity of the Dyson Brownian motion is the intrinsic mechanism behind the universality. Furthermore, we review the solution of Dyson’s conjecture on the local relaxation time of the Dyson Brownian motion. Related questions such as delocalization of eigenvectors and local version of Wigner’s semicircle law will also be discussed.",
		"container-title": "Bulletin of the American Mathematical Society",
		"DOI": "10.1090/S0273-0979-2012-01372-1",
		"ISSN": "0273-0979, 1088-9485",
		"issue": "3",
		"journalAbbreviation": "Bull. Amer. Math. Soc.",
		"language": "en",
		"page": "377-414",
		"source": "DOI.org (Crossref)",
		"title": "Universality of local spectral statistics of random matrices",
		"URL": "http://www.ams.org/jourcgi/jour-getitem?pii=S0273-0979-2012-01372-1",
		"volume": "49",
		"author": [
			{
				"family": "Erdős",
				"given": "László"
			},
			{
				"family": "Yau",
				"given": "Horng-Tzer"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					9,
					1
				]
			]
		}
	},
	{
		"id": "hopfield1982",
		"type": "article-journal",
		"abstract": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.",
		"container-title": "Proceedings of the National Academy of Sciences of the United States of America",
		"ISSN": "0027-8424",
		"issue": "8",
		"journalAbbreviation": "Proc Natl Acad Sci U S A",
		"note": "PMID: 6953413\nPMCID: PMC346238",
		"page": "2554-2558",
		"source": "PubMed Central",
		"title": "Neural networks and physical systems with emergent collective computational abilities.",
		"URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/",
		"volume": "79",
		"author": [
			{
				"family": "Hopfield",
				"given": "J J"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1982",
					4
				]
			]
		}
	},
	{
		"id": "ginibre1965",
		"type": "article-journal",
		"container-title": "Journal of Mathematical Physics",
		"DOI": "10.1063/1.1704292",
		"ISSN": "0022-2488",
		"issue": "3",
		"note": "publisher: American Institute of Physics",
		"page": "440-449",
		"source": "aip.scitation.org (Atypon)",
		"title": "Statistical Ensembles of Complex, Quaternion, and Real Matrices",
		"URL": "https://aip.scitation.org/doi/abs/10.1063/1.1704292",
		"volume": "6",
		"author": [
			{
				"family": "Ginibre",
				"given": "Jean"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1965",
					3,
					1
				]
			]
		}
	},
	{
		"id": "mehta1967",
		"type": "article-journal",
		"container-title": "Press, New York",
		"title": "Random matrices and the statistical theory of energy levels, Acad",
		"author": [
			{
				"family": "Mehta",
				"given": "M. L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"1967"
				]
			]
		}
	},
	{
		"id": "chaudhuri2015",
		"type": "article-journal",
		"container-title": "Neuron",
		"issue": "2",
		"note": "ISBN: 0896-6273\npublisher: Elsevier",
		"page": "419-431",
		"title": "A large-scale circuit mechanism for hierarchical dynamical processing in the primate cortex",
		"volume": "88",
		"author": [
			{
				"family": "Chaudhuri",
				"given": "Rishidev"
			},
			{
				"family": "Knoblauch",
				"given": "Kenneth"
			},
			{
				"family": "Gariel",
				"given": "Marie-Alice"
			},
			{
				"family": "Kennedy",
				"given": "Henry"
			},
			{
				"family": "Wang",
				"given": "Xiao-Jing"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "muller2020",
		"type": "article-journal",
		"abstract": "The biological mechanisms that allow the brain to balance flexibility and integration remain poorly understood. A potential solution may lie in a unique aspect of neurobiology, which is that numerous brain systems contain diffuse synaptic connectivity. Here, we demonstrate that increasing diffuse cortical coupling within a validated biophysical corticothalamic model traverses the system through a quasi-critical regime in which spatial heterogeneities in input noise support transient critical dynamics in distributed subregions. The presence of quasi-critical states coincides with known signatures of complex, adaptive brain network dynamics. Finally, we demonstrate the presence of similar dynamic signatures in empirical whole-brain human neuroimaging data. Together, our results establish that modulating the balance between local and diffuse synaptic coupling in a thalamocortical model subtends the emergence of quasi-critical brain states that act to flexibly transition the brain between unique modes of information processing.",
		"container-title": "Nature Communications",
		"DOI": "10.1038/s41467-020-19716-7",
		"ISSN": "2041-1723",
		"issue": "1",
		"journalAbbreviation": "Nat Commun",
		"language": "en",
		"license": "2020 Crown",
		"note": "Bandiera_abtest: a\nCc_license_type: cc_by\nCg_type: Nature Research Journals\nnumber: 1\nPrimary_atype: Research\npublisher: Nature Publishing Group\nSubject_term: Biophysical models;Dynamical systems\nSubject_term_id: biophysical-models;dynamical-systems",
		"page": "6337",
		"source": "www.nature.com",
		"title": "Diffuse neural coupling mediates complex network dynamics through the formation of quasi-critical brain states",
		"URL": "https://www.nature.com/articles/s41467-020-19716-7",
		"volume": "11",
		"author": [
			{
				"family": "Müller",
				"given": "Eli J."
			},
			{
				"family": "Munn",
				"given": "Brandon R."
			},
			{
				"family": "Shine",
				"given": "James M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12,
					10
				]
			]
		}
	},
	{
		"id": "ruelle1978",
		"type": "article-journal",
		"container-title": "Boletim da Sociedade Brasileira de Matemática-Bulletin/Brazilian Mathematical Society",
		"issue": "1",
		"note": "ISBN: 0100-3569\npublisher: Springer",
		"page": "83-87",
		"title": "An inequality for the entropy of differentiable maps",
		"volume": "9",
		"author": [
			{
				"family": "Ruelle",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1978"
				]
			]
		}
	},
	{
		"id": "kuznetsov2021",
		"type": "book",
		"abstract": "This book provides analytical and numerical methods for the estimation of dimension characteristics (Hausdorff, Fractal, Carathéodory dimensions) for attractors and invariant sets of dynamical systems and cocycles generated by smooth differential equations or maps in finite-dimensional Euclidean spaces or on manifolds. It also discusses stability investigations using estimates based on Lyapunov functions and adapted metrics. Moreover, it introduces various types of Lyapunov dimensions of dynamical systems with respect to an invariant set, based on local, global and uniform Lyapunov exponents, and derives analytical formulas for the Lyapunov dimension of the attractors of the Hénon and Lorenz systems. Lastly, the book presents estimates of the topological entropy for general dynamical systems in metric spaces and estimates of the topological dimension for orbit closures of almost periodic solutions to differential equations.",
		"collection-title": "Emergence, Complexity and Computation",
		"ISBN": "978-3-030-50986-6",
		"language": "en",
		"note": "DOI: 10.1007/978-3-030-50987-3",
		"publisher": "Springer International Publishing",
		"source": "www.springer.com",
		"title": "Attractor Dimension Estimates for Dynamical Systems: Theory and Computation: Dedicated to Gennady Leonov",
		"title-short": "Attractor Dimension Estimates for Dynamical Systems",
		"URL": "https://www.springer.com/gp/book/9783030509866",
		"author": [
			{
				"family": "Kuznetsov",
				"given": "Nikolay"
			},
			{
				"family": "Reitmann",
				"given": "Volker"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ershov1998",
		"type": "article-journal",
		"abstract": "We propose the concept of stationary Lyapunov basis — the basis of tangent vectors e(i)(x) defined at every point x of the attractor of the dynamical system, and show that one can reformulate some algorithms for calculation of Lyapunov exponents λi so that each λi can be treated as the average of a function Si(x). This enables one to use measure averaging in theoretical arguments thus proposing the rigorous basis for a number of ideas for calculation of Lyapunov exponents from time series. We also study how the Lyapunov vectors in Benettin's algorithm converge to the stationary basis and show that this convergence rate determines continuity of the field of stationary Lyapunov vectors.",
		"container-title": "Physica D: Nonlinear Phenomena",
		"DOI": "10.1016/S0167-2789(98)00013-X",
		"ISSN": "0167-2789",
		"issue": "3",
		"journalAbbreviation": "Physica D: Nonlinear Phenomena",
		"language": "en",
		"page": "167-198",
		"source": "ScienceDirect",
		"title": "On the concept of stationary Lyapunov basis",
		"URL": "https://www.sciencedirect.com/science/article/pii/S016727899800013X",
		"volume": "118",
		"author": [
			{
				"family": "Ershov",
				"given": "Sergey V."
			},
			{
				"family": "Potapov",
				"given": "Alexei B."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1998",
					7,
					15
				]
			]
		}
	},
	{
		"id": "addis2010",
		"type": "article-journal",
		"abstract": "Introduction\nOlder adults often show sustained attention toward positive information and an improved memory for positive events. Little is known about the neural changes that may underlie these effects, although recent research has suggested that older adults may show differential recruitment of prefrontal regions during the successful encoding of emotional information. In the present study, effective connectivity analyses examined the network of regions that college-age and older adults recruited during the encoding of positive and negative images.\nMethods\nParticipants viewed positive and negative images while undergoing a functional magnetic resonance imaging (fMRI) scan. Structural equation modeling was used to compare young and older adults' connectivity among regions of the emotional memory network while they encoded negative or positive items.\nResults\nAging did not impact the connectivity among regions engaged during the encoding of negative information, but age differences did arise during the encoding of positive information. Most notably, in older adults, the ventromedial prefrontal cortex and amygdala strongly influenced hippocampal activity during the encoding of positive information. By contrast, in young adults, a strong thalamic influence on hippocampal activity was evident during encoding.\nConclusions\nThese findings suggest that older adults' “positivity effect” may arise from age-related changes in the interactions between affect-processing regions and the hippocampus during the encoding of positive information.",
		"collection-title": "The Cognitive Neuroscience of Aging",
		"container-title": "Cortex",
		"DOI": "10.1016/j.cortex.2009.04.011",
		"ISSN": "0010-9452",
		"issue": "4",
		"journalAbbreviation": "Cortex",
		"language": "en",
		"page": "425-433",
		"source": "ScienceDirect",
		"title": "There are age-related changes in neural connectivity during the encoding of positive, but not negative, information",
		"URL": "https://www.sciencedirect.com/science/article/pii/S001094520900152X",
		"volume": "46",
		"author": [
			{
				"family": "Addis",
				"given": "Donna R."
			},
			{
				"family": "Leclerc",
				"given": "Christina M."
			},
			{
				"family": "Muscatell",
				"given": "Keely A."
			},
			{
				"family": "Kensinger",
				"given": "Elizabeth A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2010",
					4,
					1
				]
			]
		}
	},
	{
		"id": "munakata2004",
		"type": "article-journal",
		"container-title": "Developmental Science",
		"DOI": "10.1111/j.1467-7687.2004.00331.x",
		"ISSN": "1363-755X, 1467-7687",
		"issue": "2",
		"journalAbbreviation": "Developmental Sci",
		"language": "en",
		"page": "141-148",
		"source": "DOI.org (Crossref)",
		"title": "Hebbian learning and development",
		"URL": "http://doi.wiley.com/10.1111/j.1467-7687.2004.00331.x",
		"volume": "7",
		"author": [
			{
				"family": "Munakata",
				"given": "Yuko"
			},
			{
				"family": "Pfaffly",
				"given": "Jason"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2004",
					4
				]
			]
		}
	},
	{
		"id": "jaeger2007",
		"type": "article-journal",
		"container-title": "Scholarpedia",
		"DOI": "10.4249/scholarpedia.2330",
		"ISSN": "1941-6016",
		"issue": "9",
		"language": "en",
		"page": "2330",
		"source": "www.scholarpedia.org",
		"title": "Echo state network",
		"URL": "http://www.scholarpedia.org/article/Echo_state_network",
		"volume": "2",
		"author": [
			{
				"family": "Jaeger",
				"given": "Herbert"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					9,
					6
				]
			]
		}
	},
	{
		"id": "maass2002",
		"type": "article-journal",
		"abstract": "A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.",
		"container-title": "Neural Computation",
		"DOI": "10.1162/089976602760407955",
		"ISSN": "0899-7667",
		"issue": "11",
		"journalAbbreviation": "Neural Computation",
		"page": "2531-2560",
		"source": "Silverchair",
		"title": "Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations",
		"title-short": "Real-Time Computing Without Stable States",
		"URL": "https://doi.org/10.1162/089976602760407955",
		"volume": "14",
		"author": [
			{
				"family": "Maass",
				"given": "Wolfgang"
			},
			{
				"family": "Natschläger",
				"given": "Thomas"
			},
			{
				"family": "Markram",
				"given": "Henry"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2002",
					11,
					1
				]
			]
		}
	},
	{
		"id": "jaeger2001",
		"type": "article-journal",
		"container-title": "Bonn, Germany: German National Research Center for Information Technology GMD Technical Report",
		"issue": "34",
		"note": "publisher: Bonn",
		"page": "13",
		"title": "The “echo state” approach to analysing and training recurrent neural networks-with an erratum note",
		"volume": "148",
		"author": [
			{
				"family": "Jaeger",
				"given": "Herbert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2001"
				]
			]
		}
	},
	{
		"id": "pathak2018",
		"type": "article-journal",
		"abstract": "We demonstrate the effectiveness of using machine learning for model-free prediction of spatiotemporally chaotic systems of arbitrarily large spatial extent and attractor dimension purely from observations of the system’s past evolution. We present a parallel scheme with an example implementation based on the reservoir computing paradigm and demonstrate the scalability of our scheme using the Kuramoto-Sivashinsky equation as an example of a spatiotemporally chaotic system.",
		"container-title": "Physical Review Letters",
		"DOI": "10.1103/PhysRevLett.120.024102",
		"issue": "2",
		"journalAbbreviation": "Phys. Rev. Lett.",
		"note": "publisher: American Physical Society",
		"page": "024102",
		"source": "APS",
		"title": "Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach",
		"title-short": "Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data",
		"URL": "https://link.aps.org/doi/10.1103/PhysRevLett.120.024102",
		"volume": "120",
		"author": [
			{
				"family": "Pathak",
				"given": "Jaideep"
			},
			{
				"family": "Hunt",
				"given": "Brian"
			},
			{
				"family": "Girvan",
				"given": "Michelle"
			},
			{
				"family": "Lu",
				"given": "Zhixin"
			},
			{
				"family": "Ott",
				"given": "Edward"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					6,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					12
				]
			]
		}
	},
	{
		"id": "carroll2019",
		"type": "article-journal",
		"container-title": "Chaos: An Interdisciplinary Journal of Nonlinear Science",
		"issue": "8",
		"note": "ISBN: 1054-1500\npublisher: AIP Publishing LLC",
		"page": "083130",
		"title": "Network structure effects in reservoir computers",
		"volume": "29",
		"author": [
			{
				"family": "Carroll",
				"given": "Thomas L."
			},
			{
				"family": "Pecora",
				"given": "Louis M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "dale2019",
		"type": "paper-conference",
		"abstract": "We explore the effect of structure and connection complexity on the dynamical behaviour of Reservoir Computers (RC). At present, considerable effort is taken to design and hand-craft physical reservoir computers. Both structure and physical complexity are often pivotal to task performance, however, assessing their overall importance is challenging. Using a recently proposed framework, we evaluate and compare the dynamical freedom (referring to quality) of neural network structures, as an analogy for physical systems. The results quantify how structure affects the range of behaviours exhibited by these networks. It highlights that high quality reached by more complex structures is often also achievable in simpler structures with greater network size. Alternatively, quality is often improved in smaller networks by adding greater connection complexity. This work demonstrates the benefits of using abstract behaviour representation, rather than evaluation through benchmark tasks, to assess the quality of computing substrates, as the latter typically has biases, and often provides little insight into the complete computing quality of physical systems.",
		"collection-title": "Lecture Notes in Computer Science",
		"container-title": "Unconventional Computation and Natural Computation",
		"DOI": "10.1007/978-3-030-19311-9_6",
		"event-place": "Cham",
		"ISBN": "978-3-030-19311-9",
		"language": "en",
		"page": "52-64",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "Springer Link",
		"title": "The Role of Structure and Complexity on Reservoir Computing Quality",
		"author": [
			{
				"family": "Dale",
				"given": "Matthew"
			},
			{
				"family": "Dewhirst",
				"given": "Jack"
			},
			{
				"family": "O’Keefe",
				"given": "Simon"
			},
			{
				"family": "Sebald",
				"given": "Angelika"
			},
			{
				"family": "Stepney",
				"given": "Susan"
			},
			{
				"family": "Trefzer",
				"given": "Martin A."
			}
		],
		"editor": [
			{
				"family": "McQuillan",
				"given": "Ian"
			},
			{
				"family": "Seki",
				"given": "Shinnosuke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "girko1984",
		"type": "article-journal",
		"container-title": "Izvestiya Akademii Nauk SSSR, Seriya Fizicheskaya",
		"issue": "1",
		"page": "166-171",
		"title": "On the theory of fluctuations of unitary S matrix in nuclear reactions",
		"volume": "48",
		"author": [
			{
				"family": "Girko",
				"given": "V. L."
			},
			{
				"family": "Ol'khovskij",
				"given": "V. S."
			},
			{
				"family": "Chinarov",
				"given": "V. A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"1984"
				]
			]
		}
	},
	{
		"id": "bai1997",
		"type": "article-journal",
		"container-title": "The Annals of Probability",
		"issue": "1",
		"note": "ISBN: 0091-1798\npublisher: Institute of Mathematical Statistics",
		"page": "494-529",
		"title": "Circular law",
		"volume": "25",
		"author": [
			{
				"family": "Bai",
				"given": "Zhi Dong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1997"
				]
			]
		}
	},
	{
		"id": "gotze2010",
		"type": "article-journal",
		"container-title": "Annals of Probability",
		"issue": "4",
		"note": "ISBN: 0091-1798\npublisher: Institute of Mathematical Statistics",
		"page": "1444-1491",
		"title": "The circular law for random matrices",
		"volume": "38",
		"author": [
			{
				"family": "Götze",
				"given": "Friedrich"
			},
			{
				"family": "Tikhomirov",
				"given": "Alexander"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2010"
				]
			]
		}
	},
	{
		"id": "pan2010",
		"type": "article-journal",
		"container-title": "Journal of Multivariate Analysis",
		"issue": "3",
		"note": "ISBN: 0047-259X\npublisher: Elsevier",
		"page": "645-656",
		"title": "Circular law, extreme singular values and potential theory",
		"volume": "101",
		"author": [
			{
				"family": "Pan",
				"given": "Guangming"
			},
			{
				"family": "Zhou",
				"given": "Wang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2010"
				]
			]
		}
	},
	{
		"id": "sepah2017",
		"type": "article-journal",
		"abstract": "<h3>Objective</h3> <p>Translations of the Diabetes Prevention Program (DPP) have proliferated in recent years, with increasing expansion to digital formats. Although these DPP translations have consistently shown favorable clinical outcomes, long-term data for digital formats are limited. This study’s objective was to examine clinical outcomes up to 3 years post-baseline and the relationship between program engagement and clinical outcomes in a digital DPP.</p><h3>Research design and methods</h3> <p>In a single-arm, non-randomized trial, 220 patients previously diagnosed with prediabetes were enrolled in the Omada Health Program, a commercially available, 16-week DPP-based weight loss intervention followed by an ongoing weight maintenance intervention. Changes in body weight and A1c were assessed annually. Relationships between program engagement during the first year and clinical outcomes across 3 years were examined.</p><h3>Results</h3> <p>Participants were socioeconomically diverse (62% women, 50.2% non-Hispanic white, 51.7% college educated or higher). From baseline to 3 years, those participants who completed four or more lessons and nine or more lessons achieved significant sustained weight loss (–3.0% and –2.9%, respectively) and an absolute reduction in A1c (–0.31 and –0.33, respectively) with an average remission from the prediabetes range to the normal glycemic range. Factor analysis of engagement metrics during the first year revealed two underlying dimensions, one comprising lesson completion and health behavior tracking consistency, and the other comprising website logins and group participation. When these two factors were used to predict weight loss, only the logins and group participation factor was a significant predictor of weight loss at 16 weeks and 1 year.</p><h3>Conclusions</h3> <p>This study demonstrates significant long-term reductions in body weight and A1c in a digital DPP and identifies patterns of program engagement that predict weight loss.</p>",
		"container-title": "BMJ Open Diabetes Research and Care",
		"DOI": "10.1136/bmjdrc-2017-000422",
		"ISSN": "2052-4897",
		"issue": "1",
		"language": "en",
		"license": "© Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2017. All rights reserved. No commercial use is permitted unless otherwise expressly granted.. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/",
		"note": "publisher: BMJ Specialist Journals\nsection: Emerging Technologies, Pharmacology and Therapeutics",
		"page": "e000422",
		"source": "drc.bmj.com",
		"title": "Engagement and outcomes in a digital Diabetes Prevention Program: 3-year update",
		"title-short": "Engagement and outcomes in a digital Diabetes Prevention Program",
		"URL": "https://drc.bmj.com/content/5/1/e000422",
		"volume": "5",
		"author": [
			{
				"family": "Sepah",
				"given": "S. Cameron"
			},
			{
				"family": "Jiang",
				"given": "Luohua"
			},
			{
				"family": "Ellis",
				"given": "Robert J."
			},
			{
				"family": "McDermott",
				"given": "Kelly"
			},
			{
				"family": "Peters",
				"given": "Anne L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					4
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					9,
					1
				]
			]
		}
	},
	{
		"id": "sepah2014",
		"type": "article-journal",
		"abstract": "PurposeThe purpose of this study was to evaluate the efficacy of Prevent, an online social network-based translation of the Diabetes Prevention Program (DPP) lifestyle intervention, against the Centers for Disease Control and Prevention (CDC) Diabetes Prevention and Recognition Program (DPRP) outcome standards and weight loss outcomes of other DPP translations.MethodsTwo hundred twenty participants previously diagnosed with prediabetes were recruited online and enrolled in Prevent, a DPP-based group lifestyle intervention that integrates a private online social network, weekly lessons, health coaching, and a wireless scale and pedometer. Participants underwent a core 16-week intensive lifestyle change intervention and were then offered to continue with a post-core lifestyle change maintenance intervention, with the entire intervention (core plus post-core) totaling 12 months.ResultsOne hundred eighty-seven participants met inclusion criteria for the core program and achieved an average of 5.0% and 4.8% weight loss at 16 weeks and 12 months, respectively. They also had a 0.37% reduction in their A1C level at final measurement. One hundred forty-four of these same participants also met inclusion criteria for the post-core program and achieved an average of 5.4% and 5.2% weight loss at 16 weeks and 12 months, respectively, and a 0.40% reduction in A1C at final measurement.ConclusionResults indicate that Prevent meets CDC DPRP outcome standards for diabetes prevention programs and performs favorably to other DPP translations. Considering national initiatives to address the obesity and diabetes epidemics, online delivery platforms like Prevent offer an effective and scalable solution.",
		"container-title": "The Diabetes Educator",
		"DOI": "10.1177/0145721714531339",
		"ISSN": "0145-7217",
		"issue": "4",
		"journalAbbreviation": "Diabetes Educ",
		"language": "en",
		"note": "publisher: SAGE Publications Inc",
		"page": "435-443",
		"source": "SAGE Journals",
		"title": "Translating the Diabetes Prevention Program into an Online Social Network: Validation against CDC Standards",
		"title-short": "Translating the Diabetes Prevention Program into an Online Social Network",
		"URL": "https://doi.org/10.1177/0145721714531339",
		"volume": "40",
		"author": [
			{
				"family": "Sepah",
				"given": "S. Cameron"
			},
			{
				"family": "Jiang",
				"given": "Luohua"
			},
			{
				"family": "Peters",
				"given": "Anne L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					4
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					7,
					1
				]
			]
		}
	},
	{
		"id": "zotero-1005",
		"type": "webpage",
		"language": "en",
		"note": "DOI: 10.1016/j.arth.2018.04.004",
		"title": "Practice Management Strategies Among Current Members of the American Association of Hip and Knee Surgeons | Elsevier Enhanced Reader",
		"URL": "https://reader.elsevier.com/reader/sd/pii/S088354031830353X?token=1895D9355045C432948BDCFEDDC3A267A0AA6FF2ED0A30B23E0D477EB7E1973D4E76B2AA491A3BEB948FFAF870DAFA30&originRegion=us-east-1&originCreation=20210707224354",
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					8
				]
			]
		}
	},
	{
		"id": "teare2021",
		"type": "webpage",
		"abstract": "European startups got a bigger slice of a bigger venture funding pie in the first half of 2021. Venture funding to startups in Europe totaled an unprecedented $59 billion, Crunchbase data shows, up from $18.5 billion for the first half of 2020.",
		"container-title": "Crunchbase News",
		"language": "en-US",
		"title": "European Startups Got A Bigger Share Of Record Global VC Invested In H1 2021",
		"URL": "https://news.crunchbase.com/news/european-vc-funding-h1-2021/",
		"author": [
			{
				"family": "Teare",
				"given": "Gené"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					15
				]
			]
		}
	},
	{
		"id": "wauters2019",
		"type": "webpage",
		"abstract": "Today, Tech.eu and Stripe are releasing a new research report focused on analysing the increase of late-stage funding rounds (financing rounds of €100 million or more) for fast-growing Europe-born tech companies. Titled “Blooming Late: The rise of late-stage funding for European tech scale-ups”, it’s the third and last in a series of reports diving into […]",
		"container-title": "Tech.eu",
		"language": "en-GB",
		"title": "Blooming Late: The rise of late-stage funding for European tech scale-ups (report)",
		"title-short": "Blooming Late",
		"URL": "https://tech.eu/research/28917/blooming-late-stage-funding-europe-european-tech-report/",
		"author": [
			{
				"family": "Wauters",
				"given": "Robin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					11,
					4
				]
			]
		}
	},
	{
		"id": "orizi2016",
		"type": "webpage",
		"title": "3 out of 4 startups are acquired by US companies -",
		"URL": "https://startupeuropepartnership.eu/3-4-startups-acquired-us-companies/",
		"author": [
			{
				"family": "Orizi",
				"given": "Serena"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "nager2016",
		"type": "report",
		"abstract": "A groundbreaking ITIF survey shows why the country needs to broaden and deepen its pool of potential innovators with better STEM immigration and education policies.",
		"language": "en",
		"publisher": "Information Technology and Innovation Foundation",
		"source": "itif.org",
		"title": "The Demographics of Innovation in the United States",
		"URL": "https://itif.org/publications/2016/02/24/demographics-innovation-united-states",
		"author": [
			{
				"family": "Nager",
				"given": "Adams"
			},
			{
				"family": "Hart",
				"given": "David M."
			},
			{
				"family": "Ezell",
				"given": "Stephen"
			},
			{
				"family": "Atkinson",
				"given": "Robert D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					2,
					24
				]
			]
		}
	},
	{
		"id": "baroudy2020",
		"type": "report",
		"publisher": "McKinsey & Company",
		"title": "Europe’s start-up ecosystem: Heating up, but still facing challenges | McKinsey",
		"URL": "https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/europes-start-up-ecosystem-heating-up-but-still-facing-challenges",
		"author": [
			{
				"family": "Baroudy",
				"given": "Kim"
			},
			{
				"family": "Janmark",
				"given": "Jonatan"
			},
			{
				"family": "Strålin",
				"given": "Tobias"
			},
			{
				"family": "Satyavarapu",
				"given": "Abhi"
			},
			{
				"family": "Ziemke",
				"given": "Zeno"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "economist2019",
		"type": "article-magazine",
		"container-title": "The Economist",
		"title": "Germany’s business barons are finding it harder to keep a low profile | The Economist",
		"URL": "https://www.economist.com/business/2019/06/15/germanys-business-barons-are-finding-it-harder-to-keep-a-low-profile",
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					6,
					15
				]
			]
		}
	},
	{
		"id": "stoffels2015",
		"type": "article-magazine",
		"abstract": "Universiteiten mogen sinds kort studenten met een gemiddeld cijfer lager dan een zeven weigeren voor hun masteropleidingen. Dit is niet oké.",
		"container-title": "VICE",
		"language": "nl",
		"title": "Ik begrijp niet goed wat er mis is met de zesjescultuur",
		"URL": "https://www.vice.com/nl/article/jmw7ak/ik-begrijp-niet-goed-wat-er-mis-is-met-de-zesjescultuur-666",
		"author": [
			{
				"family": "Stoffels",
				"given": "Twan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "duncan2019",
		"type": "article-magazine",
		"abstract": "Global survey reveals a public worried about living standards and the future of the planet",
		"container-title": "the Guardian",
		"language": "en",
		"note": "section: World news",
		"title": "European countries among gloomiest in developed world – poll",
		"URL": "http://www.theguardian.com/world/2019/may/03/european-countries-gloomiest-developed-world-poll",
		"author": [
			{
				"family": "Duncan",
				"given": "Pamela"
			},
			{
				"family": "Rankin",
				"given": "Jennifer"
			},
			{
				"family": "Rice-Oxley",
				"given": "Marc"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					5,
					3
				]
			]
		}
	},
	{
		"id": "ferreira2018",
		"type": "post-weblog",
		"abstract": "Despite a considerable premium on equity compared to risk-free assets, many households do not own any financial investments. Personal risk preferences play a crucial role in understanding this economic behaviour. This column analyses financial risk attitudes across 15 countries and identifies relevant factors that affect the willingness to take risky investment decisions. The results reveal a significant heterogeneous attitude of risk-aversion in all countries and suggest that standard portfolio-investment theory does not always hold.",
		"container-title": "VoxEU.org",
		"title": "Cross-country differences in risk attitudes towards financial investment",
		"URL": "https://voxeu.org/article/cross-country-differences-risk-attitudes-towards-financial-investment",
		"author": [
			{
				"family": "Ferreira",
				"given": "Maria"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					9,
					21
				]
			]
		}
	},
	{
		"id": "roser2013",
		"type": "article-journal",
		"abstract": "How are incomes distributed and how and why did the distribution change over time?",
		"container-title": "Our World in Data",
		"journalAbbreviation": "Our World in Data",
		"source": "ourworldindata.org",
		"title": "Income Inequality",
		"URL": "https://ourworldindata.org/income-inequality",
		"author": [
			{
				"family": "Roser",
				"given": "Max"
			},
			{
				"family": "Ortiz-Ospina",
				"given": "Esteban"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013",
					12,
					5
				]
			]
		}
	},
	{
		"id": "adamson2011",
		"type": "report",
		"collection-title": "Innocenti Report Card",
		"number": "11",
		"publisher": "UNICEF",
		"title": "Child Well-being in Rich Countries: A comparative overview",
		"URL": "https://www.unicef-irc.org/publications/683-child-well-being-in-rich-countries-a-comparative-overview.html",
		"author": [
			{
				"family": "Adamson",
				"given": "Peter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		}
	},
	{
		"id": "thompson2017",
		"type": "article-journal",
		"abstract": "And the bright side of rising pessimism about the American Dream",
		"container-title": "The Atlantic",
		"language": "en",
		"note": "section: Business",
		"title": "The Dark Side of American Optimism",
		"URL": "https://www.theatlantic.com/business/archive/2017/01/the-dark-side-of-american-optimism/513680/",
		"author": [
			{
				"family": "Thompson",
				"given": "Derek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					1,
					19
				]
			]
		}
	},
	{
		"id": "rosin2015",
		"type": "article-magazine",
		"abstract": "Why are so many kids with bright prospects killing themselves in Palo Alto?",
		"container-title": "The Atlantic",
		"language": "en",
		"note": "section: Education",
		"title": "The Silicon Valley Suicides",
		"URL": "https://www.theatlantic.com/magazine/archive/2015/12/the-silicon-valley-suicides/413140/",
		"author": [
			{
				"family": "Rosin",
				"given": "Hanna"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					11,
					17
				]
			]
		}
	},
	{
		"id": "collin2014",
		"type": "article-magazine",
		"title": "The difference between raising seed capital in the EU vs. US",
		"URL": "https://tech.eu/features/3064/difference-raising-seed-capital-us-europe/",
		"author": [
			{
				"family": "Collin",
				"given": "Mathilde"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "taleb2007",
		"type": "book",
		"call-number": "Q375 .T35 2007",
		"edition": "1st ed",
		"event-place": "New York",
		"ISBN": "978-1-4000-6351-2",
		"note": "OCLC: ocm71833470",
		"number-of-pages": "366",
		"publisher": "Random House",
		"publisher-place": "New York",
		"source": "Library of Congress ISBN",
		"title": "The black swan: the impact of the highly improbable",
		"title-short": "The black swan",
		"author": [
			{
				"family": "Taleb",
				"given": "Nassim Nicholas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2007"
				]
			]
		}
	},
	{
		"id": "parker1978",
		"type": "article-journal",
		"container-title": "Annual Review of Entomology",
		"DOI": "10.1146/annurev.en.23.010178.001133",
		"issue": "1",
		"note": "_eprint: https://doi.org/10.1146/annurev.en.23.010178.001133",
		"page": "173-196",
		"source": "Annual Reviews",
		"title": "Evolution of Competitive Mate Searching",
		"URL": "https://doi.org/10.1146/annurev.en.23.010178.001133",
		"volume": "23",
		"author": [
			{
				"family": "Parker",
				"given": "G A"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					8,
					31
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1978"
				]
			]
		}
	},
	{
		"id": "renous2021",
		"type": "post-weblog",
		"abstract": "The Facebook and Ning Story",
		"container-title": "Opportunities in Consumer Social",
		"language": "en",
		"title": "Opportunities in Consumer Social",
		"URL": "https://thefutureofsocial.co/opportunities",
		"author": [
			{
				"family": "Renous",
				"given": "Ariel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					8,
					31
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "frey2017",
		"type": "article-journal",
		"abstract": "We examine how susceptible jobs are to computerisation. To assess this, we begin by implementing a novel methodology to estimate the probability of computerisation for 702 detailed occupations, using a Gaussian process classiﬁer. Based on these estimates, we examine expected impacts of future computerisation on US labour market outcomes, with the primary objective of analysing the number of jobs at risk and the relationship between an occupation’s probability of computerisation, wages and educational attainment. According to our estimates, about 47 percent of total US employment is at risk. We further provide evidence that wages and educational attainment exhibit a strong negative relationship with an occupation’s probability of computerisation.",
		"container-title": "Technological Forecasting and Social Change",
		"DOI": "10.1016/j.techfore.2016.08.019",
		"ISSN": "00401625",
		"journalAbbreviation": "Technological Forecasting and Social Change",
		"language": "en",
		"page": "254-280",
		"source": "DOI.org (Crossref)",
		"title": "The future of employment: How susceptible are jobs to computerisation?",
		"title-short": "The future of employment",
		"URL": "https://linkinghub.elsevier.com/retrieve/pii/S0040162516302244",
		"volume": "114",
		"author": [
			{
				"family": "Frey",
				"given": "Carl Benedikt"
			},
			{
				"family": "Osborne",
				"given": "Michael A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					1
				]
			]
		}
	},
	{
		"id": "frey2017a",
		"type": "article-journal",
		"abstract": "We examine how susceptible jobs are to computerisation. To assess this, we begin by implementing a novel methodology to estimate the probability of computerisation for 702 detailed occupations, using a Gaussian process classifier. Based on these estimates, we examine expected impacts of future computerisation on US labour market outcomes, with the primary objective of analysing the number of jobs at risk and the relationship between an occupations probability of computerisation, wages and educational attainment.",
		"container-title": "Technological Forecasting and Social Change",
		"DOI": "10.1016/j.techfore.2016.08.019",
		"ISSN": "0040-1625",
		"journalAbbreviation": "Technological Forecasting and Social Change",
		"language": "en",
		"page": "254-280",
		"source": "ScienceDirect",
		"title": "The future of employment: How susceptible are jobs to computerisation?",
		"title-short": "The future of employment",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0040162516302244",
		"volume": "114",
		"author": [
			{
				"family": "Frey",
				"given": "Carl Benedikt"
			},
			{
				"family": "Osborne",
				"given": "Michael A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					1,
					1
				]
			]
		}
	},
	{
		"id": "marengo2019",
		"type": "article-journal",
		"abstract": "The diffusion of digital technologies, computers, robots and now the outbreak of artificial intelligence and internet of things is causing major changes in the demand for labour. Many jobs are rapidly disappearing because the corresponding tasks are automated and this substitution concerns not only low-skill manual and routine jobs, but more and more also cognitive medium- and even high-skill jobs. In this article, I briefly discuss two alternative views. One view claims that we are in a transition phase, but, alike the previous industrial revolutions, in the long run the balance between lost and created jobs will be positive both in numbers and, especially, in quality. The other view claims instead that the economic characteristics of the technologies of the current industrial revolution are profoundly different from the previous ones and that their impact on employment and social equality is likely to be negative on the whole.",
		"container-title": "Journal of Industrial and Business Economics",
		"DOI": "10.1007/s40812-019-00123-z",
		"ISSN": "1972-4977",
		"issue": "3",
		"journalAbbreviation": "J. Ind. Bus. Econ.",
		"language": "en",
		"page": "323-331",
		"source": "Springer Link",
		"title": "Is this time different? A note on automation and labour in the fourth industrial revolution",
		"title-short": "Is this time different?",
		"URL": "https://doi.org/10.1007/s40812-019-00123-z",
		"volume": "46",
		"author": [
			{
				"family": "Marengo",
				"given": "Luigi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					9,
					1
				]
			]
		}
	},
	{
		"id": "frey2019",
		"type": "book",
		"event-place": "Princeton",
		"ISBN": "978-0-691-19195-9",
		"note": "DOI: 10.1515/9780691191959",
		"publisher": "Princeton University Press",
		"publisher-place": "Princeton",
		"source": "DOI.org (Crossref)",
		"title": "The Technology Trap: Capital, Labor, and Power in the Age of Automation",
		"title-short": "The Technology Trap",
		"URL": "https://www.degruyter.com/document/doi/10.1515/9780691191959/html",
		"author": [
			{
				"family": "Frey",
				"given": "Carl Benedikt"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					12,
					31
				]
			]
		}
	},
	{
		"id": "galeon2017",
		"type": "article-magazine",
		"abstract": "Elon Musk believes artificial intelligence that is much smarter than the smartest human on Earth could result in dangerous situations.  \nMusk argues that the government must introduce a universal basic income program in order to compensate for automation.",
		"container-title": "Futurism",
		"title": "Elon Musk: Automation Will Force Governments to Introduce Universal Basic Income",
		"title-short": "Elon Musk",
		"URL": "https://futurism.com/elon-musk-automation-will-force-governments-to-introduce-universal-basic-income",
		"author": [
			{
				"family": "Galeon",
				"given": "Dom"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "kristof2020",
		"type": "article-newspaper",
		"abstract": "Across America, working-class people — including many of our friends — are dying of despair. And we’re still blaming the wrong people.",
		"container-title": "The New York Times",
		"ISSN": "0362-4331",
		"language": "en-US",
		"section": "Opinion",
		"source": "NYTimes.com",
		"title": "Opinion | Who Killed the Knapp Family?",
		"URL": "https://www.nytimes.com/2020/01/09/opinion/sunday/deaths-despair-poverty.html",
		"author": [
			{
				"family": "Kristof",
				"given": "Nicholas"
			},
			{
				"family": "WuDunn",
				"given": "Sheryl"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					1,
					9
				]
			]
		}
	},
	{
		"id": "bosch2009",
		"type": "article-newspaper",
		"abstract": "Female part-time work is much more popular and persistent in the Netherlands than in any other OECD country. A 2001 tax reform that raised the after-tax hourly wage increased female labour force participation but actually reduced hours worked. This column explains why Dutch women are happy to work part-time.",
		"container-title": "VoxEU",
		"title": "Female part-time work in the Netherlands",
		"URL": "https://voxeu.org/article/why-dutch-women-work-part-time",
		"author": [
			{
				"family": "Bosch",
				"given": "Nicole"
			},
			{
				"family": "Ours",
				"given": "Jan",
				"dropping-particle": "van"
			},
			{
				"family": "Klaauw",
				"given": "Bas",
				"dropping-particle": "van der"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009",
					9,
					5
				]
			]
		}
	},
	{
		"id": "economist2020-worklife",
		"type": "article-magazine",
		"abstract": "Working Britons have less time for leisure than other Europeans. Covid-19 is changing that",
		"container-title": "The Economist",
		"ISSN": "0013-0613",
		"source": "The Economist",
		"title": "Britons tip the work-life balance",
		"URL": "https://www.economist.com/britain/2020/07/18/britons-tip-the-work-life-balance",
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					18
				]
			]
		}
	},
	{
		"id": "lander1987",
		"type": "article-journal",
		"abstract": "Recently, the Balans chair has been introduced with claims that, because of its semi-kneeling position, individuals will experience decreased low-back pain (LBP) as well as improvement in circulation. This study investigated the validity of these claims. Twenty healthy subjects were randomly assigned to one of two groups. Group 1 subjects sat in the Balans chair for a 30-minute study period and then sat in a conventional office chair for an additional 30-minute period. Group 2 subjects were studied in the reverse seating order. Parameters studied were cervical and lumbar paraspinous surface EMG, and pedal cutaneous blood flow measured by laser-Doppler flowimetry. In addition, a questionnaire comparing the comfort of the two chairs was completed at the end of the study session. Comfort ratings showed an overall preference for the conventional chair. Increased cervical (P = .004) and lumbar muscle EMG measurements were noted after sitting in the Balans chair. Pedal cutaneous blood flow was increased by 15% in the Balans chair (P = .001). The data do not support the manufacturer's claim that the Balans chair is likely to decrease complaints of LBP.",
		"container-title": "Spine",
		"DOI": "10.1097/00007632-198704000-00014",
		"ISSN": "1528-1159",
		"issue": "3",
		"journalAbbreviation": "Spine (Phila Pa 1976)",
		"language": "eng",
		"note": "PMID: 2954222",
		"page": "269-272",
		"source": "Europe PMC",
		"title": "The Balans chair and its semi-kneeling position: an ergonomic comparison with the conventional sitting position",
		"title-short": "The Balans chair and its semi-kneeling position",
		"URL": "https://doi.org/10.1097/00007632-198704000-00014",
		"volume": "12",
		"author": [
			{
				"family": "Lander",
				"given": "C"
			},
			{
				"family": "Korbon",
				"given": "G A"
			},
			{
				"family": "DeGood",
				"given": "D E"
			},
			{
				"family": "Rowlingson",
				"given": "J C"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1987",
					4,
					1
				]
			]
		}
	},
	{
		"id": "osullivan2012",
		"type": "article-journal",
		"abstract": "Low back pain (LBP) is a common musculoskeletal disorder and prolonged sitting often aggravates LBP. A novel dynamic ergonomic chair (‘Back App’), which facilitates less hip flexion while sitting on an unstable base has been developed. This study compared lumbar posture and trunk muscle activation on this novel chair with a standard backless office chair. Twelve painfree participants completed a typing task on both chairs. Lumbar posture and trunk muscle activation were collected simultaneously and were analysed using paired t-tests. Sitting on the novel dynamic chair significantly (p < 0.05) reduced both lumbar flexion and the activation of one back muscle (Iliocostalis Lumborum pars Thoracis). The discomfort experienced was mild and was similar (p > 0.05) between chairs. Maintaining lordosis with less muscle activation during prolonged sitting could reduce the fatigue associated with upright sitting postures. Studies with longer sitting durations, and in people with LBP, are required. Practitioner Summary: Sitting on a novel dynamic chair resulted in less lumbar flexion and less back muscle activation than sitting on a standard backless office chair during a typing task among pain-free participants. Facilitating lordotic sitting with less muscle activation may reduce the fatigue and discomfort often associated with lordotic sitting postures.",
		"container-title": "Ergonomics",
		"DOI": "10.1080/00140139.2012.721521",
		"ISSN": "0014-0139",
		"issue": "12",
		"note": "publisher: Taylor & Francis\n_eprint: https://doi.org/10.1080/00140139.2012.721521\nPMID: 23009637",
		"page": "1586-1595",
		"source": "Taylor and Francis+NEJM",
		"title": "Lumbar posture and trunk muscle activation during a typing task when sitting on a novel dynamic ergonomic chair",
		"URL": "https://doi.org/10.1080/00140139.2012.721521",
		"volume": "55",
		"author": [
			{
				"family": "O’Sullivan",
				"given": "Kieran"
			},
			{
				"family": "McCarthy",
				"given": "Raymond"
			},
			{
				"family": "White",
				"given": "Alison"
			},
			{
				"family": "O’Sullivan",
				"given": "Leonard"
			},
			{
				"family": "Dankaerts",
				"given": "Wim"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					12,
					1
				]
			]
		}
	},
	{
		"id": "minges2016",
		"type": "article-journal",
		"abstract": "CONTEXT: Reducing sedentary behaviors, or time spent sitting, is an important target for health promotion in children. Standing desks in schools may be a feasible, modifiable, and acceptable environmental strategy to this end.\nOBJECTIVE: To examine the impact of school-based standing desk interventions on sedentary behavior and physical activity, health-related outcomes, and academic and behavioral outcomes in school-aged children.\nDATA SOURCES: Ovid Embase, Medline, PsycINFO, Web of Science, Global Health, and CINAHL.\nSTUDY SELECTION: Full-text peer-reviewed journal publications written in English; samples of school-aged youth (5–18 years of age); study designs including the same participants at baseline and follow-up; and use of a standing desk as a component of the intervention.\nDATA EXTRACTION: Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines.\nRESULTS: Eight studies satisfied selection criteria and used quasi-experimental (n = 4), randomized controlled trial (n = 3), and pre–post, no control (n = 1) designs. When examined, time spent standing increased in all studies (effect sizes: 0.38–0.71), while sitting time decreased from a range of 59 to 64 minutes (effect sizes: 0.27–0.49). Some studies reported increased physical activity and energy expenditure and improved classroom behavior.\nLIMITATIONS: One-half of the studies had nonrandomized designs, and most were pilot or feasibility studies.\nCONCLUSIONS: This initial evidence supports integrating standing desks into the classroom environment; this strategy has the potential to reduce sitting time and increase standing time among elementary schoolchildren. Additional research is needed to determine the impact of standing desks on academic performance and precursors of chronic disease risk.",
		"container-title": "Pediatrics",
		"DOI": "10.1542/peds.2015-3087",
		"ISSN": "0031-4005, 1098-4275",
		"issue": "2",
		"language": "en",
		"license": "Copyright © 2016 by the American Academy of Pediatrics",
		"note": "publisher: American Academy of Pediatrics\nsection: Review Article\nPMID: 26801914",
		"source": "pediatrics.aappublications.org",
		"title": "Classroom Standing Desks and Sedentary Behavior: A Systematic Review",
		"title-short": "Classroom Standing Desks and Sedentary Behavior",
		"URL": "https://pediatrics.aappublications.org/content/137/2/e20153087",
		"volume": "137",
		"author": [
			{
				"family": "Minges",
				"given": "Karl E."
			},
			{
				"family": "Chao",
				"given": "Ariana M."
			},
			{
				"family": "Irwin",
				"given": "Melinda L."
			},
			{
				"family": "Owen",
				"given": "Neville"
			},
			{
				"family": "Park",
				"given": "Chorong"
			},
			{
				"family": "Whittemore",
				"given": "Robin"
			},
			{
				"family": "Salmon",
				"given": "Jo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					2,
					1
				]
			]
		}
	},
	{
		"id": "bodker2021",
		"type": "article-journal",
		"abstract": "Sedentary behavior is associated with cardiovascular disease (CVD) and mortality, independent of physical activity. The biological mechanisms underlying these associations are largely unknown. We hypothesized that obese subjects with sedentary desk jobs, when assigned a sit–stand desk, will reduce daily sedentary time, and show improvement in arterial flow-mediated dilation (FMD), an early indicator of CVD. Overweight and obese subjects without known CVD were recruited at our institution and given an adjustable sit–stand desk at work. Activities were quantified with an accelerometer for 7 days at baseline and during the intervention. FMD of the brachial and superficial femoral arteries, fasting lipids, insulin and glucose labs, and anthropometrics were measured at baseline, and 12 and 24 weeks. Repeated one-way ANOVA tests were used to compare measurements over time. Fifteen participants were enrolled (93% female, mean age 40 ± 5 years, mean body mass index [BMI] 33 ± 5). Mean daily sedentary time at work decreased by 90 minutes from baseline (385 ± 49 minutes) to 12 weeks (297 ± 80 minutes, p = 0.002) and 24 weeks (295 ± 127 minutes, p = 0.015). Femoral FMD increased from baseline (4.9 ± 1.7%) to 12 weeks (6.4 ± 2.3%, p = 0.043) and further to 24 weeks (8.1 ± 3.2%, p = 0.009). Significant improvement in fasting triglycerides and insulin resistance occurred. There was no change in brachial FMD, exercise activity, step counts, weight, or BMI. A significant reduction in sedentary time during working hours was identified with utilization of a sit–stand desk and sustained over 24 weeks. Improvements in FMD, triglycerides, and insulin resistance provide insight into mechanisms of adverse health risks associated with sedentary behavior.",
		"container-title": "Vascular Medicine",
		"DOI": "10.1177/1358863X211001934",
		"ISSN": "1358-863X",
		"issue": "4",
		"journalAbbreviation": "Vasc Med",
		"language": "en",
		"note": "publisher: SAGE Publications Ltd STM",
		"page": "374-382",
		"source": "SAGE Journals",
		"title": "The impact of standing desks on cardiometabolic and vascular health",
		"URL": "https://doi.org/10.1177/1358863X211001934",
		"volume": "26",
		"author": [
			{
				"family": "Bodker",
				"given": "Ariel"
			},
			{
				"family": "Visotcky",
				"given": "Alexis"
			},
			{
				"family": "Gutterman",
				"given": "David"
			},
			{
				"family": "Widlansky",
				"given": "Michael E"
			},
			{
				"family": "Kulinski",
				"given": "Jacquelyn"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					8,
					1
				]
			]
		}
	},
	{
		"id": "reiff2012",
		"type": "article-journal",
		"abstract": "<section class=\"abstract\"><div id=\"\" class=\"section\"><h3 class=\"abstractTitle text-title my-1\" id=\"d7931721e85\">Background:</h3><p>Traditional desks require students to sit; however, recently schools have provided students with nontraditional standing desks. The purpose of this study was to investigate differences in caloric expenditure of young adults while sitting at a standard classroom desk and standing at a nontraditional standing classroom desk.</p></div><div id=\"\" class=\"section\"><h3 class=\"abstractTitle text-title my-1\" id=\"d7931721e90\">Methods:</h3><p>Twenty (10 male/10 female) young (22.8 ± 1.9 y), healthy participants reported to the laboratory between the hours of 7:00 AM and 2:00 PM following a 12-h fast and 48-h break in exercise. Participants were randomly assigned to perform a series of mathematical problems either sitting at a normal classroom desk or standing at a nontraditional standing desk. Inspired and expired gases were collected for 45-min for the determination of oxygen consumption (VO<sub>2</sub>), carbon dioxide production (VCO<sub>2</sub>), and minute ventilation (V<sub>E</sub>) using a metabolic gas system.</p></div><div id=\"\" class=\"section\"><h3 class=\"abstractTitle text-title my-1\" id=\"d7931721e104\">Results:</h3><p>There were significant increases from sitting to standing in VO<sub>2</sub> (0.22 ± 0.05 vs. 0.28 ± 0.05 L·min<sup>−1</sup>, <em>P</em> ≤ .0001), VCO<sub>2</sub> (0.18 ± 0.05 vs. 0.24 ± 0.050 L·min<sup>−1</sup>, <em>P</em> ≤ .0001), V<sub>E</sub> (7.72 ± 0.67 vs. 9.41 ± 1.20 L·min<sup>−1</sup>, <em>P</em> ≤ .0001), and kilocalories expended per minute (1.36 ± 0.20 kcal/ min, <em>P</em> ≤ .0001 vs. 1.02 ± 0.22 kcal/min, <em>P</em> ≤ .0001).</p></div><div id=\"\" class=\"section\"><h3 class=\"abstractTitle text-title my-1\" id=\"d7931721e143\">Conclusions:</h3><p>Results indicate a significant increase in caloric expenditure in subjects that were standing at a standing classroom desk compared with sitting at a standard classroom desk.</p></div></section>",
		"container-title": "Journal of Physical Activity and Health",
		"DOI": "10.1123/jpah.9.7.1009",
		"ISSN": "1543-5474, 1543-3080",
		"issue": "7",
		"language": "en_US",
		"note": "publisher: Human Kinetics, Inc.\nsection: Journal of Physical Activity and Health",
		"page": "1009-1011",
		"source": "journals.humankinetics.com",
		"title": "Difference in Caloric Expenditure in Sitting Versus Standing Desks",
		"URL": "https://journals.humankinetics.com/view/journals/jpah/9/7/article-p1009.xml",
		"volume": "9",
		"author": [
			{
				"family": "Reiff",
				"given": "Christopher"
			},
			{
				"family": "Marlatt",
				"given": "Kara"
			},
			{
				"family": "Dengel",
				"given": "Donald R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					9,
					1
				]
			]
		}
	},
	{
		"id": "pronk2012",
		"type": "article-journal",
		"container-title": "Preventing Chronic Disease",
		"DOI": "10.5888/pcd9.110323",
		"ISSN": "1545-1151",
		"journalAbbreviation": "Prev Chronic Dis",
		"language": "eng",
		"source": "www.cdc.gov",
		"title": "Reducing Occupational Sitting Time and Improving Worker Health: The Take-a-Stand Project, 2011",
		"title-short": "Reducing Occupational Sitting Time and Improving Worker Health",
		"URL": "https://www.cdc.gov/pcd/issues/2012/11_0323.htm",
		"volume": "9",
		"author": [
			{
				"family": "Pronk",
				"given": "Nicolaas P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "hickman2021",
		"type": "report",
		"abstract": "Background: Climate change has significant implications for the health and futures of children and young people, yet they have little power to limit its harm, making them vulnerable to increased climate anxiety. Qualitative studies show climate anxiety is associated with perceptions of inadequate action by adults and governments, feelings of betrayal, abandonment and moral injury. This study offers the first large-scale investigation of climate anxiety in children and young people globally and its relationship to government response. Methods: We surveyed 10,000 young people (aged 16-25 years) in ten countries. Data were collected on their thoughts and feelings about climate change, and government response.  Findings: Respondents were worried about climate change (59% very or extremely worried, 84% at least moderately worried). Over 50% felt sad, anxious, angry, powerless, helpless, and guilty. Over 45% said their feelings about climate change negatively affected their daily life and functioning, and many reported a high number of negative thoughts about climate change. Respondents rated the governmental response to climate change negatively and reported greater feelings of betrayal than of reassurance. Correlations indicated that climate anxiety and distress were significantly related to perceived inadequate government response and associated feelings of betrayal.  Interpretation: Climate change and inadequate governmental responses are associated with climate anxiety and distress in many children and young people globally. These psychological stressors threaten health and wellbeing, and could be construed as morally injurious and unjust. There is an urgent need for increases in both research and government responsiveness. Funding: The costs of the survey were funded by AVAAZ.Declaration of Interest: None to declare. Ethical Approval: The study was approved by the University of Bath Psychology Ethics Committee (#21-090).",
		"event-place": "Rochester, NY",
		"genre": "SSRN Scholarly Paper",
		"language": "en",
		"note": "DOI: 10.2139/ssrn.3918955",
		"number": "ID 3918955",
		"publisher": "Social Science Research Network",
		"publisher-place": "Rochester, NY",
		"source": "papers.ssrn.com",
		"title": "Young People's Voices on Climate Anxiety, Government Betrayal and Moral Injury: A Global Phenomenon",
		"title-short": "Young People's Voices on Climate Anxiety, Government Betrayal and Moral Injury",
		"URL": "https://papers.ssrn.com/abstract=3918955",
		"author": [
			{
				"family": "Hickman",
				"given": "Caroline"
			},
			{
				"family": "Marks",
				"given": "Elizabeth"
			},
			{
				"family": "Pihkala",
				"given": "Panu"
			},
			{
				"family": "Clayton",
				"given": "Susan"
			},
			{
				"family": "Lewandowski",
				"given": "Eric R."
			},
			{
				"family": "Mayall",
				"given": "Elouise E."
			},
			{
				"family": "Wray",
				"given": "Britt"
			},
			{
				"family": "Mellor",
				"given": "Catriona"
			},
			{
				"family": "Susteren",
				"given": "Lise",
				"non-dropping-particle": "van"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					9,
					7
				]
			]
		}
	},
	{
		"id": "dunstan2012",
		"type": "article-journal",
		"container-title": "Diabetes Research and Clinical Practice",
		"DOI": "10.1016/j.diabres.2012.05.020",
		"ISSN": "01688227",
		"issue": "3",
		"journalAbbreviation": "Diabetes Research and Clinical Practice",
		"language": "en",
		"page": "368-376",
		"source": "DOI.org (Crossref)",
		"title": "Too much sitting – A health hazard",
		"URL": "https://linkinghub.elsevier.com/retrieve/pii/S0168822712002082",
		"volume": "97",
		"author": [
			{
				"family": "Dunstan",
				"given": "David W."
			},
			{
				"family": "Howard",
				"given": "Bethany"
			},
			{
				"family": "Healy",
				"given": "Genevieve N."
			},
			{
				"family": "Owen",
				"given": "Neville"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					9
				]
			]
		}
	},
	{
		"id": "segerstrom2007",
		"type": "article-journal",
		"abstract": "Experimental research reliably demonstrates that self-regulatory deficits are a consequence of prior self-regulatory effort. However, in naturalistic settings, although people know that they are sometimes vulnerable to saying, eating, or doing the wrong thing, they cannot accurately gauge their capacity to self-regulate at any given time. Because self-regulation and autonomic regulation colocalize in the brain, an autonomic measure, heart rate variability (HRV), could provide an index of self-regulatory strength and activity. During an experimental manipulation of self-regulation (eating carrots or cookies), HRV was elevated during high self-regulatory effort (eat carrots, resist cookies) compared with low self-regulatory effort (eat cookies, resist carrots). The experimental manipulation and higher HRV at baseline independently predicted persistence at a subsequent anagram task. HRV appears to index self-regulatory strength and effort, making it possible to study these phenomena in the field as well as the lab.",
		"container-title": "Psychological Science",
		"DOI": "10.1111/j.1467-9280.2007.01888.x",
		"ISSN": "0956-7976",
		"issue": "3",
		"journalAbbreviation": "Psychol Sci",
		"language": "en",
		"note": "publisher: SAGE Publications Inc",
		"page": "275-281",
		"source": "SAGE Journals",
		"title": "Heart Rate Variability Reflects Self-Regulatory Strength, Effort, and Fatigue",
		"URL": "https://doi.org/10.1111/j.1467-9280.2007.01888.x",
		"volume": "18",
		"author": [
			{
				"family": "Segerstrom",
				"given": "Suzanne C."
			},
			{
				"family": "Nes",
				"given": "Lise Solberg"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					3,
					1
				]
			]
		}
	},
	{
		"id": "mcgonigal2011",
		"type": "book",
		"title": "The Willpower Instinct: How Self-Control Works, Why It Matters, and What You Can Do to Get More of It by Kelly McGonigal | Goodreads",
		"URL": "https://www.goodreads.com/book/show/10865206-the-willpower-instinct?from_search=true&from_srp=true&qid=7BJNReIo2I&rank=1",
		"author": [
			{
				"family": "McGonigal",
				"given": "Kelly"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		}
	},
	{
		"id": "su2019",
		"type": "article-journal",
		"abstract": "Objective The purpose of this study was to evaluate the effects of high-intensity interval training (HIIT) and moderate-intensity continuous training (MICT) on cardiovascular disease (CVD) risk factors in adults with overweight and obesity. Methods Twenty-two articles were included by searching six databases, the total number of subjects was 620 in these articles. Outcomes were synthesised using a random-effects meta-analysis of the Standardized mean difference (SMD) in CVD risk factors. Results HIIT and MICT resulted in statistically significant reductions in Weight, BMI, fat%, total cholesterol(TC), and improvement in VO2max. Compared with MICT, subgroup of durations of HIIT training interval ≥2 min can significantly increase VO2max (SMD = 0.444, 95% CI:0.037~0.851,P = 0.032), subgroup of energy expenditure of HIIT equal to MICT can significantly increase VO2max (SMD = 0.399, 95% CI:0.106~0.692,P = 0.008). Conclusions HIIT appears to provide similar benefits to MICT for improving body composition, VO2maxand TC, but HIIT spent less time than MICT by 9.7 min on one session. HIIT is superior to MICT in improving cardiopulmonary fitness when durations of HIIT training interval ≥2 min or energy expenditure of HIIT same as MICT. PROSPERO ID: CRD42016045835.",
		"container-title": "PLOS ONE",
		"DOI": "10.1371/journal.pone.0210644",
		"ISSN": "1932-6203",
		"issue": "1",
		"journalAbbreviation": "PLOS ONE",
		"language": "en",
		"note": "publisher: Public Library of Science",
		"page": "e0210644",
		"source": "PLoS Journals",
		"title": "Effects of HIIT and MICT on cardiovascular risk factors in adults with overweight and/or obesity: A meta-analysis",
		"title-short": "Effects of HIIT and MICT on cardiovascular risk factors in adults with overweight and/or obesity",
		"URL": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0210644",
		"volume": "14",
		"author": [
			{
				"family": "Su",
				"given": "LiQiang"
			},
			{
				"family": "Fu",
				"given": "JinMei"
			},
			{
				"family": "Sun",
				"given": "ShunLi"
			},
			{
				"family": "Zhao",
				"given": "GuangGao"
			},
			{
				"family": "Cheng",
				"given": "Wei"
			},
			{
				"family": "Dou",
				"given": "ChuanChuan"
			},
			{
				"family": "Quan",
				"given": "MingHui"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					1,
					28
				]
			]
		}
	},
	{
		"id": "mattar2017",
		"type": "article-journal",
		"abstract": "BackgroundThe 7-minute workout composed of aerobic and resistance exercises is becoming a very popular workout. It targets individuals with time constrains and low motivation to commit to lengthy and extensive workout programs. The objective of the study is to investigate if the 7-minute workout has a 6 week effect on body weight and composition.MethodsThe training group (N.=29, age 18-30) did the 7-minute workout 7 days a week during 6 weeks while the control group (N.=29) did not perform the workout. Measurements such as height, weight, body mass index, circumferences (middle upper arm, hip, and waist), blood pressure, heart rate, hand grip, and bioelectrical impedance were collected and recorded at week 1, 3 and 6.ResultsMean BMI was 24.4kg/m2 at week 1, 24.01kg/m2 at week 3 (P=0.003). Waist circumference decreased between week 1 and 3 (P=0.003) and week 6 (P=0.01) by 4 cm on average. Hip circumference followed the same trend between week 1 and week 3 (P=-0.001). There was a decrease in fat mass and % fat mass between week 1, 3, and 6 (P=0.001). No changes were noted for mid-upper arm circumference or hand grip.ConclusionsThe findings of this research show that even a very short duration workout affect the nutritional status in normal weight individuals who did not change any of their eating habits. This implies that even in normal weight individuals who perform the 7-minute workout, improvement through a decrease in waist circumference can be achieved thus leading to a better cardio-protective nutritional status. The 7 minutes workout can be a great solution for people to get started and to plan on continuing exercising, as it is simple and of minimal constraints.",
		"container-title": "The Journal of sports medicine and physical fitness",
		"DOI": "10.23736/s0022-4707.16.06788-8",
		"ISSN": "1827-1928",
		"issue": "10",
		"journalAbbreviation": "J Sports Med Phys Fitness",
		"language": "eng",
		"note": "PMID: 28085122",
		"page": "1299-1304",
		"source": "Europe PMC",
		"title": "Effect of 7-minute workout on weight and body composition",
		"URL": "https://doi.org/10.23736/S0022-4707.16.06788-8",
		"volume": "57",
		"author": [
			{
				"family": "Mattar",
				"given": "Lama"
			},
			{
				"family": "Farran",
				"given": "Natali"
			},
			{
				"family": "Bakhour",
				"given": "Dalal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					10,
					1
				]
			]
		}
	},
	{
		"id": "javorka2002",
		"type": "article-journal",
		"abstract": "Physical exercise is associated with parasympathetic withdrawal and increased sympathetic activity resulting in heart rate increase. The rate of post-exercise cardiodeceleration is used as an index of cardiac vagal reactivation. Analysis of heart rate variability (HRV) and complexity can provide useful information about autonomic control of the cardiovascular system. The aim of the present study was to ascertain the association between heart rate decrease after exercise and HRV parameters. Heart rate was monitored in 17 healthy male subjects (mean age: 20 years) during the pre-exercise phase (25 min supine, 5 min standing), during exercise (8 min of the step test with an ascending frequency corresponding to 70% of individual maximal power output) and during the recovery phase (30 min supine). HRV analysis in the time and frequency domains and evaluation of a newly developed complexity measure - sample entropy - were performed on selected segments of heart rate time series. During recovery, heart rate decreased gradually but did not attain pre-exercise values within 30 min after exercise. On the other hand, HRV gradually increased, but did not regain rest values during the study period. Heart rate complexity was slightly reduced after exercise and attained rest values after 30-min recovery. The rate of cardiodeceleration did not correlate with pre-exercise HRV parameters, but positively correlated with HRV measures and sample entropy obtained from the early phases of recovery. In conclusion, the cardiodeceleration rate is independent of HRV measures during the rest period but it is related to early post-exercise recovery HRV measures, confirming a parasympathetic contribution to this phase.",
		"container-title": "Brazilian Journal of Medical and Biological Research",
		"DOI": "10.1590/S0100-879X2002000800018",
		"ISSN": "0100-879X, 1414-431X",
		"journalAbbreviation": "Braz J Med Biol Res",
		"language": "en",
		"note": "publisher: Associação Brasileira de Divulgação Científica",
		"page": "991-1000",
		"source": "SciELO",
		"title": "Heart rate recovery after exercise: relations to heart rate variability and complexity",
		"title-short": "Heart rate recovery after exercise",
		"URL": "http://www.scielo.br/j/bjmbr/a/7mmq5FLwxcxKSJJx6Ns55cx/?lang=en&format=html&stop=next",
		"volume": "35",
		"author": [
			{
				"family": "Javorka",
				"given": "M."
			},
			{
				"family": "Zila",
				"given": "I."
			},
			{
				"family": "Balhárek",
				"given": "T."
			},
			{
				"family": "Javorka",
				"given": "K."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2002",
					8
				]
			]
		}
	},
	{
		"id": "kim2010",
		"type": "article-journal",
		"abstract": "The notion that the brain is organized into two complementary networks, one that is task-positive and supports externally-oriented processing, and the other that is task-negative and supports internally-oriented processing, has recently attracted increasing attention. The goal of the present study was to investigate involvement of the task-positive and task-negative networks in overlapping activity between episodic memory encoding and retrieval. To this end, we performed a functional MRI study that included both encoding and retrieval tasks. We hypothesized that during the study phase, encoding success activity (remembered > forgotten) involves mainly the task-positive network, whereas encoding failure activity (forgotten > remembered) involves mainly the task-negative network. We also hypothesized that during the test phase, retrieval success activity (old > new) involves mainly the task-negative network, whereas novelty detection activity (new > old) involves mainly the task-positive network. Based on these hypotheses, we made 3 predictions regarding study-test overlap. First, there would be relatively high level of overlap between encoding success and novelty detection activity involving the task-positive network. Second, there would be relatively high level of overlap between encoding failure and retrieval success activity involving the task-negative network. Third, there would be relatively low level of overlap between encoding success and retrieval success activity as well as between encoding failure and novelty detection activity. The results fully confirmed our 3 predictions. Taken together, the present findings clarify roles of the task-positive and task-negative networks in encoding and retrieval and the function of overlapping brain activity between encoding and retrieval.",
		"container-title": "NeuroImage",
		"DOI": "10.1016/j.neuroimage.2009.07.058",
		"ISSN": "1053-8119",
		"issue": "1",
		"journalAbbreviation": "NeuroImage",
		"language": "en",
		"page": "1045-1054",
		"source": "ScienceDirect",
		"title": "Overlapping brain activity between episodic memory encoding and retrieval: Roles of the task-positive and task-negative networks",
		"title-short": "Overlapping brain activity between episodic memory encoding and retrieval",
		"URL": "https://www.sciencedirect.com/science/article/pii/S1053811909008489",
		"volume": "49",
		"author": [
			{
				"family": "Kim",
				"given": "Hongkeun"
			},
			{
				"family": "Daselaar",
				"given": "Sander M."
			},
			{
				"family": "Cabeza",
				"given": "Roberto"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2010",
					1,
					1
				]
			]
		}
	},
	{
		"id": "vallance2018",
		"type": "article-journal",
		"container-title": "American Journal of Public Health",
		"DOI": "10.2105/AJPH.2018.304649",
		"ISSN": "0090-0036, 1541-0048",
		"issue": "11",
		"journalAbbreviation": "Am J Public Health",
		"language": "en",
		"page": "1478-1482",
		"source": "DOI.org (Crossref)",
		"title": "Evaluating the Evidence on Sitting, Smoking, and Health: Is Sitting Really the New Smoking?",
		"title-short": "Evaluating the Evidence on Sitting, Smoking, and Health",
		"URL": "https://ajph.aphapublications.org/doi/10.2105/AJPH.2018.304649",
		"volume": "108",
		"author": [
			{
				"family": "Vallance",
				"given": "Jeff K."
			},
			{
				"family": "Gardiner",
				"given": "Paul A."
			},
			{
				"family": "Lynch",
				"given": "Brigid M."
			},
			{
				"family": "D’Silva",
				"given": "Adrijana"
			},
			{
				"family": "Boyle",
				"given": "Terry"
			},
			{
				"family": "Taylor",
				"given": "Lorian M."
			},
			{
				"family": "Johnson",
				"given": "Steven T."
			},
			{
				"family": "Buman",
				"given": "Matthew P."
			},
			{
				"family": "Owen",
				"given": "Neville"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					11
				]
			]
		}
	},
	{
		"id": "french2013",
		"type": "article-newspaper",
		"abstract": "Patrick French on the Mahatma-in-waiting, and the flaw at the heart of a saintly mythology",
		"container-title": "The Guardian",
		"ISSN": "0261-3077",
		"language": "en-GB",
		"section": "Books",
		"source": "The Guardian",
		"title": "Gandhi Before India by Ramachandra Guha – review",
		"URL": "https://www.theguardian.com/books/2013/oct/09/gandhi-before-india-ramachandra-guha-review",
		"author": [
			{
				"family": "French",
				"given": "Patrick"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					22
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013",
					10,
					9
				]
			]
		}
	},
	{
		"id": "popham2015",
		"type": "article-magazine",
		"abstract": "She is accused of taking donations from scoundrels of every sort",
		"container-title": "The Independent",
		"language": "en",
		"note": "section: Voices",
		"title": "How saintly was Mother Teresa, really?",
		"URL": "https://www.independent.co.uk/voices/mother-teresa-canonised-debate-remains-about-how-saintly-she-really-was-a6779346.html",
		"author": [
			{
				"family": "Popham",
				"given": "Peter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					12,
					18
				]
			]
		}
	},
	{
		"id": "hitchens1995",
		"type": "book",
		"call-number": "BX4406.5.Z8 H55 1995",
		"event-place": "London ; New York",
		"ISBN": "978-1-85984-929-3",
		"number-of-pages": "98",
		"publisher": "Verso",
		"publisher-place": "London ; New York",
		"source": "Library of Congress ISBN",
		"title": "The missionary position: Mother Teresa in theory and practice",
		"title-short": "The missionary position",
		"author": [
			{
				"family": "Hitchens",
				"given": "Christopher"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1995"
				]
			]
		}
	},
	{
		"id": "plutarch-cato",
		"type": "book",
		"collection-title": "Loeb Classical Library edition",
		"title": "The Parallel Lives",
		"URL": "https://penelope.uchicago.edu/Thayer/E/Roman/Texts/Plutarch/Lives/Cato_Minor*.html",
		"volume": "VIII",
		"author": [
			{
				"family": "Plutarch",
				"given": ""
			}
		],
		"translator": [
			{
				"family": "Dryden",
				"given": ""
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					9,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1919"
				]
			]
		}
	},
	{
		"id": "zotero-1108",
		"type": "webpage",
		"abstract": "There’s not only significant variability in how much CO2 countries emit across the world today. There are also large differences in how much each has emitted in the past. Who has contributed most to global CO2 since 1750?",
		"container-title": "Our World in Data",
		"title": "Who has contributed most to global CO2 emissions?",
		"URL": "https://ourworldindata.org/contributed-most-global-co2",
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					2
				]
			]
		}
	},
	{
		"id": "bennefield2003",
		"type": "report",
		"collection-title": "Census 2000 Brief",
		"title": "Structural and Occupancy Characteristics of Housing: 2000",
		"URL": "https://www.census.gov/prod/2003pubs/c2kbr-32.pdf",
		"author": [
			{
				"family": "Bennefield",
				"given": "Robert"
			},
			{
				"family": "Bonnette",
				"given": "Robert"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2003"
				]
			]
		}
	},
	{
		"id": "otherlab-sankey",
		"type": "report",
		"abstract": "In 2017,&nbsp;Otherlab was contracted by the Advanced Research Project Agency of the Department of Energy (ARPA-e) to review all available energy data sources and create an ultra-high resolution picture of the U.S. energy economy.",
		"language": "en-US",
		"publisher": "Otherlab",
		"title": "US Energy Flow Super Sankey",
		"URL": "https://www.otherlab.com/blog-posts/us-energy-flow-super-sankey",
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					2
				]
			]
		}
	},
	{
		"id": "2020",
		"type": "report",
		"language": "en",
		"page": "46",
		"publisher": "Congressional Budget Office",
		"source": "Zotero",
		"title": "The Distribution of Household Income, 2017",
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "fu2016",
		"type": "article-journal",
		"container-title": "Renewable Energy",
		"language": "en",
		"page": "48",
		"source": "Zotero",
		"title": "U.S. Solar Photovoltaic System Cost Benchmark: Q1 2016",
		"author": [
			{
				"family": "Fu",
				"given": "Ran"
			},
			{
				"family": "Chung",
				"given": "Donald"
			},
			{
				"family": "Lowder",
				"given": "Travis"
			},
			{
				"family": "Feldman",
				"given": "David"
			},
			{
				"family": "Ardani",
				"given": "Kristen"
			},
			{
				"family": "Margolis",
				"given": "Robert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "zotero-1118",
		"type": "webpage",
		"abstract": "Tips to save money and energy in the laundry room and reduce the wear and tear on your clothes.",
		"container-title": "Energy.gov",
		"language": "en",
		"title": "16 Ways to Save Money in the Laundry Room",
		"URL": "https://www.energy.gov/energysaver/articles/16-ways-save-money-laundry-room",
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					4
				]
			]
		}
	},
	{
		"id": "bansal2011",
		"type": "article-journal",
		"abstract": "An overview of options and potential barriers and risks for reducing the energy consumption, peak demand, and emissions for seven key energy consuming residential products (refrigerator-freezers, dishwashers, clothes washers, clothes dryers, electric ovens, gas ovens and microwave ovens) is presented. The paper primarily concentrates on the potential energy savings from the use of advanced technologies in appliances for the U.S. market. The significance and usefulness of each technology was evaluated in order to prioritize the R&D needs to improve energy efficiency of appliances in view of energy savings, cost, and complexity. The paper provides a snapshot of the future R&D needs for each of the technologies along with the associated barriers. Although significant energy savings may be achieved, one of the major barriers in most cases is high first cost. One way of addressing this issue and promoting the introduction of new technologies is to “level” the playing field for all manufacturers by establishing Minimum Energy Performance Standards (MEPS) which are not cost prohibitive and promoting energy efficient products through incentives to both manufacturers and consumers.",
		"collection-title": "SET 2010 Special Issue",
		"container-title": "Applied Thermal Engineering",
		"DOI": "10.1016/j.applthermaleng.2011.07.023",
		"ISSN": "1359-4311",
		"issue": "17",
		"journalAbbreviation": "Applied Thermal Engineering",
		"language": "en",
		"page": "3748-3760",
		"source": "ScienceDirect",
		"title": "Advances in household appliances- A review",
		"URL": "https://www.sciencedirect.com/science/article/pii/S1359431111003826",
		"volume": "31",
		"author": [
			{
				"family": "Bansal",
				"given": "Pradeep"
			},
			{
				"family": "Vineyard",
				"given": "Edward"
			},
			{
				"family": "Abdelaziz",
				"given": "Omar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					4
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011",
					12,
					1
				]
			]
		}
	},
	{
		"id": "paulfreiberger1999",
		"type": "book",
		"ISBN": "978-0-07-135892-7",
		"language": "eng",
		"number-of-pages": "566",
		"publisher": "McGraw-Hill",
		"source": "Internet Archive",
		"title": "Fire in the valley",
		"URL": "http://archive.org/details/fireinvalleymaki00frei_0",
		"author": [
			{
				"literal": "Paul Freiberger"
			}
		],
		"contributor": [
			{
				"literal": "Internet Archive"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1999"
				]
			]
		}
	},
	{
		"id": "isaacson2013",
		"type": "chapter",
		"abstract": "Walter Isaacson on \"complete rejectionist\" Bill Gates at Harvard College—and the birth of personal-computer software",
		"container-title": "Harvard Magazine",
		"language": "en",
		"title": "Bill Gates, Inside the Gates",
		"URL": "https://www.harvardmagazine.com/2013/09/walter-isaacson-on-bill-gates-at-harvard",
		"author": [
			{
				"family": "Isaacson",
				"given": "Walter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013",
					9,
					20
				]
			]
		}
	},
	{
		"id": "violaris2019",
		"type": "article-magazine",
		"title": "Einstein at the Patent Office - The Oxford Scientist",
		"URL": "https://oxsci.org/einstein-at-the-patent-office/",
		"author": [
			{
				"family": "Violaris",
				"given": "Maria"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					9
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "pokapuakorangaputaia2011",
		"type": "webpage",
		"container-title": "Science Learning Hub",
		"title": "Mendel’s experiments",
		"URL": "https://www.sciencelearn.org.nz/resources/1999-mendel-s-experiments",
		"author": [
			{
				"literal": "Pokapū Akoranga Pūtaia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					9
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		}
	},
	{
		"id": "boyes1962",
		"type": "article-journal",
		"container-title": "AIBS Bulletin",
		"DOI": "10.2307/1293124",
		"ISSN": "0096-7645",
		"issue": "3",
		"note": "publisher: [Oxford University Press, American Institute of Biological Sciences]",
		"page": "31-34",
		"source": "JSTOR",
		"title": "A Visit to Mendel's Monastery",
		"URL": "https://www.jstor.org/stable/1293124",
		"volume": "12",
		"author": [
			{
				"family": "Boyes",
				"given": "J. W."
			},
			{
				"family": "Boyes",
				"given": "B. C."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					9
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1962"
				]
			]
		}
	},
	{
		"id": "mcgrath2010",
		"type": "article-newspaper",
		"abstract": "Mr. Salinger, the author of “The Catcher in the Rye,” turned his back on success and adulation.",
		"container-title": "The New York Times",
		"ISSN": "0362-4331",
		"language": "en-US",
		"section": "Books",
		"source": "NYTimes.com",
		"title": "J. D. Salinger, Literary Recluse, Dies at 91",
		"URL": "https://www.nytimes.com/2010/01/29/books/29salinger.html",
		"author": [
			{
				"family": "McGrath",
				"given": "Charles"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					9
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2010",
					1,
					28
				]
			]
		}
	},
	{
		"id": "abe2005a",
		"type": "article-journal",
		"abstract": "This study examined whether maternal ratings of the Five-Factor Model (FFM) obtained when children were 3.5 years would show theoretically coherent patterns of relations with a variety of behavioral referents in the laboratory at 5 years as well as with maternal and self-ratings of psychological functioning in adolescence. As expected, Agreeableness and Conscientiousness were associated with measures of self-regulation at both ages as well as with an internal locus of control, but only Conscientiousness was associated with high academic performance. By contrast, Neuroticism was associated with measures of anxiety and Extraversion was associated with difficulty inhibiting behaviors at both ages. Openness to Experience was associated with sophisticated play behavior at 5 years and self-confidence in adolescence. Overall, this study yielded strong support for the predictive validity of the FFM with preschool age children and provided further evidence that there are striking continuities in personality from early childhood to adolescence.",
		"container-title": "Journal of Research in Personality",
		"DOI": "10.1016/j.jrp.2004.05.002",
		"ISSN": "0092-6566",
		"issue": "4",
		"journalAbbreviation": "Journal of Research in Personality",
		"language": "en",
		"page": "423-442",
		"source": "ScienceDirect",
		"title": "The predictive validity of the Five-Factor Model of personality with preschool age children: A nine year follow-up study",
		"title-short": "The predictive validity of the Five-Factor Model of personality with preschool age children",
		"URL": "https://www.sciencedirect.com/science/article/pii/S009265660400039X",
		"volume": "39",
		"author": [
			{
				"family": "Abe",
				"given": "Jo Ann A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2005",
					8,
					1
				]
			]
		}
	},
	{
		"id": "costa1986",
		"type": "article-journal",
		"abstract": "Over the past decade, a series of longitudinal studies have demonstrated that personality traits are stable in adulthood: There are no age-related shifts in mean levels, and individuals maintain very similar rank ordering on traits after intervals of up to 30 years. These findings should be of interest to clinicians because they point to important similarities between normal personality and personality disorders, facilitate research on the psychological processes that maintain both adaptive and maladaptive traits, serve as a reminder that current problems in functioning may be the expression of enduring personality patterns, and foster more realistic expectations about how much therapeutic change is possible.",
		"collection-title": "Special Issue Personality Assessment in the 80's: Issues and Advances",
		"container-title": "Clinical Psychology Review",
		"DOI": "10.1016/0272-7358(86)90029-2",
		"ISSN": "0272-7358",
		"issue": "5",
		"journalAbbreviation": "Clinical Psychology Review",
		"language": "en",
		"page": "407-423",
		"source": "ScienceDirect",
		"title": "Personality stability and its implications for clinical psychology",
		"URL": "https://www.sciencedirect.com/science/article/pii/0272735886900292",
		"volume": "6",
		"author": [
			{
				"family": "Costa",
				"given": "Paul T."
			},
			{
				"family": "McCrae",
				"given": "Robert R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1986",
					1,
					1
				]
			]
		}
	},
	{
		"id": "taleb2020",
		"type": "post-weblog",
		"abstract": "Background : “IQ” is a stale test meant to measure mental capacity but in fact mostly measures extreme unintelligence (learning…",
		"container-title": "INCERTO",
		"language": "en",
		"title": "IQ is largely a pseudoscientific swindle",
		"URL": "https://medium.com/incerto/iq-is-largely-a-pseudoscientific-swindle-f131c101ba39",
		"author": [
			{
				"family": "Taleb",
				"given": "Nassim Nicholas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					6,
					24
				]
			]
		}
	},
	{
		"id": "jecker1969",
		"type": "article-journal",
		"container-title": "Human Relations",
		"DOI": "10.1177/001872676902200407",
		"ISSN": "0018-7267",
		"issue": "4",
		"journalAbbreviation": "Human Relations",
		"language": "en",
		"note": "publisher: SAGE Publications Ltd",
		"page": "371-378",
		"source": "SAGE Journals",
		"title": "Liking a Person as a Function of Doing Him a Favour",
		"URL": "https://doi.org/10.1177/001872676902200407",
		"volume": "22",
		"author": [
			{
				"family": "Jecker",
				"given": "Jon"
			},
			{
				"family": "Landy",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					13
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1969",
					8,
					1
				]
			]
		}
	},
	{
		"id": "schopler1971",
		"type": "article-journal",
		"abstract": "Tested the hypothesis that being kind to someone, in comparison to being harsh, would lead to the self-perception of greater liking for the target of kindness. 24 male undergraduates, serving in the role of an e, administered a learning task to 2 accomplices. Through random designation, each s acted kindly toward 1 accomplice and harshly toward the other. Ss then rated their attraction for each accomplice. The hypothesis was confirmed. Additional measures suggest that the effect was not mediated by distorting actual task success in line with evaluative direction of the behavior or by the ease of enacting the kind acts. Order of presentation showed no significant effects, but one of the accomplices was significantly better liked than the other. (PsycInfo Database Record (c) 2020 APA, all rights reserved)",
		"container-title": "Journal of Personality and Social Psychology",
		"DOI": "10.1037/h0031689",
		"ISSN": "1939-1315",
		"issue": "2",
		"note": "publisher-place: US\npublisher: American Psychological Association",
		"page": "155-159",
		"source": "APA PsycNet",
		"title": "Effects of being kind or harsh to another on liking",
		"volume": "20",
		"author": [
			{
				"family": "Schopler",
				"given": "John"
			},
			{
				"family": "Compere",
				"given": "John S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"1971"
				]
			]
		}
	},
	{
		"id": "axelrod1981",
		"type": "article-journal",
		"container-title": "Science",
		"issue": "4489",
		"page": "1390-1396",
		"title": "The Evolution of Cooperation",
		"URL": "https://www.science.org/doi/abs/10.1126/science.7466396",
		"volume": "211",
		"author": [
			{
				"family": "Axelrod",
				"given": "Robert"
			},
			{
				"family": "Hamilton",
				"given": "William D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1981"
				]
			]
		}
	},
	{
		"id": "campbell1979",
		"type": "article-journal",
		"abstract": "It is a special characteristic of all modern societies that we consciously decide on and plan projects designed to improve our social systems. It is our universal predicament that our projects do not always have their intended effects. Very probably we all share in the experience that often we cannot tell whether the project had any impact at all, so complex is the flux of historical changes that would have been going on anyway, and so many are the other projects that might be expected to modify the same indicators. It seems inevitable that in most countries this common set of problems, combined with the obvious relevance of social science research procedures, will have generated a methodology and methodological specialists focused on the problem of assessing the impact of planned social change. It is an assumption if this paper that, in spite of differences in the forms of government and approaches to social planning and problem-solving, much of this methodology can be usefully shared — that social project evaluation methodology is one of the fields of science that has enough universality to make scientific sharing mutually beneficial. As a part of this sharing, this paper reports on program impact assessment methodology as it is developing in the United States today.",
		"container-title": "Evaluation and Program Planning",
		"DOI": "10.1016/0149-7189(79)90048-X",
		"ISSN": "0149-7189",
		"issue": "1",
		"journalAbbreviation": "Evaluation and Program Planning",
		"language": "en",
		"page": "67-90",
		"source": "ScienceDirect",
		"title": "Assessing the impact of planned social change",
		"URL": "https://www.sciencedirect.com/science/article/pii/014971897990048X",
		"volume": "2",
		"author": [
			{
				"family": "Campbell",
				"given": "Donald T."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					11,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1979",
					1,
					1
				]
			]
		}
	},
	{
		"id": "goodhart1984",
		"type": "chapter",
		"abstract": "In 1971 the monetary authorities1 in the UK adopted a new approach to monetary management, a change of policy announced and described in several papers on competition and credit control. The subsequent experience of trying to operate this revised system has, however, been troublesome and at times unhappy. The purpose here is to examine certain aspects of recent monetary developments in order to illustrate a number of more general analytical themes which may have relevance among several countries.",
		"container-title": "Monetary Theory and Practice: The UK Experience",
		"event-place": "London",
		"ISBN": "978-1-349-17295-5",
		"language": "en",
		"note": "DOI: 10.1007/978-1-349-17295-5_4",
		"page": "91-121",
		"publisher": "Macmillan Education UK",
		"publisher-place": "London",
		"source": "Springer Link",
		"title": "Problems of Monetary Management: The UK Experience",
		"title-short": "Problems of Monetary Management",
		"URL": "https://doi.org/10.1007/978-1-349-17295-5_4",
		"author": [
			{
				"family": "Goodhart",
				"given": "C. A. E."
			}
		],
		"editor": [
			{
				"family": "Goodhart",
				"given": "C. A. E."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					11,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1984"
				]
			]
		}
	},
	{
		"id": "wallace-wells2019",
		"type": "book",
		"abstract": "\"It is worse, much worse, than you think. If your anxiety about global warming is dominated by fears of sea-level rise, you are barely scratching the surface of what terrors are possible. In California, wildfires now rage year-round, destroying thousands of homes. Across the US, \"500-year\" storms pummel communities month after month, and floods displace tens of millions annually. This is only a preview of the changes to come. And they are coming fast. Without a revolution in how billions of humans conduct their lives, parts of the Earth could become close to uninhabitable, and other parts horrifically inhospitable, as soon as the end of this century. In his travelogue of our near future, David Wallace-Wells brings into stark relief the climate troubles that await--food shortages, refugee emergencies, and other crises that will reshape the globe. But the world will be remade by warming in more profound ways as well, transforming our politics, our culture, our relationship to technology, and our sense of history. It will be all-encompassing, shaping and distorting nearly every aspect of human life as it is lived today. Like An Inconvenient Truth and Silent Spring before it, The Uninhabitable Earth is both a meditation on the devastation we have brought upon ourselves and an impassioned call to action. For just as the world was brought to the brink of catastrophe within the span of a lifetime, the responsibility to avoid it now belongs to a single generation\"--",
		"call-number": "GF75 .W36 2019",
		"edition": "First edition",
		"event-place": "New York",
		"ISBN": "978-0-525-57670-9",
		"number-of-pages": "310",
		"publisher": "Tim Duggan Books",
		"publisher-place": "New York",
		"source": "Library of Congress ISBN",
		"title": "The uninhabitable earth: life after warming",
		"title-short": "The uninhabitable earth",
		"author": [
			{
				"family": "Wallace-Wells",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "oreopoulos2013",
		"type": "report",
		"abstract": "Recent stories of soaring student debt levels and under-placed college graduates have caused some to question whether a college education is still a sound investment. In this paper, we review the literature on the returns to higher education in an attempt to determine who benefits from college. Despite the tremendous heterogeneity across potential college students, we conclude that the investment appears to payoff for both the average and marginal student. During the past three decades in particular, the earnings premium associated with a college education has risen substantially. Beyond the pecuniary benefits of higher education, we suggest that there also may exist non-pecuniary benefits. Given these findings, it is perhaps surprising that among recent cohorts college completion rates have stagnated. We discuss potential explanations for this trend and conclude by succinctly interpreting the evidence on how to make the most out of college.",
		"genre": "Working Paper",
		"note": "collection-title: Working Paper Series\nDOI: 10.3386/w19053",
		"number": "19053",
		"publisher": "National Bureau of Economic Research",
		"source": "National Bureau of Economic Research",
		"title": "Making College Worth It: A Review of Research on the Returns to Higher Education",
		"title-short": "Making College Worth It",
		"URL": "https://www.nber.org/papers/w19053",
		"author": [
			{
				"family": "Oreopoulos",
				"given": "Philip"
			},
			{
				"family": "Petronijevic",
				"given": "Uros"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013",
					5
				]
			]
		}
	},
	{
		"id": "borchardt1958",
		"type": "article-journal",
		"container-title": "The Australian Library Journal",
		"DOI": "10.1080/00049670.1958.10756023",
		"ISSN": "0004-9670, 2201-4276",
		"issue": "4",
		"journalAbbreviation": "The Australian Library Journal",
		"language": "en",
		"page": "123-127",
		"source": "DOI.org (Crossref)",
		"title": "The Bibliographic Classification of Henry Bliss: An Interpretation",
		"title-short": "The Bibliographic Classification of Henry Bliss",
		"URL": "http://www.tandfonline.com/doi/abs/10.1080/00049670.1958.10756023",
		"volume": "7",
		"author": [
			{
				"family": "Borchardt",
				"given": "D. H."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					12,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1958",
					1
				]
			]
		}
	},
	{
		"id": "chen2021",
		"type": "book",
		"abstract": "\"A startup executive and investor draws on expertise developed at the premier venture capital firm, Andreessen Horowitz, and as an executive at Uber to address how tech's most successful products have solved the dreaded \"cold start problem\"-by leveraging networks effects to launch and scale towards billions of users\"--",
		"call-number": "HD62.5 .C447 2021",
		"edition": "First edition",
		"event-place": "New York, NY",
		"ISBN": "978-0-06-296974-3",
		"publisher": "Harper Business, an imprint of HarperCollinsPublishers",
		"publisher-place": "New York, NY",
		"source": "Library of Congress ISBN",
		"title": "The cold start problem: how to start and scale network effects",
		"title-short": "The cold start problem",
		"author": [
			{
				"family": "Chen",
				"given": "Andrew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "azoulay2019does",
		"type": "article-journal",
		"container-title": "American Economic Review",
		"issue": "8",
		"page": "2889–2920",
		"title": "Does science advance one funeral at a time?",
		"volume": "109",
		"author": [
			{
				"family": "Azoulay",
				"given": "Pierre"
			},
			{
				"family": "Fons-Rosen",
				"given": "Christian"
			},
			{
				"family": "Graff Zivin",
				"given": "Joshua S"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "legg2007",
		"type": "article-journal",
		"DOI": "10.48550/ARXIV.0712.3329",
		"note": "publisher: arXiv\ntex.copyright: Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004",
		"title": "Universal intelligence: A definition of machine intelligence",
		"URL": "https://arxiv.org/abs/0712.3329",
		"author": [
			{
				"family": "Legg",
				"given": "Shane"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2007"
				]
			]
		}
	},
	{
		"id": "zotero-1207",
		"type": "webpage",
		"title": "Faulty Reward Functions in the Wild",
		"URL": "https://openai.com/blog/faulty-reward-functions/",
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					15
				]
			]
		}
	},
	{
		"id": "zotero-1208",
		"type": "webpage",
		"title": "[2206.13353] Is Power-Seeking AI an Existential Risk?",
		"URL": "https://arxiv.org/abs/2206.13353",
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					18
				]
			]
		}
	},
	{
		"id": "griffiths2020",
		"type": "article",
		"abstract": "Recent progress in artificial intelligence provides the opportunity to ask the question of what is unique about human intelligence, but with a new comparison class. I argue that we can understand human intelligence, and the ways in which it may differ from artificial intelligence, by considering the characteristics of the kind of computational problems that human minds have to solve. I claim that these problems acquire their structure from three fundamental limitations that apply to human beings: limited time, limited computation, and limited communication. From these limitations we can derive many of the properties we associate with human intelligence, such as rapid learning, the ability to break down problems into parts, and the capacity for cumulative cultural evolution.",
		"DOI": "10.48550/arXiv.2009.14050",
		"note": "arXiv:2009.14050 [cs]",
		"number": "arXiv:2009.14050",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Understanding Human Intelligence through Human Limitations",
		"URL": "http://arxiv.org/abs/2009.14050",
		"author": [
			{
				"family": "Griffiths",
				"given": "Thomas L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					9,
					29
				]
			]
		}
	},
	{
		"id": "lohn2022",
		"type": "report",
		"abstract": "Between 2012 and 2018, the amount of computing power used by record-breaking artificial intelligence models doubled every 3.4 months. Even with money pouring into the AI field, this trendline is unsustainable. Because of cost, hardware availability and engineering difficulties, the next decade of AI can't rely exclusively on applying more and more computing power to drive further progress.",
		"language": "en-US",
		"title": "AI and Compute",
		"URL": "https://cset.georgetown.edu/publication/ai-and-compute/",
		"author": [
			{
				"family": "Lohn",
				"given": "Andrew"
			},
			{
				"family": "Musser",
				"given": "Micah"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					1
				]
			]
		}
	},
	{
		"id": "ganguli2022",
		"type": "paper-conference",
		"abstract": "Large-scale pre-training has recently emerged as a technique for creating capable, general-purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have a paradoxical combination of predictable loss on a broad training distribution (as embodied in their ”scaling laws”), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend for this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, funders who want to support work addressing these challenges, and academics who want to analyze, critique, and potentially develop large generative models.",
		"collection-title": "FAccT '22",
		"container-title": "2022 ACM Conference on Fairness, Accountability, and Transparency",
		"DOI": "10.1145/3531146.3533229",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"page": "1747–1764",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Predictability and Surprise in Large Generative Models",
		"URL": "https://doi.org/10.1145/3531146.3533229",
		"author": [
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Dassarma",
				"given": "Nova"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "El Showk",
				"given": "Sheer"
			},
			{
				"family": "Fort",
				"given": "Stanislav"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Kernian",
				"given": "Jackson"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Amodei",
				"given": "Daniela"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Clark",
				"given": "Jack"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					21
				]
			]
		}
	},
	{
		"id": "power2022",
		"type": "article",
		"abstract": "In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of \"grokking\" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.",
		"DOI": "10.48550/arXiv.2201.02177",
		"note": "arXiv:2201.02177 [cs]",
		"number": "arXiv:2201.02177",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
		"title-short": "Grokking",
		"URL": "http://arxiv.org/abs/2201.02177",
		"author": [
			{
				"family": "Power",
				"given": "Alethea"
			},
			{
				"family": "Burda",
				"given": "Yuri"
			},
			{
				"family": "Edwards",
				"given": "Harri"
			},
			{
				"family": "Babuschkin",
				"given": "Igor"
			},
			{
				"family": "Misra",
				"given": "Vedant"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					1,
					6
				]
			]
		}
	},
	{
		"id": "ziegler2022",
		"type": "article",
		"abstract": "In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance. In this work, we used a language generation task as a testbed for achieving high reliability through adversarial training. We created a series of adversarial training techniques -- including a tool that assists human adversaries -- to find and eliminate failures in a classifier that filters text completions suggested by a generator. In our simple \"avoid injuries\" task, we determined that we can set very conservative classifier thresholds without significantly impacting the quality of the filtered outputs. With our chosen thresholds, filtering with our baseline classifier decreases the rate of unsafe completions from about 2.4% to 0.003% on in-distribution data, which is near the limit of our ability to measure. We found that adversarial training significantly increased robustness to the adversarial attacks that we trained on, without affecting in-distribution performance. We hope to see further work in the high-stakes reliability setting, including more powerful tools for enhancing human adversaries and better ways to measure high levels of reliability, until we can confidently rule out the possibility of catastrophic deployment-time failures of powerful models.",
		"DOI": "10.48550/arXiv.2205.01663",
		"note": "arXiv:2205.01663 [cs]",
		"number": "arXiv:2205.01663",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adversarial Training for High-Stakes Reliability",
		"URL": "http://arxiv.org/abs/2205.01663",
		"author": [
			{
				"family": "Ziegler",
				"given": "Daniel M."
			},
			{
				"family": "Nix",
				"given": "Seraphina"
			},
			{
				"family": "Chan",
				"given": "Lawrence"
			},
			{
				"family": "Bauman",
				"given": "Tim"
			},
			{
				"family": "Schmidt-Nielsen",
				"given": "Peter"
			},
			{
				"family": "Lin",
				"given": "Tao"
			},
			{
				"family": "Scherlis",
				"given": "Adam"
			},
			{
				"family": "Nabeshima",
				"given": "Noa"
			},
			{
				"family": "Weinstein-Raun",
				"given": "Ben"
			},
			{
				"family": "Haas",
				"given": "Daniel",
				"non-dropping-particle": "de"
			},
			{
				"family": "Shlegeris",
				"given": "Buck"
			},
			{
				"family": "Thomas",
				"given": "Nate"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					15
				]
			]
		}
	},
	{
		"id": "wei2022",
		"type": "article",
		"abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
		"note": "arXiv:2206.07682 [cs]",
		"number": "arXiv:2206.07682",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Emergent Abilities of Large Language Models",
		"URL": "http://arxiv.org/abs/2206.07682",
		"author": [
			{
				"family": "Wei",
				"given": "Jason"
			},
			{
				"family": "Tay",
				"given": "Yi"
			},
			{
				"family": "Bommasani",
				"given": "Rishi"
			},
			{
				"family": "Raffel",
				"given": "Colin"
			},
			{
				"family": "Zoph",
				"given": "Barret"
			},
			{
				"family": "Borgeaud",
				"given": "Sebastian"
			},
			{
				"family": "Yogatama",
				"given": "Dani"
			},
			{
				"family": "Bosma",
				"given": "Maarten"
			},
			{
				"family": "Zhou",
				"given": "Denny"
			},
			{
				"family": "Metzler",
				"given": "Donald"
			},
			{
				"family": "Chi",
				"given": "Ed H."
			},
			{
				"family": "Hashimoto",
				"given": "Tatsunori"
			},
			{
				"family": "Vinyals",
				"given": "Oriol"
			},
			{
				"family": "Liang",
				"given": "Percy"
			},
			{
				"family": "Dean",
				"given": "Jeff"
			},
			{
				"family": "Fedus",
				"given": "William"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					15
				]
			]
		}
	},
	{
		"id": "phuong2022",
		"type": "article",
		"abstract": "This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.",
		"note": "arXiv:2207.09238 [cs]",
		"number": "arXiv:2207.09238",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Formal Algorithms for Transformers",
		"URL": "http://arxiv.org/abs/2207.09238",
		"author": [
			{
				"family": "Phuong",
				"given": "Mary"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					19
				]
			]
		}
	},
	{
		"id": "brown2020b",
		"type": "article",
		"abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
		"DOI": "10.48550/arXiv.2005.14165",
		"note": "arXiv:2005.14165 [cs]",
		"number": "arXiv:2005.14165",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Language Models are Few-Shot Learners",
		"URL": "http://arxiv.org/abs/2005.14165",
		"author": [
			{
				"family": "Brown",
				"given": "Tom B."
			},
			{
				"family": "Mann",
				"given": "Benjamin"
			},
			{
				"family": "Ryder",
				"given": "Nick"
			},
			{
				"family": "Subbiah",
				"given": "Melanie"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Neelakantan",
				"given": "Arvind"
			},
			{
				"family": "Shyam",
				"given": "Pranav"
			},
			{
				"family": "Sastry",
				"given": "Girish"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Agarwal",
				"given": "Sandhini"
			},
			{
				"family": "Herbert-Voss",
				"given": "Ariel"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Ramesh",
				"given": "Aditya"
			},
			{
				"family": "Ziegler",
				"given": "Daniel M."
			},
			{
				"family": "Wu",
				"given": "Jeffrey"
			},
			{
				"family": "Winter",
				"given": "Clemens"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Chen",
				"given": "Mark"
			},
			{
				"family": "Sigler",
				"given": "Eric"
			},
			{
				"family": "Litwin",
				"given": "Mateusz"
			},
			{
				"family": "Gray",
				"given": "Scott"
			},
			{
				"family": "Chess",
				"given": "Benjamin"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Berner",
				"given": "Christopher"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					22
				]
			]
		}
	},
	{
		"id": "henighan2020",
		"type": "article",
		"abstract": "We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question \"Is a picture worth a thousand words?\"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",
		"DOI": "10.48550/arXiv.2010.14701",
		"note": "arXiv:2010.14701 [cs]",
		"number": "arXiv:2010.14701",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Scaling Laws for Autoregressive Generative Modeling",
		"URL": "http://arxiv.org/abs/2010.14701",
		"author": [
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Katz",
				"given": "Mor"
			},
			{
				"family": "Chen",
				"given": "Mark"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Jackson",
				"given": "Jacob"
			},
			{
				"family": "Jun",
				"given": "Heewoo"
			},
			{
				"family": "Brown",
				"given": "Tom B."
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Gray",
				"given": "Scott"
			},
			{
				"family": "Hallacy",
				"given": "Chris"
			},
			{
				"family": "Mann",
				"given": "Benjamin"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Ramesh",
				"given": "Aditya"
			},
			{
				"family": "Ryder",
				"given": "Nick"
			},
			{
				"family": "Ziegler",
				"given": "Daniel M."
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					11,
					5
				]
			]
		}
	},
	{
		"id": "winata2021",
		"type": "article-journal",
		"abstract": "It is shown that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones, and they are competitive compared to the existing state-of-the-art cross-lingual models and translation models. General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models and translation models.",
		"container-title": "MRL",
		"DOI": "10.18653/v1/2021.mrl-1.1",
		"source": "Semantic Scholar",
		"title": "Language Models are Few-shot Multilingual Learners",
		"author": [
			{
				"family": "Winata",
				"given": "Genta Indra"
			},
			{
				"family": "Madotto",
				"given": "Andrea"
			},
			{
				"family": "Lin",
				"given": "Zhaojiang"
			},
			{
				"family": "Liu",
				"given": "Rosanne"
			},
			{
				"family": "Yosinski",
				"given": "J."
			},
			{
				"family": "Fung",
				"given": "Pascale"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chowdhery2022",
		"type": "article",
		"abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
		"note": "arXiv:2204.02311 [cs]",
		"number": "arXiv:2204.02311",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "PaLM: Scaling Language Modeling with Pathways",
		"title-short": "PaLM",
		"URL": "http://arxiv.org/abs/2204.02311",
		"author": [
			{
				"family": "Chowdhery",
				"given": "Aakanksha"
			},
			{
				"family": "Narang",
				"given": "Sharan"
			},
			{
				"family": "Devlin",
				"given": "Jacob"
			},
			{
				"family": "Bosma",
				"given": "Maarten"
			},
			{
				"family": "Mishra",
				"given": "Gaurav"
			},
			{
				"family": "Roberts",
				"given": "Adam"
			},
			{
				"family": "Barham",
				"given": "Paul"
			},
			{
				"family": "Chung",
				"given": "Hyung Won"
			},
			{
				"family": "Sutton",
				"given": "Charles"
			},
			{
				"family": "Gehrmann",
				"given": "Sebastian"
			},
			{
				"family": "Schuh",
				"given": "Parker"
			},
			{
				"family": "Shi",
				"given": "Kensen"
			},
			{
				"family": "Tsvyashchenko",
				"given": "Sasha"
			},
			{
				"family": "Maynez",
				"given": "Joshua"
			},
			{
				"family": "Rao",
				"given": "Abhishek"
			},
			{
				"family": "Barnes",
				"given": "Parker"
			},
			{
				"family": "Tay",
				"given": "Yi"
			},
			{
				"family": "Shazeer",
				"given": "Noam"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			},
			{
				"family": "Reif",
				"given": "Emily"
			},
			{
				"family": "Du",
				"given": "Nan"
			},
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Pope",
				"given": "Reiner"
			},
			{
				"family": "Bradbury",
				"given": "James"
			},
			{
				"family": "Austin",
				"given": "Jacob"
			},
			{
				"family": "Isard",
				"given": "Michael"
			},
			{
				"family": "Gur-Ari",
				"given": "Guy"
			},
			{
				"family": "Yin",
				"given": "Pengcheng"
			},
			{
				"family": "Duke",
				"given": "Toju"
			},
			{
				"family": "Levskaya",
				"given": "Anselm"
			},
			{
				"family": "Ghemawat",
				"given": "Sanjay"
			},
			{
				"family": "Dev",
				"given": "Sunipa"
			},
			{
				"family": "Michalewski",
				"given": "Henryk"
			},
			{
				"family": "Garcia",
				"given": "Xavier"
			},
			{
				"family": "Misra",
				"given": "Vedant"
			},
			{
				"family": "Robinson",
				"given": "Kevin"
			},
			{
				"family": "Fedus",
				"given": "Liam"
			},
			{
				"family": "Zhou",
				"given": "Denny"
			},
			{
				"family": "Ippolito",
				"given": "Daphne"
			},
			{
				"family": "Luan",
				"given": "David"
			},
			{
				"family": "Lim",
				"given": "Hyeontaek"
			},
			{
				"family": "Zoph",
				"given": "Barret"
			},
			{
				"family": "Spiridonov",
				"given": "Alexander"
			},
			{
				"family": "Sepassi",
				"given": "Ryan"
			},
			{
				"family": "Dohan",
				"given": "David"
			},
			{
				"family": "Agrawal",
				"given": "Shivani"
			},
			{
				"family": "Omernick",
				"given": "Mark"
			},
			{
				"family": "Dai",
				"given": "Andrew M."
			},
			{
				"family": "Pillai",
				"given": "Thanumalayan Sankaranarayana"
			},
			{
				"family": "Pellat",
				"given": "Marie"
			},
			{
				"family": "Lewkowycz",
				"given": "Aitor"
			},
			{
				"family": "Moreira",
				"given": "Erica"
			},
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Polozov",
				"given": "Oleksandr"
			},
			{
				"family": "Lee",
				"given": "Katherine"
			},
			{
				"family": "Zhou",
				"given": "Zongwei"
			},
			{
				"family": "Wang",
				"given": "Xuezhi"
			},
			{
				"family": "Saeta",
				"given": "Brennan"
			},
			{
				"family": "Diaz",
				"given": "Mark"
			},
			{
				"family": "Firat",
				"given": "Orhan"
			},
			{
				"family": "Catasta",
				"given": "Michele"
			},
			{
				"family": "Wei",
				"given": "Jason"
			},
			{
				"family": "Meier-Hellstern",
				"given": "Kathy"
			},
			{
				"family": "Eck",
				"given": "Douglas"
			},
			{
				"family": "Dean",
				"given": "Jeff"
			},
			{
				"family": "Petrov",
				"given": "Slav"
			},
			{
				"family": "Fiedel",
				"given": "Noah"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					4,
					19
				]
			]
		}
	},
	{
		"id": "zotero-1248",
		"type": "post-weblog",
		"abstract": "Posted by Sharan Narang and Aakanksha Chowdhery, Software Engineers, Google Research    In recent years, large neural networks trained for l...",
		"container-title": "Google AI Blog",
		"language": "en",
		"title": "Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance",
		"title-short": "Pathways Language Model (PaLM)",
		"URL": "http://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html",
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					25
				]
			]
		}
	},
	{
		"id": "hoffmann2022",
		"type": "article",
		"abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \\nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \\chinchilla, that uses the same compute budget as \\gopher but with 70B parameters and 4$\\times$ more more data. \\chinchilla uniformly and significantly outperforms \\Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \\chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \\chinchilla reaches a state-of-the-art average accuracy of 67.5\\% on the MMLU benchmark, greater than a 7\\% improvement over \\gopher.",
		"DOI": "10.48550/arXiv.2203.15556",
		"note": "arXiv:2203.15556 [cs]",
		"number": "arXiv:2203.15556",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Training Compute-Optimal Large Language Models",
		"URL": "http://arxiv.org/abs/2203.15556",
		"author": [
			{
				"family": "Hoffmann",
				"given": "Jordan"
			},
			{
				"family": "Borgeaud",
				"given": "Sebastian"
			},
			{
				"family": "Mensch",
				"given": "Arthur"
			},
			{
				"family": "Buchatskaya",
				"given": "Elena"
			},
			{
				"family": "Cai",
				"given": "Trevor"
			},
			{
				"family": "Rutherford",
				"given": "Eliza"
			},
			{
				"family": "Casas",
				"given": "Diego de Las"
			},
			{
				"family": "Hendricks",
				"given": "Lisa Anne"
			},
			{
				"family": "Welbl",
				"given": "Johannes"
			},
			{
				"family": "Clark",
				"given": "Aidan"
			},
			{
				"family": "Hennigan",
				"given": "Tom"
			},
			{
				"family": "Noland",
				"given": "Eric"
			},
			{
				"family": "Millican",
				"given": "Katie"
			},
			{
				"family": "Driessche",
				"given": "George",
				"dropping-particle": "van den"
			},
			{
				"family": "Damoc",
				"given": "Bogdan"
			},
			{
				"family": "Guy",
				"given": "Aurelia"
			},
			{
				"family": "Osindero",
				"given": "Simon"
			},
			{
				"family": "Simonyan",
				"given": "Karen"
			},
			{
				"family": "Elsen",
				"given": "Erich"
			},
			{
				"family": "Rae",
				"given": "Jack W."
			},
			{
				"family": "Vinyals",
				"given": "Oriol"
			},
			{
				"family": "Sifre",
				"given": "Laurent"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					29
				]
			]
		}
	},
	{
		"id": "zotero-1250",
		"type": "webpage",
		"abstract": "We investigate the optimal model and dataset size for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training \\nummodels language models ranging from 70 million to 10 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the training dataset size should be scaled equally: for every doubling of model size the training dataset size should also be doubled. We test this hypothesis by training a more compute-optimal model, \\Chinchilla, using the same compute budget as \\gopher but with 70B parameters and 4$\\times$ more data. \\chinchilla uniformly and significantly outperforms \\Gopher, GPT-3, Jurassic-1, and \\mtnlg on a large range of downstream evaluation tasks. As a highlight, \\chinchilla reaches an average accuracy of 67.5\\% on the MMLU benchmark, over a 7\\% improvement over \\gopher.",
		"language": "en",
		"title": "An empirical analysis of compute-optimal large language model training",
		"URL": "https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training",
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					25
				]
			]
		}
	},
	{
		"id": "tirumala2022",
		"type": "article",
		"abstract": "Despite their wide adoption, the underlying training and memorization dynamics of very large language models is not well understood. We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples. Together, these findings present another piece of the broader puzzle of trying to understand what actually improves as models get bigger.",
		"DOI": "10.48550/arXiv.2205.10770",
		"note": "arXiv:2205.10770 [cs]",
		"number": "arXiv:2205.10770",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
		"title-short": "Memorization Without Overfitting",
		"URL": "http://arxiv.org/abs/2205.10770",
		"author": [
			{
				"family": "Tirumala",
				"given": "Kushal"
			},
			{
				"family": "Markosyan",
				"given": "Aram H."
			},
			{
				"family": "Zettlemoyer",
				"given": "Luke"
			},
			{
				"family": "Aghajanyan",
				"given": "Armen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					22
				]
			]
		}
	},
	{
		"id": "hendrycks2022",
		"type": "article",
		"abstract": "Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (\"Robustness\"), identifying hazards (\"Monitoring\"), reducing inherent model hazards (\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout, we clarify each problem's motivation and provide concrete research directions.",
		"DOI": "10.48550/arXiv.2109.13916",
		"note": "arXiv:2109.13916 [cs]",
		"number": "arXiv:2109.13916",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Unsolved Problems in ML Safety",
		"URL": "http://arxiv.org/abs/2109.13916",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Carlini",
				"given": "Nicholas"
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					16
				]
			]
		}
	},
	{
		"id": "hendrycks2022a",
		"type": "article",
		"abstract": "Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk.",
		"DOI": "10.48550/arXiv.2206.05862",
		"note": "arXiv:2206.05862 [cs]",
		"number": "arXiv:2206.05862",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "X-Risk Analysis for AI Research",
		"URL": "http://arxiv.org/abs/2206.05862",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					20
				]
			]
		}
	},
	{
		"id": "dafoe2020",
		"type": "article",
		"abstract": "Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.",
		"note": "arXiv:2012.08630 [cs]",
		"number": "arXiv:2012.08630",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Open Problems in Cooperative AI",
		"URL": "http://arxiv.org/abs/2012.08630",
		"author": [
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Hughes",
				"given": "Edward"
			},
			{
				"family": "Bachrach",
				"given": "Yoram"
			},
			{
				"family": "Collins",
				"given": "Tantum"
			},
			{
				"family": "McKee",
				"given": "Kevin R."
			},
			{
				"family": "Leibo",
				"given": "Joel Z."
			},
			{
				"family": "Larson",
				"given": "Kate"
			},
			{
				"family": "Graepel",
				"given": "Thore"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12,
					15
				]
			]
		}
	},
	{
		"id": "roodman2020",
		"type": "post-weblog",
		"abstract": "In arriving at our funding priorities—including criminal justice reform, farm animal welfare, pandemic preparedness, health-related science, and artificial intelligence safety—Open Philanthropy has pondered profound questions. How much should we care about people who will live far in the future? Or about chickens today? What events cpould extinguish civilization? Could artificial intelligence (AI) surpass human intelligence? One",
		"container-title": "Open Philanthropy -",
		"language": "en-us",
		"title": "Modeling the Human Trajectory - Open Philanthropy",
		"URL": "https://www.openphilanthropy.org/research/modeling-the-human-trajectory/",
		"author": [
			{
				"family": "Roodman",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					6,
					15
				]
			]
		}
	},
	{
		"id": "hanson2001",
		"type": "article-journal",
		"abstract": "A world product time series covering two million years is well fit by either a sum of four exponentials, or a constant elasticity of substitution (CES) combination of three exponential growth modes: \"hunting,\" \"farming,\" and \"industry.\" The CES parameters suggest that farming substituted for hunting, while industry complemented farming, making the industrial revolution a smoother transition. Each mode grew world product by a factor of a few hundred, and grew a hundred times faster than its predecessor. This weakly suggests that within the next century a new mode might appear with a doubling time measured in days, not years. participants of the GMU economics Brown Bag talk series and the SEA 2000 Annual Meeting.",
		"source": "ResearchGate",
		"title": "Long-term growth as a sequence of exponential modes",
		"author": [
			{
				"family": "Hanson",
				"given": "Robin"
			},
			{
				"family": "Adams",
				"given": "Curt"
			},
			{
				"family": "Caplan",
				"given": "Bryan"
			},
			{
				"family": "Cohen",
				"given": "Joel"
			},
			{
				"family": "Congleton",
				"given": "Roger"
			},
			{
				"family": "Cowen",
				"given": "Tyler"
			},
			{
				"family": "Crain",
				"given": "Mark"
			},
			{
				"family": "Hallgrimsson",
				"given": "Eirikur"
			},
			{
				"family": "Feynman",
				"given": "Carl"
			},
			{
				"family": "Finney",
				"given": "Hal"
			},
			{
				"family": "Long",
				"given": "Brad"
			},
			{
				"family": "Meyer",
				"given": "Perrin"
			},
			{
				"family": "Ramirez",
				"given": "Carlos"
			},
			{
				"family": "Schroeter",
				"given": "John"
			},
			{
				"family": "Sornette",
				"given": "Didier"
			},
			{
				"family": "Varian",
				"given": "Hal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2001",
					1,
					1
				]
			]
		}
	},
	{
		"id": "rauker2022",
		"type": "article",
		"abstract": "The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are generally difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify failures, fix bugs, and improve basic understanding. In particular, \"inner\" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. Finally, we discuss key challenges and argue for future work emphasizing diagnostics, benchmarking, and robustness.",
		"DOI": "10.48550/arXiv.2207.13243",
		"note": "arXiv:2207.13243 [cs]",
		"number": "arXiv:2207.13243",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks",
		"title-short": "Toward Transparent AI",
		"URL": "http://arxiv.org/abs/2207.13243",
		"author": [
			{
				"family": "Räuker",
				"given": "Tilman"
			},
			{
				"family": "Ho",
				"given": "Anson"
			},
			{
				"family": "Casper",
				"given": "Stephen"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					5
				]
			]
		}
	},
	{
		"id": "valle-perez2019",
		"type": "article",
		"abstract": "Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime where classical learning theory would instead predict that they would severely overfit. While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit. In this paper, we provide a new explanation. By applying a very general probability-complexity bound recently derived from algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions. We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks applied to CIFAR10 and MNIST. As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems. This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than the more conventional prior over parameter space. If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets. By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood, we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks.",
		"DOI": "10.48550/arXiv.1805.08522",
		"note": "arXiv:1805.08522 [cs, stat]",
		"number": "arXiv:1805.08522",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Deep learning generalizes because the parameter-function map is biased towards simple functions",
		"URL": "http://arxiv.org/abs/1805.08522",
		"author": [
			{
				"family": "Valle-Pérez",
				"given": "Guillermo"
			},
			{
				"family": "Camargo",
				"given": "Chico Q."
			},
			{
				"family": "Louis",
				"given": "Ard A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					4,
					21
				]
			]
		}
	},
	{
		"id": "mingard2020",
		"type": "article",
		"abstract": "Understanding the inductive bias of neural networks is critical to explaining their ability to generalise. Here, for one of the simplest neural networks -- a single-layer perceptron with n input neurons, one output neuron, and no threshold bias term -- we prove that upon random initialisation of weights, the a priori probability $P(t)$ that it represents a Boolean function that classifies t points in ${0,1}^n$ as 1 has a remarkably simple form: $P(t) = 2^{-n}$ for $0\\leq t < 2^n$. Since a perceptron can express far fewer Boolean functions with small or large values of t (low entropy) than with intermediate values of t (high entropy) there is, on average, a strong intrinsic a-priori bias towards individual functions with low entropy. Furthermore, within a class of functions with fixed t, we often observe a further intrinsic bias towards functions of lower complexity. Finally, we prove that, regardless of the distribution of inputs, the bias towards low entropy becomes monotonically stronger upon adding ReLU layers, and empirically show that increasing the variance of the bias term has a similar effect.",
		"DOI": "10.48550/arXiv.1909.11522",
		"note": "arXiv:1909.11522 [cs, stat]",
		"number": "arXiv:1909.11522",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Neural networks are a priori biased towards Boolean functions with low entropy",
		"URL": "http://arxiv.org/abs/1909.11522",
		"author": [
			{
				"family": "Mingard",
				"given": "Chris"
			},
			{
				"family": "Skalse",
				"given": "Joar"
			},
			{
				"family": "Valle-Pérez",
				"given": "Guillermo"
			},
			{
				"family": "Martínez-Rubio",
				"given": "David"
			},
			{
				"family": "Mikulik",
				"given": "Vladimir"
			},
			{
				"family": "Louis",
				"given": "Ard A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					1,
					2
				]
			]
		}
	},
	{
		"id": "harzli2022",
		"type": "article",
		"abstract": "Double-descent curves in neural networks describe the phenomenon that the generalisation error initially descends with increasing parameters, then grows after reaching an optimal number of parameters which is less than the number of data points, but then descends again in the overparameterized regime. Here we use a neural network Gaussian process (NNGP) which maps exactly to a fully connected network (FCN) in the infinite-width limit, combined with techniques from random matrix theory, to calculate this generalisation behaviour. An advantage of our NNGP approach is that the analytical calculations are easier to interpret. We argue that the fact that the generalisation error of neural networks decreases in the overparameterized regime and has a finite theoretical value is explained by the convergence to their limiting Gaussian processes. Our analysis thus provides a mathematical explanation for a surprising phenomenon that could not explained by conventional statistical learning theory. However, understanding what drives these finite theoretical values to be the state-of-the-art generalisation performances in many applications remains an open question, for which we only provide new leads in this paper.",
		"DOI": "10.48550/arXiv.2102.07238",
		"note": "arXiv:2102.07238 [cs, stat]",
		"number": "arXiv:2102.07238",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Double-descent curves in neural networks: a new perspective using Gaussian processes",
		"title-short": "Double-descent curves in neural networks",
		"URL": "http://arxiv.org/abs/2102.07238",
		"author": [
			{
				"family": "Harzli",
				"given": "Ouns El"
			},
			{
				"family": "Valle-Pérez",
				"given": "Guillermo"
			},
			{
				"family": "Louis",
				"given": "Ard A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					9,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					2,
					20
				]
			]
		}
	},
	{
		"id": "zhang2021",
		"type": "article",
		"abstract": "The intuition that local flatness of the loss landscape is correlated with better generalization for deep neural networks (DNNs) has been explored for decades, spawning many different flatness measures. Recently, this link with generalization has been called into question by a demonstration that many measures of flatness are vulnerable to parameter re-scaling which arbitrarily changes their value without changing neural network outputs. Here we show that, in addition, some popular variants of SGD such as Adam and Entropy-SGD, can also break the flatness-generalization correlation. As an alternative to flatness measures, we use a function based picture and propose using the log of Bayesian prior upon initialization, $\\log P(f)$, as a predictor of the generalization when a DNN converges on function $f$ after training to zero error. The prior is directly proportional to the Bayesian posterior for functions that give zero error on a test set. For the case of image classification, we show that $\\log P(f)$ is a significantly more robust predictor of generalization than flatness measures are. Whilst local flatness measures fail under parameter re-scaling, the prior/posterior, which is global quantity, remains invariant under re-scaling. Moreover, the correlation with generalization as a function of data complexity remains good for different variants of SGD.",
		"DOI": "10.48550/arXiv.2103.06219",
		"note": "arXiv:2103.06219 [cs, stat]",
		"number": "arXiv:2103.06219",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Why flatness does and does not correlate with generalization for deep neural networks",
		"URL": "http://arxiv.org/abs/2103.06219",
		"author": [
			{
				"family": "Zhang",
				"given": "Shuofeng"
			},
			{
				"family": "Reid",
				"given": "Isaac"
			},
			{
				"family": "Pérez",
				"given": "Guillermo Valle"
			},
			{
				"family": "Louis",
				"given": "Ard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					21
				]
			]
		}
	},
	{
		"id": "baldassi2022",
		"type": "article-journal",
		"abstract": "Current deep neural networks are highly overparameterized (up to billions of connection weights) and nonlinear. Yet they can fit data almost perfectly through variants of gradient descent algorithms and achieve unexpected levels of prediction accuracy without overfitting. These are formidable results that defy predictions of statistical learning and pose conceptual challenges for non-convex optimization. In this paper, we use methods from statistical physics of disordered systems to analytically study the computational fallout of overparameterization in non-convex binary neural network models, trained on data generated from a structurally simpler but \"hidden\" network. As the number of connection weights increases, we follow the changes of the geometrical structure of different minima of the error loss function and relate them to learning and generalization performance. A first transition happens at the so-called interpolation point, when solutions begin to exist (perfect fitting becomes possible). This transition reflects the properties of typical solutions, which however are in sharp minima and hard to sample. After a gap, a second transition occurs, with the discontinuous appearance of a different kind of \"atypical\" structures: wide regions of the weight space that are particularly solution-dense and have good generalization properties. The two kinds of solutions coexist, with the typical ones being exponentially more numerous, but empirically we find that efficient algorithms sample the atypical, rare ones. This suggests that the atypical phase transition is the relevant one for learning. The results of numerical tests with realistic networks on observables suggested by the theory are consistent with this scenario.",
		"container-title": "Physical Review E",
		"DOI": "10.1103/PhysRevE.106.014116",
		"ISSN": "2470-0045, 2470-0053",
		"issue": "1",
		"journalAbbreviation": "Phys. Rev. E",
		"note": "arXiv:2110.00683 [cond-mat, stat]",
		"page": "014116",
		"source": "arXiv.org",
		"title": "Learning through atypical \"phase transitions\" in overparameterized neural networks",
		"URL": "http://arxiv.org/abs/2110.00683",
		"volume": "106",
		"author": [
			{
				"family": "Baldassi",
				"given": "Carlo"
			},
			{
				"family": "Lauditi",
				"given": "Clarissa"
			},
			{
				"family": "Malatesta",
				"given": "Enrico M."
			},
			{
				"family": "Pacelli",
				"given": "Rosalba"
			},
			{
				"family": "Perugini",
				"given": "Gabriele"
			},
			{
				"family": "Zecchina",
				"given": "Riccardo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					13
				]
			]
		}
	},
	{
		"id": "wei2022a",
		"type": "article",
		"abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
		"note": "arXiv:2206.07682 [cs]",
		"number": "arXiv:2206.07682",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Emergent Abilities of Large Language Models",
		"URL": "http://arxiv.org/abs/2206.07682",
		"author": [
			{
				"family": "Wei",
				"given": "Jason"
			},
			{
				"family": "Tay",
				"given": "Yi"
			},
			{
				"family": "Bommasani",
				"given": "Rishi"
			},
			{
				"family": "Raffel",
				"given": "Colin"
			},
			{
				"family": "Zoph",
				"given": "Barret"
			},
			{
				"family": "Borgeaud",
				"given": "Sebastian"
			},
			{
				"family": "Yogatama",
				"given": "Dani"
			},
			{
				"family": "Bosma",
				"given": "Maarten"
			},
			{
				"family": "Zhou",
				"given": "Denny"
			},
			{
				"family": "Metzler",
				"given": "Donald"
			},
			{
				"family": "Chi",
				"given": "Ed H."
			},
			{
				"family": "Hashimoto",
				"given": "Tatsunori"
			},
			{
				"family": "Vinyals",
				"given": "Oriol"
			},
			{
				"family": "Liang",
				"given": "Percy"
			},
			{
				"family": "Dean",
				"given": "Jeff"
			},
			{
				"family": "Fedus",
				"given": "William"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					15
				]
			]
		}
	},
	{
		"id": "vaswani2017a",
		"type": "article",
		"abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
		"DOI": "10.48550/arXiv.1706.03762",
		"note": "arXiv:1706.03762 [cs]",
		"number": "arXiv:1706.03762",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Attention Is All You Need",
		"URL": "http://arxiv.org/abs/1706.03762",
		"author": [
			{
				"family": "Vaswani",
				"given": "Ashish"
			},
			{
				"family": "Shazeer",
				"given": "Noam"
			},
			{
				"family": "Parmar",
				"given": "Niki"
			},
			{
				"family": "Uszkoreit",
				"given": "Jakob"
			},
			{
				"family": "Jones",
				"given": "Llion"
			},
			{
				"family": "Gomez",
				"given": "Aidan N."
			},
			{
				"family": "Kaiser",
				"given": "Lukasz"
			},
			{
				"family": "Polosukhin",
				"given": "Illia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					12,
					5
				]
			]
		}
	},
	{
		"id": "phuong2022a",
		"type": "article",
		"abstract": "This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.",
		"note": "arXiv:2207.09238 [cs]",
		"number": "arXiv:2207.09238",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Formal Algorithms for Transformers",
		"URL": "http://arxiv.org/abs/2207.09238",
		"author": [
			{
				"family": "Phuong",
				"given": "Mary"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					19
				]
			]
		}
	},
	{
		"id": "henighan2020a",
		"type": "article",
		"abstract": "We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question \"Is a picture worth a thousand words?\"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",
		"DOI": "10.48550/arXiv.2010.14701",
		"note": "arXiv:2010.14701 [cs]",
		"number": "arXiv:2010.14701",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Scaling Laws for Autoregressive Generative Modeling",
		"URL": "http://arxiv.org/abs/2010.14701",
		"author": [
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Katz",
				"given": "Mor"
			},
			{
				"family": "Chen",
				"given": "Mark"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Jackson",
				"given": "Jacob"
			},
			{
				"family": "Jun",
				"given": "Heewoo"
			},
			{
				"family": "Brown",
				"given": "Tom B."
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Gray",
				"given": "Scott"
			},
			{
				"family": "Hallacy",
				"given": "Chris"
			},
			{
				"family": "Mann",
				"given": "Benjamin"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Ramesh",
				"given": "Aditya"
			},
			{
				"family": "Ryder",
				"given": "Nick"
			},
			{
				"family": "Ziegler",
				"given": "Daniel M."
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					11,
					5
				]
			]
		}
	},
	{
		"id": "winata2021a",
		"type": "article-journal",
		"abstract": "It is shown that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones, and they are competitive compared to the existing state-of-the-art cross-lingual models and translation models. General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models and translation models.",
		"container-title": "MRL",
		"DOI": "10.18653/v1/2021.mrl-1.1",
		"source": "Semantic Scholar",
		"title": "Language Models are Few-shot Multilingual Learners",
		"author": [
			{
				"family": "Winata",
				"given": "Genta Indra"
			},
			{
				"family": "Madotto",
				"given": "Andrea"
			},
			{
				"family": "Lin",
				"given": "Zhaojiang"
			},
			{
				"family": "Liu",
				"given": "Rosanne"
			},
			{
				"family": "Yosinski",
				"given": "J."
			},
			{
				"family": "Fung",
				"given": "Pascale"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "zotero-1315",
		"type": "webpage",
		"title": "[PDF]  | Semantic Scholar",
		"URL": "https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe",
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		}
	},
	{
		"id": "brown2020c",
		"type": "article",
		"abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
		"DOI": "10.48550/arXiv.2005.14165",
		"note": "arXiv:2005.14165 [cs]",
		"number": "arXiv:2005.14165",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Language Models are Few-Shot Learners",
		"URL": "http://arxiv.org/abs/2005.14165",
		"author": [
			{
				"family": "Brown",
				"given": "Tom B."
			},
			{
				"family": "Mann",
				"given": "Benjamin"
			},
			{
				"family": "Ryder",
				"given": "Nick"
			},
			{
				"family": "Subbiah",
				"given": "Melanie"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Neelakantan",
				"given": "Arvind"
			},
			{
				"family": "Shyam",
				"given": "Pranav"
			},
			{
				"family": "Sastry",
				"given": "Girish"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Agarwal",
				"given": "Sandhini"
			},
			{
				"family": "Herbert-Voss",
				"given": "Ariel"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Ramesh",
				"given": "Aditya"
			},
			{
				"family": "Ziegler",
				"given": "Daniel M."
			},
			{
				"family": "Wu",
				"given": "Jeffrey"
			},
			{
				"family": "Winter",
				"given": "Clemens"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Chen",
				"given": "Mark"
			},
			{
				"family": "Sigler",
				"given": "Eric"
			},
			{
				"family": "Litwin",
				"given": "Mateusz"
			},
			{
				"family": "Gray",
				"given": "Scott"
			},
			{
				"family": "Chess",
				"given": "Benjamin"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Berner",
				"given": "Christopher"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					22
				]
			]
		}
	},
	{
		"id": "radford2019language",
		"type": "article-journal",
		"container-title": "OpenAI blog",
		"issue": "8",
		"page": "9",
		"title": "Language models are unsupervised multitask learners",
		"volume": "1",
		"author": [
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Wu",
				"given": "Jeffrey"
			},
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Luan",
				"given": "David"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"literal": "others"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "chowdhery2022a",
		"type": "article",
		"abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
		"note": "arXiv:2204.02311 [cs]",
		"number": "arXiv:2204.02311",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "PaLM: Scaling Language Modeling with Pathways",
		"title-short": "PaLM",
		"URL": "http://arxiv.org/abs/2204.02311",
		"author": [
			{
				"family": "Chowdhery",
				"given": "Aakanksha"
			},
			{
				"family": "Narang",
				"given": "Sharan"
			},
			{
				"family": "Devlin",
				"given": "Jacob"
			},
			{
				"family": "Bosma",
				"given": "Maarten"
			},
			{
				"family": "Mishra",
				"given": "Gaurav"
			},
			{
				"family": "Roberts",
				"given": "Adam"
			},
			{
				"family": "Barham",
				"given": "Paul"
			},
			{
				"family": "Chung",
				"given": "Hyung Won"
			},
			{
				"family": "Sutton",
				"given": "Charles"
			},
			{
				"family": "Gehrmann",
				"given": "Sebastian"
			},
			{
				"family": "Schuh",
				"given": "Parker"
			},
			{
				"family": "Shi",
				"given": "Kensen"
			},
			{
				"family": "Tsvyashchenko",
				"given": "Sasha"
			},
			{
				"family": "Maynez",
				"given": "Joshua"
			},
			{
				"family": "Rao",
				"given": "Abhishek"
			},
			{
				"family": "Barnes",
				"given": "Parker"
			},
			{
				"family": "Tay",
				"given": "Yi"
			},
			{
				"family": "Shazeer",
				"given": "Noam"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			},
			{
				"family": "Reif",
				"given": "Emily"
			},
			{
				"family": "Du",
				"given": "Nan"
			},
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Pope",
				"given": "Reiner"
			},
			{
				"family": "Bradbury",
				"given": "James"
			},
			{
				"family": "Austin",
				"given": "Jacob"
			},
			{
				"family": "Isard",
				"given": "Michael"
			},
			{
				"family": "Gur-Ari",
				"given": "Guy"
			},
			{
				"family": "Yin",
				"given": "Pengcheng"
			},
			{
				"family": "Duke",
				"given": "Toju"
			},
			{
				"family": "Levskaya",
				"given": "Anselm"
			},
			{
				"family": "Ghemawat",
				"given": "Sanjay"
			},
			{
				"family": "Dev",
				"given": "Sunipa"
			},
			{
				"family": "Michalewski",
				"given": "Henryk"
			},
			{
				"family": "Garcia",
				"given": "Xavier"
			},
			{
				"family": "Misra",
				"given": "Vedant"
			},
			{
				"family": "Robinson",
				"given": "Kevin"
			},
			{
				"family": "Fedus",
				"given": "Liam"
			},
			{
				"family": "Zhou",
				"given": "Denny"
			},
			{
				"family": "Ippolito",
				"given": "Daphne"
			},
			{
				"family": "Luan",
				"given": "David"
			},
			{
				"family": "Lim",
				"given": "Hyeontaek"
			},
			{
				"family": "Zoph",
				"given": "Barret"
			},
			{
				"family": "Spiridonov",
				"given": "Alexander"
			},
			{
				"family": "Sepassi",
				"given": "Ryan"
			},
			{
				"family": "Dohan",
				"given": "David"
			},
			{
				"family": "Agrawal",
				"given": "Shivani"
			},
			{
				"family": "Omernick",
				"given": "Mark"
			},
			{
				"family": "Dai",
				"given": "Andrew M."
			},
			{
				"family": "Pillai",
				"given": "Thanumalayan Sankaranarayana"
			},
			{
				"family": "Pellat",
				"given": "Marie"
			},
			{
				"family": "Lewkowycz",
				"given": "Aitor"
			},
			{
				"family": "Moreira",
				"given": "Erica"
			},
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Polozov",
				"given": "Oleksandr"
			},
			{
				"family": "Lee",
				"given": "Katherine"
			},
			{
				"family": "Zhou",
				"given": "Zongwei"
			},
			{
				"family": "Wang",
				"given": "Xuezhi"
			},
			{
				"family": "Saeta",
				"given": "Brennan"
			},
			{
				"family": "Diaz",
				"given": "Mark"
			},
			{
				"family": "Firat",
				"given": "Orhan"
			},
			{
				"family": "Catasta",
				"given": "Michele"
			},
			{
				"family": "Wei",
				"given": "Jason"
			},
			{
				"family": "Meier-Hellstern",
				"given": "Kathy"
			},
			{
				"family": "Eck",
				"given": "Douglas"
			},
			{
				"family": "Dean",
				"given": "Jeff"
			},
			{
				"family": "Petrov",
				"given": "Slav"
			},
			{
				"family": "Fiedel",
				"given": "Noah"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					29
				]
			]
		}
	},
	{
		"id": "hoffmann2022a",
		"type": "article",
		"abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \\nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \\chinchilla, that uses the same compute budget as \\gopher but with 70B parameters and 4$\\times$ more more data. \\chinchilla uniformly and significantly outperforms \\Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \\chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \\chinchilla reaches a state-of-the-art average accuracy of 67.5\\% on the MMLU benchmark, greater than a 7\\% improvement over \\gopher.",
		"DOI": "10.48550/arXiv.2203.15556",
		"note": "arXiv:2203.15556 [cs]",
		"number": "arXiv:2203.15556",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Training Compute-Optimal Large Language Models",
		"URL": "http://arxiv.org/abs/2203.15556",
		"author": [
			{
				"family": "Hoffmann",
				"given": "Jordan"
			},
			{
				"family": "Borgeaud",
				"given": "Sebastian"
			},
			{
				"family": "Mensch",
				"given": "Arthur"
			},
			{
				"family": "Buchatskaya",
				"given": "Elena"
			},
			{
				"family": "Cai",
				"given": "Trevor"
			},
			{
				"family": "Rutherford",
				"given": "Eliza"
			},
			{
				"family": "Casas",
				"given": "Diego de Las"
			},
			{
				"family": "Hendricks",
				"given": "Lisa Anne"
			},
			{
				"family": "Welbl",
				"given": "Johannes"
			},
			{
				"family": "Clark",
				"given": "Aidan"
			},
			{
				"family": "Hennigan",
				"given": "Tom"
			},
			{
				"family": "Noland",
				"given": "Eric"
			},
			{
				"family": "Millican",
				"given": "Katie"
			},
			{
				"family": "Driessche",
				"given": "George",
				"dropping-particle": "van den"
			},
			{
				"family": "Damoc",
				"given": "Bogdan"
			},
			{
				"family": "Guy",
				"given": "Aurelia"
			},
			{
				"family": "Osindero",
				"given": "Simon"
			},
			{
				"family": "Simonyan",
				"given": "Karen"
			},
			{
				"family": "Elsen",
				"given": "Erich"
			},
			{
				"family": "Rae",
				"given": "Jack W."
			},
			{
				"family": "Vinyals",
				"given": "Oriol"
			},
			{
				"family": "Sifre",
				"given": "Laurent"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					29
				]
			]
		}
	},
	{
		"id": "tirumala2022a",
		"type": "article",
		"abstract": "Despite their wide adoption, the underlying training and memorization dynamics of very large language models is not well understood. We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples. Together, these findings present another piece of the broader puzzle of trying to understand what actually improves as models get bigger.",
		"DOI": "10.48550/arXiv.2205.10770",
		"note": "arXiv:2205.10770 [cs]",
		"number": "arXiv:2205.10770",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
		"title-short": "Memorization Without Overfitting",
		"URL": "http://arxiv.org/abs/2205.10770",
		"author": [
			{
				"family": "Tirumala",
				"given": "Kushal"
			},
			{
				"family": "Markosyan",
				"given": "Aram H."
			},
			{
				"family": "Zettlemoyer",
				"given": "Luke"
			},
			{
				"family": "Aghajanyan",
				"given": "Armen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					22
				]
			]
		}
	},
	{
		"id": "denain2022",
		"type": "article",
		"abstract": "Transparency methods such as model visualizations provide information that outputs alone might miss, since they describe the internals of neural networks. But can we trust that model explanations reflect model behavior? For instance, can they diagnose abnormal behavior such as backdoors or shape bias? To evaluate model explanations, we define a model as anomalous if it differs from a reference set of normal models, and we test whether transparency methods assign different explanations to anomalous and normal models. We find that while existing methods can detect stark anomalies such as shape bias or adversarial training, they struggle to identify more subtle anomalies such as models trained on incomplete data. Moreover, they generally fail to distinguish the inputs that induce anomalous behavior, e.g. images containing a backdoor trigger. These results reveal new blind spots in existing model explanations, pointing to the need for further method development.",
		"DOI": "10.48550/arXiv.2206.13498",
		"note": "arXiv:2206.13498 [cs]",
		"number": "arXiv:2206.13498",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior",
		"title-short": "Auditing Visualizations",
		"URL": "http://arxiv.org/abs/2206.13498",
		"author": [
			{
				"family": "Denain",
				"given": "Jean-Stanislas"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					27
				]
			]
		}
	},
	{
		"id": "ding2021",
		"type": "article",
		"abstract": "To understand neural network behavior, recent works quantitatively compare different networks' learned representations using canonical correlation analysis (CCA), centered kernel alignment (CKA), and other dissimilarity measures. Unfortunately, these widely used measures often disagree on fundamental observations, such as whether deep networks differing only in random initialization learn similar representations. These disagreements raise the question: which, if any, of these dissimilarity measures should we believe? We provide a framework to ground this question through a concrete test: measures should have sensitivity to changes that affect functional behavior, and specificity against changes that do not. We quantify this through a variety of functional behaviors including probing accuracy and robustness to distribution shift, and examine changes such as varying random initialization and deleting principal components. We find that current metrics exhibit different weaknesses, note that a classical baseline performs surprisingly well, and highlight settings where all metrics appear to fail, thus providing a challenge set for further improvement.",
		"DOI": "10.48550/arXiv.2108.01661",
		"note": "arXiv:2108.01661 [cs, stat]",
		"number": "arXiv:2108.01661",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Grounding Representation Similarity with Statistical Testing",
		"URL": "http://arxiv.org/abs/2108.01661",
		"author": [
			{
				"family": "Ding",
				"given": "Frances"
			},
			{
				"family": "Denain",
				"given": "Jean-Stanislas"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					11,
					3
				]
			]
		}
	},
	{
		"id": "adebayo2020",
		"type": "article",
		"abstract": "We investigate whether post-hoc model explanations are effective for diagnosing model errors--model debugging. In response to the challenge of explaining a model's prediction, a vast array of explanation methods have been proposed. Despite increasing use, it is unclear if they are effective. To start, we categorize \\textit{bugs}, based on their source, into:~\\textit{data, model, and test-time} contamination bugs. For several explanation methods, we assess their ability to: detect spurious correlation artifacts (data contamination), diagnose mislabeled training examples (data contamination), differentiate between a (partially) re-initialized model and a trained one (model contamination), and detect out-of-distribution inputs (test-time contamination). We find that the methods tested are able to diagnose a spurious background bug, but not conclusively identify mislabeled training examples. In addition, a class of methods, that modify the back-propagation algorithm are invariant to the higher layer parameters of a deep network; hence, ineffective for diagnosing model contamination. We complement our analysis with a human subject study, and find that subjects fail to identify defective models using attributions, but instead rely, primarily, on model predictions. Taken together, our results provide guidance for practitioners and researchers turning to explanations as tools for model debugging.",
		"DOI": "10.48550/arXiv.2011.05429",
		"note": "arXiv:2011.05429 [cs]",
		"number": "arXiv:2011.05429",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Debugging Tests for Model Explanations",
		"URL": "http://arxiv.org/abs/2011.05429",
		"author": [
			{
				"family": "Adebayo",
				"given": "Julius"
			},
			{
				"family": "Muelly",
				"given": "Michael"
			},
			{
				"family": "Liccardi",
				"given": "Ilaria"
			},
			{
				"family": "Kim",
				"given": "Been"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					11,
					10
				]
			]
		}
	},
	{
		"id": "bastings2021",
		"type": "article",
		"abstract": "Feature attribution a.k.a. input salience methods which assign an importance score to a feature are abundant but may produce surprisingly different results for the same model on the same input. While differences are expected if disparate definitions of importance are assumed, most methods claim to provide faithful attributions and point at the features most relevant for a model's prediction. Existing work on faithfulness evaluation is not conclusive and does not provide a clear answer as to how different methods are to be compared. Focusing on text classification and the model debugging scenario, our main contribution is a protocol for faithfulness evaluation that makes use of partially synthetic data to obtain ground truth for feature importance ranking. Following the protocol, we do an in-depth analysis of four standard salience method classes on a range of datasets and shortcuts for BERT and LSTM models and demonstrate that some of the most popular method configurations provide poor results even for simplest shortcuts. We recommend following the protocol for each new task and model combination to find the best method for identifying shortcuts.",
		"DOI": "10.48550/arXiv.2111.07367",
		"note": "arXiv:2111.07367 [cs]",
		"number": "arXiv:2111.07367",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "\"Will You Find These Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification",
		"title-short": "\"Will You Find These Shortcuts?",
		"URL": "http://arxiv.org/abs/2111.07367",
		"author": [
			{
				"family": "Bastings",
				"given": "Jasmijn"
			},
			{
				"family": "Ebert",
				"given": "Sebastian"
			},
			{
				"family": "Zablotskaia",
				"given": "Polina"
			},
			{
				"family": "Sandholm",
				"given": "Anders"
			},
			{
				"family": "Filippova",
				"given": "Katja"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					11,
					14
				]
			]
		}
	},
	{
		"id": "elhage2022solu",
		"type": "article-journal",
		"container-title": "Transformer Circuits Thread",
		"title": "Softmax linear units",
		"author": [
			{
				"literal": "Elhage"
			},
			{
				"literal": "Nelson"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "ElShowk",
				"given": "Sheer"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"literal": "Jones"
			},
			{
				"family": "Drain",
				"given": "Dawn",
				"non-dropping-particle": "and"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Fort",
				"given": "Stanislav"
			},
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Jacobson",
				"given": "Josh"
			},
			{
				"family": "Tran-Johnson",
				"given": "Eli"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hase2021",
		"type": "article",
		"abstract": "Do language models have beliefs about the world? Dennett (1995) famously argues that even thermostats have beliefs, on the view that a belief is simply an informational state decoupled from any motivational state. In this paper, we discuss approaches to detecting when models have beliefs about the world, and we improve on methods for updating model beliefs to be more truthful, with a focus on methods based on learned optimizers or hypernetworks. Our main contributions include: (1) new metrics for evaluating belief-updating methods that focus on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing model updates (SLAG) that improves the performance of learned optimizers, and (3) the introduction of the belief graph, which is a new form of interface with language models that shows the interdependencies between model beliefs. Our experiments suggest that models possess belief-like qualities to only a limited extent, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work. Code is available at https://github.com/peterbhase/SLAG-Belief-Updating",
		"note": "arXiv:2111.13654 [cs]",
		"number": "arXiv:2111.13654",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs",
		"title-short": "Do Language Models Have Beliefs?",
		"URL": "http://arxiv.org/abs/2111.13654",
		"author": [
			{
				"family": "Hase",
				"given": "Peter"
			},
			{
				"family": "Diab",
				"given": "Mona"
			},
			{
				"family": "Celikyilmaz",
				"given": "Asli"
			},
			{
				"family": "Li",
				"given": "Xian"
			},
			{
				"family": "Kozareva",
				"given": "Zornitsa"
			},
			{
				"family": "Stoyanov",
				"given": "Veselin"
			},
			{
				"family": "Bansal",
				"given": "Mohit"
			},
			{
				"family": "Iyer",
				"given": "Srinivasan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					11,
					26
				]
			]
		}
	},
	{
		"id": "olsson2022",
		"type": "article-journal",
		"container-title": "Transformer Circuits Thread",
		"title": "In-context learning and induction heads",
		"author": [
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "erhan2010does",
		"type": "paper-conference",
		"container-title": "Proceedings of the thirteenth international conference on artificial intelligence and statistics",
		"note": "tex.organization: JMLR Workshop and Conference Proceedings",
		"page": "201–208",
		"title": "Why does unsupervised pre-training help deep learning?",
		"author": [
			{
				"family": "Erhan",
				"given": "Dumitru"
			},
			{
				"family": "Courville",
				"given": "Aaron"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Vincent",
				"given": "Pascal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2010"
				]
			]
		}
	},
	{
		"id": "levine2020",
		"type": "paper-conference",
		"abstract": "Self-attention architectures, which are rapidly pushing the frontier in natural language processing, demonstrate a surprising depth-inefficient behavior: Empirical signals indicate that increasing the internal representation (network width) is just as useful as increasing the number of self-attention layers (network depth). In this paper, we theoretically study the interplay between depth and width in self-attention. We shed light on the root of the above phenomenon, and establish two distinct parameter regimes of depth efficiency and inefficiency in self-attention. We invalidate the seemingly plausible hypothesis by which widening is as effective as deepening for self-attention, and show that in fact stacking self-attention layers is so effective that it quickly saturates a capacity of the network width. Specifically, we pinpoint a ``depth threshold\" that is logarithmic in the network width: for networks of depth that is below the threshold, we establish a double-exponential depth-efficiency of the self-attention operation, while for depths over the threshold we show that depth-inefficiency kicks in. Our predictions accord with existing empirical ablations, and we further demonstrate the two depth-(in)efficiency regimes experimentally for common network depths of 6, 12, and 24. By identifying network width as a limiting factor, our analysis indicates that solutions for dramatically increasing the width can facilitate the next leap in self-attention expressivity.",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "22640–22651",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "Limits to Depth Efficiencies of Self-Attention",
		"URL": "https://papers.nips.cc/paper/2020/hash/ff4dfdf5904e920ce52b48c1cef97829-Abstract.html",
		"volume": "33",
		"author": [
			{
				"family": "Levine",
				"given": "Yoav"
			},
			{
				"family": "Wies",
				"given": "Noam"
			},
			{
				"family": "Sharir",
				"given": "Or"
			},
			{
				"family": "Bata",
				"given": "Hofit"
			},
			{
				"family": "Shashua",
				"given": "Amnon"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "hahn2020",
		"type": "article-journal",
		"abstract": "Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.",
		"container-title": "Transactions of the Association for Computational Linguistics",
		"DOI": "10.1162/tacl_a_00306",
		"ISSN": "2307-387X",
		"journalAbbreviation": "Transactions of the Association for Computational Linguistics",
		"note": "arXiv:1906.06755 [cs]",
		"page": "156-171",
		"source": "arXiv.org",
		"title": "Theoretical Limitations of Self-Attention in Neural Sequence Models",
		"URL": "http://arxiv.org/abs/1906.06755",
		"volume": "8",
		"author": [
			{
				"family": "Hahn",
				"given": "Michael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12
				]
			]
		}
	},
	{
		"id": "perez",
		"type": "article-journal",
		"abstract": "Alternatives to recurrent neural networks, in particular, architectures based on self-attention, are gaining momentum for processing input sequences. In spite of their relevance, the computational properties of such networks have not yet been fully explored. We study the computational power of the Transformer, one of the most paradigmatic architectures exemplifying self-attention. We show that the Transformer with hard-attention is Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. Our study also reveals some minimal sets of elements needed to obtain this completeness result.",
		"language": "en",
		"page": "35",
		"source": "Zotero",
		"title": "Attention is Turing Complete",
		"author": [
			{
				"family": "Perez",
				"given": "Jorge"
			},
			{
				"family": "Barcelo",
				"given": "Pablo"
			},
			{
				"family": "Marinkovic",
				"given": "Javier"
			}
		]
	},
	{
		"id": "chiang2022",
		"type": "article",
		"abstract": "Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer's classification decisions become less and less confident (that is, with cross-entropy approaching 1 bit per string) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation suggested by Hahn's lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.",
		"DOI": "10.48550/arXiv.2202.12172",
		"note": "arXiv:2202.12172 [cs]",
		"number": "arXiv:2202.12172",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Overcoming a Theoretical Limitation of Self-Attention",
		"URL": "http://arxiv.org/abs/2202.12172",
		"author": [
			{
				"family": "Chiang",
				"given": "David"
			},
			{
				"family": "Cholak",
				"given": "Peter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					2,
					24
				]
			]
		}
	},
	{
		"id": "micheli2022",
		"type": "article",
		"abstract": "Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games. Our approach sets a new state of the art for methods without lookahead search, and even surpasses MuZero. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our codebase at https://github.com/eloialonso/iris.",
		"DOI": "10.48550/arXiv.2209.00588",
		"note": "arXiv:2209.00588 [cs]",
		"number": "arXiv:2209.00588",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Transformers are Sample Efficient World Models",
		"URL": "http://arxiv.org/abs/2209.00588",
		"author": [
			{
				"family": "Micheli",
				"given": "Vincent"
			},
			{
				"family": "Alonso",
				"given": "Eloi"
			},
			{
				"family": "Fleuret",
				"given": "François"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					1
				]
			]
		}
	},
	{
		"id": "perez2021attention",
		"type": "article-journal",
		"container-title": "Journal of Machine Learning Research",
		"issue": "75",
		"journalAbbreviation": "J. Mach. Learn. Res.",
		"page": "1–35",
		"title": "Attention is turing-complete.",
		"volume": "22",
		"author": [
			{
				"family": "Pérez",
				"given": "Jorge"
			},
			{
				"family": "Barceló",
				"given": "Pablo"
			},
			{
				"family": "Marinkovic",
				"given": "Javier"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "langosco2022",
		"type": "article",
		"abstract": "We study goal misgeneralization, a type of out-of-distribution generalization failure in reinforcement learning (RL). Goal misgeneralization failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong goal. For instance, an agent might continue to competently avoid obstacles, but navigate to the wrong place. In contrast, previous works have typically focused on capability generalization failures, where an agent fails to do anything sensible at test time. We formalize this distinction between capability and goal generalization, provide the first empirical demonstrations of goal misgeneralization, and present a partial characterization of its causes.",
		"DOI": "10.48550/arXiv.2105.14111",
		"note": "arXiv:2105.14111 [cs]",
		"number": "arXiv:2105.14111",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Goal Misgeneralization in Deep Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2105.14111",
		"author": [
			{
				"family": "Langosco",
				"given": "Lauro"
			},
			{
				"family": "Koch",
				"given": "Jack"
			},
			{
				"family": "Sharkey",
				"given": "Lee"
			},
			{
				"family": "Pfau",
				"given": "Jacob"
			},
			{
				"family": "Orseau",
				"given": "Laurent"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					4
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					7
				]
			]
		}
	},
	{
		"id": "ngo2022",
		"type": "article",
		"abstract": "Within the coming decades, artificial general intelligence (AGI) may surpass human capabilities at a wide range of important tasks. This report makes a case for why, without substantial action to prevent it, AGIs will likely use their intelligence to pursue goals which are very undesirable (in other words, misaligned) from a human perspective, with potentially catastrophic consequences. The report aims to cover the key arguments motivating concern about the alignment problem in a way that's as succinct, concrete and technically-grounded as possible. I argue that realistic training processes plausibly lead to the development of misaligned goals in AGIs, in particular because neural networks trained via reinforcement learning will learn to plan towards achieving a range of goals; gain more reward by deceptively pursuing misaligned goals; and generalize in ways which undermine obedience. As in an earlier report from Cotra (2022), I explain my claims with reference to an illustrative AGI training process, then outline possible research directions for addressing different aspects of the problem.",
		"DOI": "10.48550/arXiv.2209.00626",
		"note": "arXiv:2209.00626 [cs]",
		"number": "arXiv:2209.00626",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The alignment problem from a deep learning perspective",
		"URL": "http://arxiv.org/abs/2209.00626",
		"author": [
			{
				"family": "Ngo",
				"given": "Richard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					4
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					8,
					29
				]
			]
		}
	},
	{
		"id": "turner2021",
		"type": "article",
		"abstract": "Some researchers speculate that intelligent reinforcement learning (RL) agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers point out that RL agents need not have human-like power-seeking instincts. To clarify this discussion, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.",
		"note": "arXiv:1912.01683 [cs]",
		"number": "arXiv:1912.01683",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Optimal Policies Tend to Seek Power",
		"URL": "http://arxiv.org/abs/1912.01683",
		"author": [
			{
				"family": "Turner",
				"given": "Alexander Matt"
			},
			{
				"family": "Smith",
				"given": "Logan"
			},
			{
				"family": "Shah",
				"given": "Rohin"
			},
			{
				"family": "Critch",
				"given": "Andrew"
			},
			{
				"family": "Tadepalli",
				"given": "Prasad"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					4
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					3
				]
			]
		}
	},
	{
		"id": "guez2019",
		"type": "article",
		"abstract": "The field of reinforcement learning (RL) is facing increasingly challenging domains with combinatorial complexity. For an RL agent to address these challenges, it is essential that it can plan effectively. Prior work has typically utilized an explicit model of the environment, combined with a specific planning algorithm (such as tree search). More recently, a new family of methods have been proposed that learn how to plan, by providing the structure for planning via an inductive bias in the function approximator (such as a tree structured neural network), trained end-to-end by a model-free RL algorithm. In this paper, we go even further, and demonstrate empirically that an entirely model-free approach, without special structure beyond standard neural network components such as convolutional networks and LSTMs, can learn to exhibit many of the characteristics typically associated with a model-based planner. We measure our agent's effectiveness at planning in terms of its ability to generalize across a combinatorial and irreversible state space, its data efficiency, and its ability to utilize additional thinking time. We find that our agent has many of the characteristics that one might expect to find in a planning algorithm. Furthermore, it exceeds the state-of-the-art in challenging combinatorial domains such as Sokoban and outperforms other model-free approaches that utilize strong inductive biases toward planning.",
		"DOI": "10.48550/arXiv.1901.03559",
		"note": "arXiv:1901.03559 [cs, stat]",
		"number": "arXiv:1901.03559",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "An investigation of model-free planning",
		"URL": "http://arxiv.org/abs/1901.03559",
		"author": [
			{
				"family": "Guez",
				"given": "Arthur"
			},
			{
				"family": "Mirza",
				"given": "Mehdi"
			},
			{
				"family": "Gregor",
				"given": "Karol"
			},
			{
				"family": "Kabra",
				"given": "Rishabh"
			},
			{
				"family": "Racanière",
				"given": "Sébastien"
			},
			{
				"family": "Weber",
				"given": "Théophane"
			},
			{
				"family": "Raposo",
				"given": "David"
			},
			{
				"family": "Santoro",
				"given": "Adam"
			},
			{
				"family": "Orseau",
				"given": "Laurent"
			},
			{
				"family": "Eccles",
				"given": "Tom"
			},
			{
				"family": "Wayne",
				"given": "Greg"
			},
			{
				"family": "Silver",
				"given": "David"
			},
			{
				"family": "Lillicrap",
				"given": "Timothy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					5,
					20
				]
			]
		}
	},
	{
		"id": "denain2022a",
		"type": "article",
		"abstract": "Transparency methods such as model visualizations provide information that outputs alone might miss, since they describe the internals of neural networks. But can we trust that model explanations reflect model behavior? For instance, can they diagnose abnormal behavior such as backdoors or shape bias? To evaluate model explanations, we define a model as anomalous if it differs from a reference set of normal models, and we test whether transparency methods assign different explanations to anomalous and normal models. We find that while existing methods can detect stark anomalies such as shape bias or adversarial training, they struggle to identify more subtle anomalies such as models trained on incomplete data. Moreover, they generally fail to distinguish the inputs that induce anomalous behavior, e.g. images containing a backdoor trigger. These results reveal new blind spots in existing model explanations, pointing to the need for further method development.",
		"note": "arXiv:2206.13498 [cs]",
		"number": "arXiv:2206.13498",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior",
		"title-short": "Auditing Visualizations",
		"URL": "http://arxiv.org/abs/2206.13498",
		"author": [
			{
				"family": "Denain",
				"given": "Jean-Stanislas"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					27
				]
			]
		}
	},
	{
		"id": "adebayo2020a",
		"type": "article",
		"abstract": "We investigate whether post-hoc model explanations are effective for diagnosing model errors--model debugging. In response to the challenge of explaining a model's prediction, a vast array of explanation methods have been proposed. Despite increasing use, it is unclear if they are effective. To start, we categorize \\textit{bugs}, based on their source, into:~\\textit{data, model, and test-time} contamination bugs. For several explanation methods, we assess their ability to: detect spurious correlation artifacts (data contamination), diagnose mislabeled training examples (data contamination), differentiate between a (partially) re-initialized model and a trained one (model contamination), and detect out-of-distribution inputs (test-time contamination). We find that the methods tested are able to diagnose a spurious background bug, but not conclusively identify mislabeled training examples. In addition, a class of methods, that modify the back-propagation algorithm are invariant to the higher layer parameters of a deep network; hence, ineffective for diagnosing model contamination. We complement our analysis with a human subject study, and find that subjects fail to identify defective models using attributions, but instead rely, primarily, on model predictions. Taken together, our results provide guidance for practitioners and researchers turning to explanations as tools for model debugging.",
		"note": "arXiv:2011.05429 [cs]",
		"number": "arXiv:2011.05429",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Debugging Tests for Model Explanations",
		"URL": "http://arxiv.org/abs/2011.05429",
		"author": [
			{
				"family": "Adebayo",
				"given": "Julius"
			},
			{
				"family": "Muelly",
				"given": "Michael"
			},
			{
				"family": "Liccardi",
				"given": "Ilaria"
			},
			{
				"family": "Kim",
				"given": "Been"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					11,
					10
				]
			]
		}
	},
	{
		"id": "bastings2021a",
		"type": "article",
		"abstract": "Feature attribution a.k.a. input salience methods which assign an importance score to a feature are abundant but may produce surprisingly different results for the same model on the same input. While differences are expected if disparate definitions of importance are assumed, most methods claim to provide faithful attributions and point at the features most relevant for a model's prediction. Existing work on faithfulness evaluation is not conclusive and does not provide a clear answer as to how different methods are to be compared. Focusing on text classification and the model debugging scenario, our main contribution is a protocol for faithfulness evaluation that makes use of partially synthetic data to obtain ground truth for feature importance ranking. Following the protocol, we do an in-depth analysis of four standard salience method classes on a range of datasets and shortcuts for BERT and LSTM models and demonstrate that some of the most popular method configurations provide poor results even for simplest shortcuts. We recommend following the protocol for each new task and model combination to find the best method for identifying shortcuts.",
		"note": "arXiv:2111.07367 [cs]",
		"number": "arXiv:2111.07367",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "\"Will You Find These Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification",
		"title-short": "\"Will You Find These Shortcuts?",
		"URL": "http://arxiv.org/abs/2111.07367",
		"author": [
			{
				"family": "Bastings",
				"given": "Jasmijn"
			},
			{
				"family": "Ebert",
				"given": "Sebastian"
			},
			{
				"family": "Zablotskaia",
				"given": "Polina"
			},
			{
				"family": "Sandholm",
				"given": "Anders"
			},
			{
				"family": "Filippova",
				"given": "Katja"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					5
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					11,
					14
				]
			]
		}
	},
	{
		"id": "zou2022",
		"type": "article",
		"abstract": "Forecasting future world events is a challenging but valuable task. Forecasts of climate, geopolitical conflict, pandemics and economic indicators help shape policy and decision making. In these domains, the judgment of expert humans contributes to the best forecasts. Given advances in language modeling, can these forecasts be automated? To this end, we introduce Autocast, a dataset containing thousands of forecasting questions and an accompanying news corpus. Questions are taken from forecasting tournaments, ensuring high quality, real-world importance, and diversity. The news corpus is organized by date, allowing us to precisely simulate the conditions under which humans made past forecasts (avoiding leakage from the future). Motivated by the difficulty of forecasting numbers across orders of magnitude (e.g. global cases of COVID-19 in 2022), we also curate IntervalQA, a dataset of numerical questions and metrics for calibration. We test language models on our forecasting task and find that performance is far below a human expert baseline. However, performance improves with increased model size and incorporation of relevant information from the news corpus. In sum, Autocast poses a novel challenge for large language models and improved performance could bring large practical benefits.",
		"note": "arXiv:2206.15474 [cs]",
		"number": "arXiv:2206.15474",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Forecasting Future World Events with Neural Networks",
		"URL": "http://arxiv.org/abs/2206.15474",
		"author": [
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Xiao",
				"given": "Tristan"
			},
			{
				"family": "Jia",
				"given": "Ryan"
			},
			{
				"family": "Kwon",
				"given": "Joe"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Li",
				"given": "Richard"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			},
			{
				"family": "Evans",
				"given": "Owain"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					6
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					30
				]
			]
		}
	},
	{
		"id": "hendrycks2022b",
		"type": "article",
		"abstract": "Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk.",
		"note": "arXiv:2206.05862 [cs]",
		"number": "arXiv:2206.05862",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "X-Risk Analysis for AI Research",
		"URL": "http://arxiv.org/abs/2206.05862",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					6
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					20
				]
			]
		}
	},
	{
		"id": "liu2022",
		"type": "article",
		"abstract": "Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the \"LU mechanism\" because training and test losses (against model weight norm) typically resemble \"L\" and \"U\", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.",
		"DOI": "10.48550/arXiv.2210.01117",
		"note": "arXiv:2210.01117 [physics, stat]",
		"number": "arXiv:2210.01117",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Omnigrok: Grokking Beyond Algorithmic Data",
		"title-short": "Omnigrok",
		"URL": "http://arxiv.org/abs/2210.01117",
		"author": [
			{
				"family": "Liu",
				"given": "Ziming"
			},
			{
				"family": "Michaud",
				"given": "Eric J."
			},
			{
				"family": "Tegmark",
				"given": "Max"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					6
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					3
				]
			]
		}
	},
	{
		"id": "rauker2022a",
		"type": "article",
		"abstract": "The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are generally difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify failures, fix bugs, and improve basic understanding. In particular, \"inner\" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. Finally, we discuss key challenges and argue for future work emphasizing diagnostics, benchmarking, and robustness.",
		"DOI": "10.48550/arXiv.2207.13243",
		"note": "arXiv:2207.13243 [cs]",
		"number": "arXiv:2207.13243",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks",
		"title-short": "Toward Transparent AI",
		"URL": "http://arxiv.org/abs/2207.13243",
		"author": [
			{
				"family": "Räuker",
				"given": "Tilman"
			},
			{
				"family": "Ho",
				"given": "Anson"
			},
			{
				"family": "Casper",
				"given": "Stephen"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					6
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					5
				]
			]
		}
	},
	{
		"id": "rauker2022b",
		"type": "article",
		"abstract": "The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are generally difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify failures, fix bugs, and improve basic understanding. In particular, \"inner\" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. Finally, we discuss key challenges and argue for future work emphasizing diagnostics, benchmarking, and robustness.",
		"DOI": "10.48550/arXiv.2207.13243",
		"note": "arXiv:2207.13243 [cs]",
		"number": "arXiv:2207.13243",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks",
		"title-short": "Toward Transparent AI",
		"URL": "http://arxiv.org/abs/2207.13243",
		"author": [
			{
				"family": "Räuker",
				"given": "Tilman"
			},
			{
				"family": "Ho",
				"given": "Anson"
			},
			{
				"family": "Casper",
				"given": "Stephen"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					5
				]
			]
		}
	},
	{
		"id": "pope2021",
		"type": "article",
		"abstract": "We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model's prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent white-box and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals.",
		"note": "arXiv:2110.11589 [cs]",
		"number": "arXiv:2110.11589",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Text Counterfactuals via Latent Optimization and Shapley-Guided Search",
		"URL": "http://arxiv.org/abs/2110.11589",
		"author": [
			{
				"family": "Pope",
				"given": "Quintin"
			},
			{
				"family": "Fern",
				"given": "Xiaoli Z."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					10,
					22
				]
			]
		}
	},
	{
		"id": "meng2022",
		"type": "article",
		"abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/",
		"note": "arXiv:2202.05262 [cs]",
		"number": "arXiv:2202.05262",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Locating and Editing Factual Associations in GPT",
		"URL": "http://arxiv.org/abs/2202.05262",
		"author": [
			{
				"family": "Meng",
				"given": "Kevin"
			},
			{
				"family": "Bau",
				"given": "David"
			},
			{
				"family": "Andonian",
				"given": "Alex"
			},
			{
				"family": "Belinkov",
				"given": "Yonatan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					1
				]
			]
		}
	},
	{
		"id": "ainsworth2022",
		"type": "article",
		"abstract": "The success of deep learning is thanks to our ability to solve certain massive non-convex optimization problems with relative ease. Despite non-convex optimization being NP-hard, simple algorithms -- often variants of stochastic gradient descent -- exhibit surprising effectiveness in fitting large neural networks in practice. We argue that neural network loss landscapes contain (nearly) a single basin, after accounting for all possible permutation symmetries of hidden units. We introduce three algorithms to permute the units of one model to bring them into alignment with units of a reference model. This transformation produces a functionally equivalent set of weights that lie in an approximately convex basin near the reference model. Experimentally, we demonstrate the single basin phenomenon across a variety of model architectures and datasets, including the first (to our knowledge) demonstration of zero-barrier linear mode connectivity between independently trained ResNet models on CIFAR-10 and CIFAR-100. Additionally, we identify intriguing phenomena relating model width and training time to mode connectivity across a variety of models and datasets. Finally, we discuss shortcomings of a single basin theory, including a counterexample to the linear mode connectivity hypothesis.",
		"note": "arXiv:2209.04836 [cs]",
		"number": "arXiv:2209.04836",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Git Re-Basin: Merging Models modulo Permutation Symmetries",
		"title-short": "Git Re-Basin",
		"URL": "http://arxiv.org/abs/2209.04836",
		"author": [
			{
				"family": "Ainsworth",
				"given": "Samuel K."
			},
			{
				"family": "Hayase",
				"given": "Jonathan"
			},
			{
				"family": "Srinivasa",
				"given": "Siddhartha"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					11
				]
			]
		}
	},
	{
		"id": "wortsman2022",
		"type": "article",
		"abstract": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results \"model soups.\" When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.",
		"note": "arXiv:2203.05482 [cs]",
		"number": "arXiv:2203.05482",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
		"title-short": "Model soups",
		"URL": "http://arxiv.org/abs/2203.05482",
		"author": [
			{
				"family": "Wortsman",
				"given": "Mitchell"
			},
			{
				"family": "Ilharco",
				"given": "Gabriel"
			},
			{
				"family": "Gadre",
				"given": "Samir Yitzhak"
			},
			{
				"family": "Roelofs",
				"given": "Rebecca"
			},
			{
				"family": "Gontijo-Lopes",
				"given": "Raphael"
			},
			{
				"family": "Morcos",
				"given": "Ari S."
			},
			{
				"family": "Namkoong",
				"given": "Hongseok"
			},
			{
				"family": "Farhadi",
				"given": "Ali"
			},
			{
				"family": "Carmon",
				"given": "Yair"
			},
			{
				"family": "Kornblith",
				"given": "Simon"
			},
			{
				"family": "Schmidt",
				"given": "Ludwig"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					1
				]
			]
		}
	},
	{
		"id": "hendrycks2022c",
		"type": "article",
		"abstract": "In real-world applications of machine learning, reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out-of-distribution (OOD) robustness, prediction consistency, resilience to adversaries, calibrated uncertainty estimates, and the ability to detect anomalous inputs. However, improving performance towards these goals is often a balancing act that today's methods cannot achieve without sacrificing performance on other safety axes. For instance, adversarial training improves adversarial robustness but sharply degrades other classifier performance metrics. Similarly, strong data augmentation and regularization techniques often improve OOD robustness but harm anomaly detection, raising the question of whether a Pareto improvement on all existing safety measures is possible. To meet this challenge, we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals, which outperforms numerous baselines, is near Pareto-optimal, and roundly improves safety measures.",
		"note": "arXiv:2112.05135 [cs]",
		"number": "arXiv:2112.05135",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures",
		"title-short": "PixMix",
		"URL": "http://arxiv.org/abs/2112.05135",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Tang",
				"given": "Leonard"
			},
			{
				"family": "Li",
				"given": "Bo"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					29
				]
			]
		}
	},
	{
		"id": "ghiasi2021",
		"type": "article",
		"abstract": "Building instance segmentation models that are data-efficient and can handle rare object categories is an important challenge in computer vision. Leveraging data augmentations is a promising direction towards addressing this challenge. Here, we perform a systematic study of the Copy-Paste augmentation ([13, 12]) for instance segmentation where we randomly paste objects onto an image. Prior studies on Copy-Paste relied on modeling the surrounding visual context for pasting the objects. However, we find that the simple mechanism of pasting objects randomly is good enough and can provide solid gains on top of strong baselines. Furthermore, we show Copy-Paste is additive with semi-supervised methods that leverage extra data through pseudo labeling (e.g. self-training). On COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an improvement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art. We further demonstrate that Copy-Paste can lead to significant improvements on the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge winning entry by +3.6 mask AP on rare categories.",
		"note": "arXiv:2012.07177 [cs]\nversion: 2",
		"number": "arXiv:2012.07177",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation",
		"URL": "http://arxiv.org/abs/2012.07177",
		"author": [
			{
				"family": "Ghiasi",
				"given": "Golnaz"
			},
			{
				"family": "Cui",
				"given": "Yin"
			},
			{
				"family": "Srinivas",
				"given": "Aravind"
			},
			{
				"family": "Qian",
				"given": "Rui"
			},
			{
				"family": "Lin",
				"given": "Tsung-Yi"
			},
			{
				"family": "Cubuk",
				"given": "Ekin D."
			},
			{
				"family": "Le",
				"given": "Quoc V."
			},
			{
				"family": "Zoph",
				"given": "Barret"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					23
				]
			]
		}
	},
	{
		"id": "chen2021a",
		"type": "article",
		"abstract": "We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",
		"DOI": "10.48550/arXiv.2106.01345",
		"note": "arXiv:2106.01345 [cs]",
		"number": "arXiv:2106.01345",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
		"title-short": "Decision Transformer",
		"URL": "http://arxiv.org/abs/2106.01345",
		"author": [
			{
				"family": "Chen",
				"given": "Lili"
			},
			{
				"family": "Lu",
				"given": "Kevin"
			},
			{
				"family": "Rajeswaran",
				"given": "Aravind"
			},
			{
				"family": "Lee",
				"given": "Kimin"
			},
			{
				"family": "Grover",
				"given": "Aditya"
			},
			{
				"family": "Laskin",
				"given": "Michael"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Srinivas",
				"given": "Aravind"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					24
				]
			]
		}
	},
	{
		"id": "silver2017",
		"type": "article-journal",
		"abstract": "A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.",
		"container-title": "Nature",
		"DOI": "10.1038/nature24270",
		"ISSN": "1476-4687",
		"issue": "7676",
		"language": "en",
		"license": "2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",
		"note": "number: 7676\npublisher: Nature Publishing Group",
		"page": "354-359",
		"source": "www.nature.com",
		"title": "Mastering the game of Go without human knowledge",
		"URL": "https://www.nature.com/articles/nature24270",
		"volume": "550",
		"author": [
			{
				"family": "Silver",
				"given": "David"
			},
			{
				"family": "Schrittwieser",
				"given": "Julian"
			},
			{
				"family": "Simonyan",
				"given": "Karen"
			},
			{
				"family": "Antonoglou",
				"given": "Ioannis"
			},
			{
				"family": "Huang",
				"given": "Aja"
			},
			{
				"family": "Guez",
				"given": "Arthur"
			},
			{
				"family": "Hubert",
				"given": "Thomas"
			},
			{
				"family": "Baker",
				"given": "Lucas"
			},
			{
				"family": "Lai",
				"given": "Matthew"
			},
			{
				"family": "Bolton",
				"given": "Adrian"
			},
			{
				"family": "Chen",
				"given": "Yutian"
			},
			{
				"family": "Lillicrap",
				"given": "Timothy"
			},
			{
				"family": "Hui",
				"given": "Fan"
			},
			{
				"family": "Sifre",
				"given": "Laurent"
			},
			{
				"family": "Driessche",
				"given": "George",
				"non-dropping-particle": "van den"
			},
			{
				"family": "Graepel",
				"given": "Thore"
			},
			{
				"family": "Hassabis",
				"given": "Demis"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					10
				]
			]
		}
	},
	{
		"id": "szegedy2014",
		"type": "article",
		"abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
		"DOI": "10.48550/arXiv.1312.6199",
		"note": "arXiv:1312.6199 [cs]\nversion: 4",
		"number": "arXiv:1312.6199",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Intriguing properties of neural networks",
		"URL": "http://arxiv.org/abs/1312.6199",
		"author": [
			{
				"family": "Szegedy",
				"given": "Christian"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Bruna",
				"given": "Joan"
			},
			{
				"family": "Erhan",
				"given": "Dumitru"
			},
			{
				"family": "Goodfellow",
				"given": "Ian"
			},
			{
				"family": "Fergus",
				"given": "Rob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					2,
					19
				]
			]
		}
	},
	{
		"id": "christiano2018",
		"type": "article",
		"abstract": "Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.",
		"note": "arXiv:1810.08575 [cs, stat]",
		"number": "arXiv:1810.08575",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Supervising strong learners by amplifying weak experts",
		"URL": "http://arxiv.org/abs/1810.08575",
		"author": [
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Shlegeris",
				"given": "Buck"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					19
				]
			]
		}
	},
	{
		"id": "geirhos2018",
		"type": "book",
		"abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies hint to a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on \"Stylized-ImageNet\", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.",
		"source": "ResearchGate",
		"title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
		"author": [
			{
				"family": "Geirhos",
				"given": "Robert"
			},
			{
				"family": "Rubisch",
				"given": "Patricia"
			},
			{
				"family": "Michaelis",
				"given": "Claudio"
			},
			{
				"family": "Bethge",
				"given": "Matthias"
			},
			{
				"family": "Wichmann",
				"given": "Felix"
			},
			{
				"family": "Brendel",
				"given": "Wieland"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					11,
					29
				]
			]
		}
	},
	{
		"id": "ouyang2022",
		"type": "article",
		"abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
		"DOI": "10.48550/arXiv.2203.02155",
		"note": "arXiv:2203.02155 [cs]",
		"number": "arXiv:2203.02155",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Training language models to follow instructions with human feedback",
		"URL": "http://arxiv.org/abs/2203.02155",
		"author": [
			{
				"family": "Ouyang",
				"given": "Long"
			},
			{
				"family": "Wu",
				"given": "Jeff"
			},
			{
				"family": "Jiang",
				"given": "Xu"
			},
			{
				"family": "Almeida",
				"given": "Diogo"
			},
			{
				"family": "Wainwright",
				"given": "Carroll L."
			},
			{
				"family": "Mishkin",
				"given": "Pamela"
			},
			{
				"family": "Zhang",
				"given": "Chong"
			},
			{
				"family": "Agarwal",
				"given": "Sandhini"
			},
			{
				"family": "Slama",
				"given": "Katarina"
			},
			{
				"family": "Ray",
				"given": "Alex"
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Hilton",
				"given": "Jacob"
			},
			{
				"family": "Kelton",
				"given": "Fraser"
			},
			{
				"family": "Miller",
				"given": "Luke"
			},
			{
				"family": "Simens",
				"given": "Maddie"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Welinder",
				"given": "Peter"
			},
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Lowe",
				"given": "Ryan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					4
				]
			]
		}
	},
	{
		"id": "qntm",
		"type": "webpage",
		"container-title": "Things Of Interest",
		"title": "Lena",
		"URL": "https://qntm.org/mmacevedo",
		"author": [
			{
				"family": "qntm",
				"given": ""
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					11
				]
			]
		}
	},
	{
		"id": "ziegler2022a",
		"type": "article",
		"abstract": "In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance. In this work, we used a safe language generation task (``avoid injuries'') as a testbed for achieving high reliability through adversarial training. We created a series of adversarial training techniques -- including a tool that assists human adversaries -- to find and eliminate failures in a classifier that filters text completions suggested by a generator. In our task, we determined that we can set very conservative classifier thresholds without significantly impacting the quality of the filtered outputs. We found that adversarial training increased robustness to the adversarial attacks that we trained on -- doubling the time for our contractors to find adversarial examples both with our tool (from 13 to 26 minutes) and without (from 20 to 44 minutes) -- without affecting in-distribution performance. We hope to see further work in the high-stakes reliability setting, including more powerful tools for enhancing human adversaries and better ways to measure high levels of reliability, until we can confidently rule out the possibility of catastrophic deployment-time failures of powerful models.",
		"DOI": "10.48550/arXiv.2205.01663",
		"note": "arXiv:2205.01663 [cs]",
		"number": "arXiv:2205.01663",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adversarial Training for High-Stakes Reliability",
		"URL": "http://arxiv.org/abs/2205.01663",
		"author": [
			{
				"family": "Ziegler",
				"given": "Daniel M."
			},
			{
				"family": "Nix",
				"given": "Seraphina"
			},
			{
				"family": "Chan",
				"given": "Lawrence"
			},
			{
				"family": "Bauman",
				"given": "Tim"
			},
			{
				"family": "Schmidt-Nielsen",
				"given": "Peter"
			},
			{
				"family": "Lin",
				"given": "Tao"
			},
			{
				"family": "Scherlis",
				"given": "Adam"
			},
			{
				"family": "Nabeshima",
				"given": "Noa"
			},
			{
				"family": "Weinstein-Raun",
				"given": "Ben"
			},
			{
				"family": "Haas",
				"given": "Daniel",
				"non-dropping-particle": "de"
			},
			{
				"family": "Shlegeris",
				"given": "Buck"
			},
			{
				"family": "Thomas",
				"given": "Nate"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					6
				]
			]
		}
	},
	{
		"id": "scheurer2022",
		"type": "article",
		"abstract": "Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many refinements. Second, we choose the refinement with the highest similarity to the feedback. Third, we finetune a language model to maximize the likelihood of the chosen refinement given the input. In synthetic experiments, we first evaluate whether language models accurately incorporate feedback to produce refinements, finding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm finetunes a GPT-3 model to roughly human-level summarization.",
		"note": "arXiv:2204.14146 [cs]",
		"number": "arXiv:2204.14146",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Training Language Models with Language Feedback",
		"URL": "http://arxiv.org/abs/2204.14146",
		"author": [
			{
				"family": "Scheurer",
				"given": "Jérémy"
			},
			{
				"family": "Campos",
				"given": "Jon Ander"
			},
			{
				"family": "Chan",
				"given": "Jun Shern"
			},
			{
				"family": "Chen",
				"given": "Angelica"
			},
			{
				"family": "Cho",
				"given": "Kyunghyun"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					20
				]
			]
		}
	},
	{
		"id": "jeon2020",
		"type": "article",
		"abstract": "It is often difficult to hand-specify what the correct reward function is for a task, so researchers have instead aimed to learn reward functions from human behavior or feedback. The types of behavior interpreted as evidence of the reward function have expanded greatly in recent years. We've gone from demonstrations, to comparisons, to reading into the information leaked when the human is pushing the robot away or turning it off. And surely, there is more to come. How will a robot make sense of all these diverse types of behavior? Our key insight is that different types of behavior can be interpreted in a single unifying formalism - as a reward-rational choice that the human is making, often implicitly. The formalism offers both a unifying lens with which to view past work, as well as a recipe for interpreting new sources of information that are yet to be uncovered. We provide two examples to showcase this: interpreting a new feedback type, and reading into how the choice of feedback itself leaks information about the reward.",
		"DOI": "10.48550/arXiv.2002.04833",
		"note": "arXiv:2002.04833 [cs]",
		"number": "arXiv:2002.04833",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Reward-rational (implicit) choice: A unifying formalism for reward learning",
		"title-short": "Reward-rational (implicit) choice",
		"URL": "http://arxiv.org/abs/2002.04833",
		"author": [
			{
				"family": "Jeon",
				"given": "Hong Jun"
			},
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12,
					11
				]
			]
		}
	},
	{
		"id": "askell2021",
		"type": "article",
		"abstract": "Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.",
		"DOI": "10.48550/arXiv.2112.00861",
		"note": "arXiv:2112.00861 [cs]",
		"number": "arXiv:2112.00861",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A General Language Assistant as a Laboratory for Alignment",
		"URL": "http://arxiv.org/abs/2112.00861",
		"author": [
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Chris"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					9
				]
			]
		}
	},
	{
		"id": "hadfield-menell2016",
		"type": "article",
		"abstract": "For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.",
		"DOI": "10.48550/arXiv.1606.03137",
		"note": "arXiv:1606.03137 [cs]",
		"number": "arXiv:1606.03137",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Cooperative Inverse Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1606.03137",
		"author": [
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					11,
					12
				]
			]
		}
	},
	{
		"id": "cheung2019",
		"type": "article",
		"abstract": "We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.",
		"DOI": "10.48550/arXiv.1902.05522",
		"note": "arXiv:1902.05522 [cs]",
		"number": "arXiv:1902.05522",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Superposition of many models into one",
		"URL": "http://arxiv.org/abs/1902.05522",
		"author": [
			{
				"family": "Cheung",
				"given": "Brian"
			},
			{
				"family": "Terekhov",
				"given": "Alex"
			},
			{
				"family": "Chen",
				"given": "Yubei"
			},
			{
				"family": "Agrawal",
				"given": "Pulkit"
			},
			{
				"family": "Olshausen",
				"given": "Bruno"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					13
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					6,
					17
				]
			]
		}
	},
	{
		"id": "elhage2022",
		"type": "article-journal",
		"container-title": "Transformer Circuits Thread",
		"title": "Toy models of superposition",
		"author": [
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Lasenby",
				"given": "Robert"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Chen",
				"given": "Carol"
			},
			{
				"family": "Grosse",
				"given": "Roger"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Wattenberg",
				"given": "Martin"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "pallasdies2019",
		"type": "article-journal",
		"abstract": "Jellyfish nerve nets provide insight into the origins of nervous systems, as both their taxonomic position and their evolutionary age imply that jellyfish resemble some of the earliest neuron-bearing, actively-swimming animals. Here, we develop the first neuronal network model for the nerve nets of jellyfish. Specifically, we focus on the moon jelly Aurelia aurita and the control of its energy-efficient swimming motion. The proposed single neuron model disentangles the contributions of different currents to a spike. The network model identifies factors ensuring non-pathological activity and suggests an optimization for the transmission of signals. After modeling the jellyfish’s muscle system and its bell in a hydrodynamic environment, we explore the swimming elicited by neural activity. We find that different delays between nerve net activations lead to well-controlled, differently directed movements. Our model bridges the scales from single neurons to behavior, allowing for a comprehensive understanding of jellyfish neural control of locomotion.",
		"container-title": "eLife",
		"DOI": "10.7554/eLife.50084",
		"ISSN": "2050-084X",
		"note": "publisher: eLife Sciences Publications, Ltd",
		"page": "e50084",
		"source": "eLife",
		"title": "From single neurons to behavior in the jellyfish Aurelia aurita",
		"URL": "https://doi.org/10.7554/eLife.50084",
		"volume": "8",
		"author": [
			{
				"family": "Pallasdies",
				"given": "Fabian"
			},
			{
				"family": "Goedeke",
				"given": "Sven"
			},
			{
				"family": "Braun",
				"given": "Wilhelm"
			},
			{
				"family": "Memmesheimer",
				"given": "Raoul-Martin"
			}
		],
		"editor": [
			{
				"family": "Calabrese",
				"given": "Ronald L"
			},
			{
				"family": "Satterlie",
				"given": "Richard"
			},
			{
				"family": "Kanso",
				"given": "Eva"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					12,
					23
				]
			]
		}
	},
	{
		"id": "february82017",
		"type": "webpage",
		"abstract": "Throughout my American Nations series (based on the books American Nations: A History of the Eleven Rival Regional Cultures of North America by Colin Woodard and Albion's Seed: Four British Folkways in America by David Hackett Fischer) I've talked about how North America is divided into distinct ethnocultural regions based on historic settlement patterns. These various regions are visible in many ways, from dialect, politics, enlistment in the military, support for marijuana, average IQ (Maps of the American Nations), attitudes towards the death penalty, abortion, guns, same-sex marriage, and school corporal punishment, as well as overall health, lifespan, and behaviors such as smoking and drug use (More",
		"container-title": "The Unz Review",
		"title": "The Genetics of the American Nations",
		"URL": "https://www.unz.com/jman/the-genetics-of-the-american-nations/",
		"author": [
			{
				"family": "February 8",
				"given": "Jayman •"
			},
			{
				"family": "2017 • 1",
				"given": ""
			},
			{
				"family": "Reply",
				"given": "900 Words • 103 Comments •"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					15
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					2,
					8
				]
			]
		}
	},
	{
		"id": "betancourt2018",
		"type": "webpage",
		"title": "Probability Theory (For Scientists and Engineers)",
		"URL": "https://betanalpha.github.io/assets/case_studies/probability_theory.html#4_representing_probability_distributions_with_densities",
		"author": [
			{
				"family": "Betancourt",
				"given": "Michael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					15
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10
				]
			]
		}
	},
	{
		"id": "zia2009",
		"type": "article-journal",
		"abstract": "The Legendre transform is an important tool in theoretical physics, playing a critical role in classical mechanics, statistical mechanics, and thermodynamics. Yet, in typical undergraduate or graduate courses, the power of motivation and elegance of the method are often missing, unlike the treatments frequently enjoyed by Fourier transforms. We review and modify the presentation of Legendre transforms in a way that explicates the formal mathematics, resulting in manifestly symmetric equations, thereby clarifying the structure of the transform algebraically and geometrically. Then we bring in the physics to motivate the transform as a way of choosing independent variables that are more easily controlled. We demonstrate how the Legendre transform arises naturally from statistical mechanics and show how the use of dimensionless thermodynamic potentials leads to more natural and symmetric relations.",
		"container-title": "American Journal of Physics",
		"DOI": "10.1119/1.3119512",
		"ISSN": "0002-9505, 1943-2909",
		"issue": "7",
		"journalAbbreviation": "American Journal of Physics",
		"note": "arXiv:0806.1147 [physics]",
		"page": "614-622",
		"source": "arXiv.org",
		"title": "Making Sense of the Legendre Transform",
		"URL": "http://arxiv.org/abs/0806.1147",
		"volume": "77",
		"author": [
			{
				"family": "Zia",
				"given": "R. K. P."
			},
			{
				"family": "Redish",
				"given": "Edward F."
			},
			{
				"family": "McKay",
				"given": "Susan R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009",
					7
				]
			]
		}
	},
	{
		"id": "graves2017",
		"type": "article",
		"abstract": "This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. ACT requires minimal changes to the network architecture, is deterministic and differentiable, and does not add any noise to the parameter gradients. Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers. Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem. We also present character-level language modelling results on the Hutter prize Wikipedia dataset. In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences. This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.",
		"note": "arXiv:1603.08983 [cs]",
		"number": "arXiv:1603.08983",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adaptive Computation Time for Recurrent Neural Networks",
		"URL": "http://arxiv.org/abs/1603.08983",
		"author": [
			{
				"family": "Graves",
				"given": "Alex"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					2,
					21
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/6773082/items/UQVZSA65",
		"type": "document",
		"title": "vandermaaten08a.pdf",
		"URL": "https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf",
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					18
				]
			]
		}
	},
	{
		"id": "turner2021a",
		"type": "article",
		"abstract": "Some researchers speculate that intelligent reinforcement learning (RL) agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers point out that RL agents need not have human-like power-seeking instincts. To clarify this discussion, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.",
		"note": "arXiv:1912.01683 [cs]",
		"number": "arXiv:1912.01683",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Optimal Policies Tend to Seek Power",
		"URL": "http://arxiv.org/abs/1912.01683",
		"author": [
			{
				"family": "Turner",
				"given": "Alexander Matt"
			},
			{
				"family": "Smith",
				"given": "Logan"
			},
			{
				"family": "Shah",
				"given": "Rohin"
			},
			{
				"family": "Critch",
				"given": "Andrew"
			},
			{
				"family": "Tadepalli",
				"given": "Prasad"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					18
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					3
				]
			]
		}
	},
	{
		"id": "zhou2022",
		"type": "article",
		"abstract": "Although chain-of-thought prompting has shown impressive results on many natural language reasoning tasks, it often performs poorly on tasks which need to solve problems harder than the demonstration examples. To tackle such easy-to-hard generalization issues, we propose a novel prompting strategy, least-to-most prompting. It reduces a complex problem into a list of subproblems, and then sequentially solve these subproblems, whereby solving a given subproblem is facilitated by the answers to previously solved subproblems. Experiments on symbolic manipulation, compositional generalization and math reasoning show that least-to-most prompting can generalize to the examples that are harder than those seen in the prompt, and outperform chain-of-thought prompting by a large margin. A notable result is that the GPT-3 code-davinci-002 model with least-to-most-prompting solves the SCAN benchmark regardless of splits (such as length split) with an accuracy of 99.7% using 14 examples versus an accuracy of 16.2% by chain-of-thought prompting, and neural-symbolic models in the literature specialized for solving SCAN are trained with the full training set of more than 15,000 examples.",
		"DOI": "10.48550/arXiv.2205.10625",
		"note": "arXiv:2205.10625 [cs]",
		"number": "arXiv:2205.10625",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
		"URL": "http://arxiv.org/abs/2205.10625",
		"author": [
			{
				"family": "Zhou",
				"given": "Denny"
			},
			{
				"family": "Schärli",
				"given": "Nathanael"
			},
			{
				"family": "Hou",
				"given": "Le"
			},
			{
				"family": "Wei",
				"given": "Jason"
			},
			{
				"family": "Scales",
				"given": "Nathan"
			},
			{
				"family": "Wang",
				"given": "Xuezhi"
			},
			{
				"family": "Schuurmans",
				"given": "Dale"
			},
			{
				"family": "Cui",
				"given": "Claire"
			},
			{
				"family": "Bousquet",
				"given": "Olivier"
			},
			{
				"family": "Le",
				"given": "Quoc"
			},
			{
				"family": "Chi",
				"given": "Ed"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					6
				]
			]
		}
	},
	{
		"id": "nakano2022",
		"type": "article",
		"abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.",
		"DOI": "10.48550/arXiv.2112.09332",
		"note": "arXiv:2112.09332 [cs]",
		"number": "arXiv:2112.09332",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "WebGPT: Browser-assisted question-answering with human feedback",
		"title-short": "WebGPT",
		"URL": "http://arxiv.org/abs/2112.09332",
		"author": [
			{
				"family": "Nakano",
				"given": "Reiichiro"
			},
			{
				"family": "Hilton",
				"given": "Jacob"
			},
			{
				"family": "Balaji",
				"given": "Suchir"
			},
			{
				"family": "Wu",
				"given": "Jeff"
			},
			{
				"family": "Ouyang",
				"given": "Long"
			},
			{
				"family": "Kim",
				"given": "Christina"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Jain",
				"given": "Shantanu"
			},
			{
				"family": "Kosaraju",
				"given": "Vineet"
			},
			{
				"family": "Saunders",
				"given": "William"
			},
			{
				"family": "Jiang",
				"given": "Xu"
			},
			{
				"family": "Cobbe",
				"given": "Karl"
			},
			{
				"family": "Eloundou",
				"given": "Tyna"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Button",
				"given": "Kevin"
			},
			{
				"family": "Knight",
				"given": "Matthew"
			},
			{
				"family": "Chess",
				"given": "Benjamin"
			},
			{
				"family": "Schulman",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					1
				]
			]
		}
	},
	{
		"id": "menick2022",
		"type": "article",
		"abstract": "Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train \"open-book\" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\\% of the time on this Natural Questions subset, and 67\\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\\% and 80\\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.",
		"DOI": "10.48550/arXiv.2203.11147",
		"note": "arXiv:2203.11147 [cs]",
		"number": "arXiv:2203.11147",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Teaching language models to support answers with verified quotes",
		"URL": "http://arxiv.org/abs/2203.11147",
		"author": [
			{
				"family": "Menick",
				"given": "Jacob"
			},
			{
				"family": "Trebacz",
				"given": "Maja"
			},
			{
				"family": "Mikulik",
				"given": "Vladimir"
			},
			{
				"family": "Aslanides",
				"given": "John"
			},
			{
				"family": "Song",
				"given": "Francis"
			},
			{
				"family": "Chadwick",
				"given": "Martin"
			},
			{
				"family": "Glaese",
				"given": "Mia"
			},
			{
				"family": "Young",
				"given": "Susannah"
			},
			{
				"family": "Campbell-Gillingham",
				"given": "Lucy"
			},
			{
				"family": "Irving",
				"given": "Geoffrey"
			},
			{
				"family": "McAleese",
				"given": "Nat"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					21
				]
			]
		}
	},
	{
		"id": "yang2022",
		"type": "article",
		"abstract": "Imitation learning aims to extract high-performance policies from logged demonstrations of expert behavior. It is common to frame imitation learning as a supervised learning problem in which one fits a function approximator to the input-output mapping exhibited by the logged demonstrations (input observations to output actions). While the framing of imitation learning as a supervised input-output learning problem allows for applicability in a wide variety of settings, it is also an overly simplistic view of the problem in situations where the expert demonstrations provide much richer insight into expert behavior. For example, applications such as path navigation, robot manipulation, and strategy games acquire expert demonstrations via planning, search, or some other multi-step algorithm, revealing not just the output action to be imitated but also the procedure for how to determine this action. While these intermediate computations may use tools not available to the agent during inference (e.g., environment simulators), they are nevertheless informative as a way to explain an expert's mapping of state to actions. To properly leverage expert procedure information without relying on the privileged tools the expert may have used to perform the procedure, we propose procedure cloning, which applies supervised sequence prediction to imitate the series of expert computations. This way, procedure cloning learns not only what to do (i.e., the output action), but how and why to do it (i.e., the procedure). Through empirical analysis on navigation, simulated robotic manipulation, and game-playing environments, we show that imitating the intermediate computations of an expert's behavior enables procedure cloning to learn policies exhibiting significant generalization to unseen environment configurations, including those configurations for which running the expert's procedure directly is infeasible.",
		"note": "arXiv:2205.10816 [cs]",
		"number": "arXiv:2205.10816",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Chain of Thought Imitation with Procedure Cloning",
		"URL": "http://arxiv.org/abs/2205.10816",
		"author": [
			{
				"family": "Yang",
				"given": "Mengjiao"
			},
			{
				"family": "Schuurmans",
				"given": "Dale"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Nachum",
				"given": "Ofir"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					22
				]
			]
		}
	},
	{
		"id": "he2015",
		"type": "article",
		"abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
		"note": "arXiv:1512.03385 [cs]",
		"number": "arXiv:1512.03385",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Deep Residual Learning for Image Recognition",
		"URL": "http://arxiv.org/abs/1512.03385",
		"author": [
			{
				"family": "He",
				"given": "Kaiming"
			},
			{
				"family": "Zhang",
				"given": "Xiangyu"
			},
			{
				"family": "Ren",
				"given": "Shaoqing"
			},
			{
				"family": "Sun",
				"given": "Jian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					12,
					10
				]
			]
		}
	},
	{
		"id": "ioffe2015",
		"type": "article",
		"abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.",
		"note": "arXiv:1502.03167 [cs]",
		"number": "arXiv:1502.03167",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
		"title-short": "Batch Normalization",
		"URL": "http://arxiv.org/abs/1502.03167",
		"author": [
			{
				"family": "Ioffe",
				"given": "Sergey"
			},
			{
				"family": "Szegedy",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					3,
					2
				]
			]
		}
	},
	{
		"id": "crutchfield1994",
		"type": "article-journal",
		"abstract": "Deﬁning structure and detecting the emergence of complexity in nature are inherently subjective, though essential, scientiﬁc activities. Despite the difﬁculties, these problems can be analyzed in terms of how model-building observers infer from measurements the computational capabilities embedded in nonlinear processes. An observer’s notion of what is ordered, what is random, and what is complex in its environment depends directly on its computational resources: the amount of raw measurement data, of memory, and of time available for estimation and inference. The discovery of structure in an environment depends more critically and subtlely, though, on how those resources are organized. The descriptive power of the observer’s chosen (or implicit) computational model class, for example, can be an overwhelming determinant in ﬁnding regularity in data.",
		"container-title": "Physica D: Nonlinear Phenomena",
		"DOI": "10.1016/0167-2789(94)90273-9",
		"ISSN": "01672789",
		"issue": "1-3",
		"journalAbbreviation": "Physica D: Nonlinear Phenomena",
		"language": "en",
		"page": "11-54",
		"source": "DOI.org (Crossref)",
		"title": "The calculi of emergence: computation, dynamics and induction",
		"title-short": "The calculi of emergence",
		"URL": "https://linkinghub.elsevier.com/retrieve/pii/0167278994902739",
		"volume": "75",
		"author": [
			{
				"family": "Crutchfield",
				"given": "James P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1994",
					8
				]
			]
		}
	},
	{
		"id": "murfet2020",
		"type": "article",
		"abstract": "In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus \"dividing\" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice.",
		"DOI": "10.48550/arXiv.2010.11560",
		"note": "arXiv:2010.11560 [cs]",
		"number": "arXiv:2010.11560",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Deep Learning is Singular, and That's Good",
		"URL": "http://arxiv.org/abs/2010.11560",
		"author": [
			{
				"family": "Murfet",
				"given": "Daniel"
			},
			{
				"family": "Wei",
				"given": "Susan"
			},
			{
				"family": "Gong",
				"given": "Mingming"
			},
			{
				"family": "Li",
				"given": "Hui"
			},
			{
				"family": "Gell-Redman",
				"given": "Jesse"
			},
			{
				"family": "Quella",
				"given": "Thomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					22
				]
			]
		}
	},
	{
		"id": "carroll",
		"type": "article-journal",
		"language": "en",
		"page": "72",
		"source": "Zotero",
		"title": "Phase Transitions in Neural Networks",
		"author": [
			{
				"family": "Carroll",
				"given": "Liam"
			}
		]
	},
	{
		"id": "oppenlaender2022",
		"type": "article",
		"abstract": "Text-to-image generation has seen an explosion of interest since 2021. Today, beautiful and intriguing digital images and artworks can be synthesized from textual inputs (\"prompts\") with deep generative models. Online communities around text-to-image generation and AI generated art have quickly emerged. This paper identifies six types of prompt modifiers used by practitioners in the online community based on a 3-month ethnographic study. The novel taxonomy of prompt modifiers provides researchers a conceptual starting point for investigating the practice of text-to-image generation, but may also help practitioners of AI generated art improve their images. We further outline how prompt modifiers are applied in the practice of \"prompt engineering.\" We discuss research opportunities of this novel creative practice in the field of Human-Computer Interaction (HCI). The paper concludes with a discussion of broader implications of prompt engineering from the perspective of Human-AI Interaction (HAI) in future applications beyond the use case of text-to-image generation and AI generated art.",
		"note": "arXiv:2204.13988 [cs]",
		"number": "arXiv:2204.13988",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Taxonomy of Prompt Modifiers for Text-To-Image Generation",
		"URL": "http://arxiv.org/abs/2204.13988",
		"author": [
			{
				"family": "Oppenlaender",
				"given": "Jonas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					24
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					31
				]
			]
		}
	},
	{
		"id": "li2016",
		"type": "article",
		"abstract": "Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units.",
		"note": "arXiv:1511.07543 [cs]",
		"number": "arXiv:1511.07543",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Convergent Learning: Do different neural networks learn the same representations?",
		"title-short": "Convergent Learning",
		"URL": "http://arxiv.org/abs/1511.07543",
		"author": [
			{
				"family": "Li",
				"given": "Yixuan"
			},
			{
				"family": "Yosinski",
				"given": "Jason"
			},
			{
				"family": "Clune",
				"given": "Jeff"
			},
			{
				"family": "Lipson",
				"given": "Hod"
			},
			{
				"family": "Hopcroft",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					2,
					28
				]
			]
		}
	},
	{
		"id": "wang2018",
		"type": "paper-conference",
		"abstract": "It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations.  We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match.",
		"container-title": "Advances in Neural Information Processing Systems",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation",
		"title-short": "Towards Understanding Learning Representations",
		"URL": "https://proceedings.neurips.cc/paper/2018/hash/5fc34ed307aac159a30d81181c99847e-Abstract.html",
		"volume": "31",
		"author": [
			{
				"family": "Wang",
				"given": "Liwei"
			},
			{
				"family": "Hu",
				"given": "Lunjia"
			},
			{
				"family": "Gu",
				"given": "Jiayuan"
			},
			{
				"family": "Hu",
				"given": "Zhiqiang"
			},
			{
				"family": "Wu",
				"given": "Yue"
			},
			{
				"family": "He",
				"given": "Kun"
			},
			{
				"family": "Hopcroft",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "xiong2020",
		"type": "article",
		"abstract": "The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the ﬁnal performance but will slow down the optimization and bring more hyperparameter tunings. In this paper, we ﬁrst study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Speciﬁcally, we prove with mean ﬁeld theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring signiﬁcantly less training time and hyper-parameter tuning on a wide range of applications.",
		"language": "en",
		"note": "arXiv:2002.04745 [cs, stat]",
		"number": "arXiv:2002.04745",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On Layer Normalization in the Transformer Architecture",
		"URL": "http://arxiv.org/abs/2002.04745",
		"author": [
			{
				"family": "Xiong",
				"given": "Ruibin"
			},
			{
				"family": "Yang",
				"given": "Yunchang"
			},
			{
				"family": "He",
				"given": "Di"
			},
			{
				"family": "Zheng",
				"given": "Kai"
			},
			{
				"family": "Zheng",
				"given": "Shuxin"
			},
			{
				"family": "Xing",
				"given": "Chen"
			},
			{
				"family": "Zhang",
				"given": "Huishuai"
			},
			{
				"family": "Lan",
				"given": "Yanyan"
			},
			{
				"family": "Wang",
				"given": "Liwei"
			},
			{
				"family": "Liu",
				"given": "Tie-Yan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					26
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					6,
					29
				]
			]
		}
	},
	{
		"id": "mcgrath2022",
		"type": "article",
		"abstract": "What is learned by sophisticated neural network agents such as AlphaZero? This question is of both scientific and practical interest. If the representations of strong neural networks bear no resemblance to human concepts, our ability to understand faithful explanations of their decisions will be restricted, ultimately limiting what we can achieve with neural network interpretability. In this work we provide evidence that human knowledge is acquired by the AlphaZero neural network as it trains on the game of chess. By probing for a broad range of human chess concepts we show when and where these concepts are represented in the AlphaZero network. We also provide a behavioural analysis focusing on opening play, including qualitative analysis from chess Grandmaster Vladimir Kramnik. Finally, we carry out a preliminary investigation looking at the low-level details of AlphaZero's representations, and make the resulting behavioural and representational analyses available online.",
		"DOI": "10.48550/arXiv.2111.09259",
		"note": "arXiv:2111.09259 [cs, stat]",
		"number": "arXiv:2111.09259",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Acquisition of Chess Knowledge in AlphaZero",
		"URL": "http://arxiv.org/abs/2111.09259",
		"author": [
			{
				"family": "McGrath",
				"given": "Thomas"
			},
			{
				"family": "Kapishnikov",
				"given": "Andrei"
			},
			{
				"family": "Tomašev",
				"given": "Nenad"
			},
			{
				"family": "Pearce",
				"given": "Adam"
			},
			{
				"family": "Hassabis",
				"given": "Demis"
			},
			{
				"family": "Kim",
				"given": "Been"
			},
			{
				"family": "Paquet",
				"given": "Ulrich"
			},
			{
				"family": "Kramnik",
				"given": "Vladimir"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					8,
					18
				]
			]
		}
	},
	{
		"id": "bau2020",
		"type": "article",
		"abstract": "A deep generative model such as a GAN learns to model a rich set of semantic and physical rules about the target distribution, but up to now, it has been obscure how such rules are encoded in the network, or how a rule could be changed. In this paper, we introduce a new problem setting: manipulation of specific rules encoded by a deep generative model. To address the problem, we propose a formulation in which the desired rule is changed by manipulating a layer of a deep network as a linear associative memory. We derive an algorithm for modifying one entry of the associative memory, and we demonstrate that several interesting structural rules can be located and modified within the layers of state-of-the-art generative models. We present a user interface to enable users to interactively change the rules of a generative model to achieve desired effects, and we show several proof-of-concept applications. Finally, results on multiple datasets demonstrate the advantage of our method against standard fine-tuning methods and edit transfer algorithms.",
		"note": "arXiv:2007.15646 [cs]",
		"number": "arXiv:2007.15646",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Rewriting a Deep Generative Model",
		"URL": "http://arxiv.org/abs/2007.15646",
		"author": [
			{
				"family": "Bau",
				"given": "David"
			},
			{
				"family": "Liu",
				"given": "Steven"
			},
			{
				"family": "Wang",
				"given": "Tongzhou"
			},
			{
				"family": "Zhu",
				"given": "Jun-Yan"
			},
			{
				"family": "Torralba",
				"given": "Antonio"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					30
				]
			]
		}
	},
	{
		"id": "mu2021",
		"type": "article",
		"abstract": "We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple \"copy-paste\" adversarial examples that change model behavior in predictable ways.",
		"DOI": "10.48550/arXiv.2006.14032",
		"note": "arXiv:2006.14032 [cs, stat]",
		"number": "arXiv:2006.14032",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Compositional Explanations of Neurons",
		"URL": "http://arxiv.org/abs/2006.14032",
		"author": [
			{
				"family": "Mu",
				"given": "Jesse"
			},
			{
				"family": "Andreas",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					2,
					2
				]
			]
		}
	},
	{
		"id": "kim2018",
		"type": "article",
		"abstract": "The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of \"zebra\" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.",
		"DOI": "10.48550/arXiv.1711.11279",
		"note": "arXiv:1711.11279 [stat]",
		"number": "arXiv:1711.11279",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
		"title-short": "Interpretability Beyond Feature Attribution",
		"URL": "http://arxiv.org/abs/1711.11279",
		"author": [
			{
				"family": "Kim",
				"given": "Been"
			},
			{
				"family": "Wattenberg",
				"given": "Martin"
			},
			{
				"family": "Gilmer",
				"given": "Justin"
			},
			{
				"family": "Cai",
				"given": "Carrie"
			},
			{
				"family": "Wexler",
				"given": "James"
			},
			{
				"family": "Viegas",
				"given": "Fernanda"
			},
			{
				"family": "Sayres",
				"given": "Rory"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6,
					7
				]
			]
		}
	},
	{
		"id": "critch2016",
		"type": "article",
		"abstract": "L\\\"ob's theorem and G\\\"odel's theorems make predictions about the behavior of systems capable of self-reference with unbounded computational resources with which to write and evaluate proofs. However, in the real world, systems capable of self-reference will have limited memory and processing speed, so in this paper we introduce an effective version of L\\\"ob's theorem which is applicable given such bounded resources. These results have powerful implications for the game theory of bounded agents who are able to write proofs about themselves and one another, including the capacity to out-perform classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner's Dilemma. Previous cooperative program equilibria studied by Tennenholtz (2004) and Fortnow (2009) have depended on tests for program equality, a fragile condition, whereas \"L\\\"obian\" cooperation is much more robust and agnostic of the opponent's implementation.",
		"DOI": "10.48550/arXiv.1602.04184",
		"note": "arXiv:1602.04184 [cs]",
		"number": "arXiv:1602.04184",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Parametric Bounded L\\\"ob's Theorem and Robust Cooperation of Bounded Agents",
		"URL": "http://arxiv.org/abs/1602.04184",
		"author": [
			{
				"family": "Critch",
				"given": "Andrew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					8,
					24
				]
			]
		}
	},
	{
		"id": "needham1993",
		"type": "article-journal",
		"container-title": "The American Mathematical Monthly",
		"DOI": "10.2307/2324783",
		"ISSN": "00029890",
		"issue": "8",
		"journalAbbreviation": "The American Mathematical Monthly",
		"page": "768",
		"source": "DOI.org (Crossref)",
		"title": "A Visual Explanation of Jensen's Inequality",
		"URL": "https://www.jstor.org/stable/2324783?origin=crossref",
		"volume": "100",
		"author": [
			{
				"family": "Needham",
				"given": "Tristan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1993",
					10
				]
			]
		}
	},
	{
		"id": "roth2012",
		"type": "chapter",
		"abstract": "Primates are, on average, more intelligent than other mammals, with great apes and finally humans on top. They generally have larger brains and cortices, and because of higher relative cortex volume and neuron packing density (NPD), they have much more cortical neurons than other mammalian taxa with the same brain size. Likewise, information processing capacity is generally higher in primates due to short interneuronal distance and high axonal conduction velocity. Across primate taxa, differences in intelligence correlate best with differences in number of cortical neurons and synapses plus information processing speed. The human brain stands out by having a large cortical volume with relatively high NPD, high conduction velocity, and high cortical parcellation. All aspects of human intelligence are present at least in rudimentary form in nonhuman primates or some mammals or vertebrates except syntactical language. The latter can be regarded as a very potent “intelligence amplifier.”",
		"collection-title": "Evolution of the Primate Brain",
		"container-title": "Progress in Brain Research",
		"language": "en",
		"note": "DOI: 10.1016/B978-0-444-53860-4.00020-9",
		"page": "413-430",
		"publisher": "Elsevier",
		"source": "ScienceDirect",
		"title": "Chapter 20 - Evolution of the brain and intelligence in primates",
		"URL": "https://www.sciencedirect.com/science/article/pii/B9780444538604000209",
		"volume": "195",
		"author": [
			{
				"family": "Roth",
				"given": "Gerhard"
			},
			{
				"family": "Dicke",
				"given": "Ursula"
			}
		],
		"editor": [
			{
				"family": "Hofman",
				"given": "Michel A."
			},
			{
				"family": "Falk",
				"given": "Dean"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					1,
					1
				]
			]
		}
	},
	{
		"id": "saniotis2020",
		"type": "article-journal",
		"abstract": "Human intelligence has been theorized since the ancient Greeks. Plato and Aristotle incorporated theories of human intelligence into their metaphysical and cosmological theories which informed the social and medical sciences for centuries. With the advent of the 20th century, human intelligence became increasingly standardized based on Intelligence Quotients (IQ). Moreover, multiple theories of human intelligence were posited on morphological features of the human brain, focusing on cranial volume and size of the pre-frontal cortex which was suggestive of superior human cognitive abilities. This article argues that fixation with anatomical features of the brain was tended to ignore the importance of neuro-hormonal regulation which is a more appropriate indicator of human cognitive abilities. The article challenges the correlation between brain size and human cognitive abilities while offering an alternate theory of human cognitive abilities which emphasizes the roles of neurotransmitters, neurotrophins, and enteric gut microbiome (EGM) regulation.",
		"container-title": "Frontiers in Neuroanatomy",
		"ISSN": "1662-5129",
		"source": "Frontiers",
		"title": "Neuro-hormonal Regulation Is a Better Indicator of Human Cognitive Abilities Than Brain Anatomy: The Need for a New Paradigm",
		"title-short": "Neuro-hormonal Regulation Is a Better Indicator of Human Cognitive Abilities Than Brain Anatomy",
		"URL": "https://www.frontiersin.org/articles/10.3389/fnana.2019.00101",
		"volume": "13",
		"author": [
			{
				"family": "Saniotis",
				"given": "Arthur"
			},
			{
				"family": "Grantham",
				"given": "James P."
			},
			{
				"family": "Kumaratilake",
				"given": "Jaliya"
			},
			{
				"family": "Henneberg",
				"given": "Maciej"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "goriounova2018",
		"type": "article-journal",
		"abstract": "It is generally assumed that human intelligence relies on efficient processing by neurons in our brain. Although grey matter thickness and activity of temporal and frontal cortical areas correlate with IQ scores, no direct evidence exists that links structural and physiological properties of neurons to human intelligence. Here, we find that high IQ scores and large temporal cortical thickness associate with larger, more complex dendrites of human pyramidal neurons. We show in silico that larger dendritic trees enable pyramidal neurons to track activity of synaptic inputs with higher temporal precision, due to fast action potential kinetics. Indeed, we find that human pyramidal neurons of individuals with higher IQ scores sustain fast action potential kinetics during repeated firing. These findings provide the first evidence that human intelligence is associated with neuronal complexity, action potential kinetics and efficient information transfer from inputs to output within cortical neurons.",
		"container-title": "eLife",
		"DOI": "10.7554/eLife.41714",
		"ISSN": "2050-084X",
		"note": "publisher: eLife Sciences Publications, Ltd",
		"page": "e41714",
		"source": "eLife",
		"title": "Large and fast human pyramidal neurons associate with intelligence",
		"URL": "https://doi.org/10.7554/eLife.41714",
		"volume": "7",
		"author": [
			{
				"family": "Goriounova",
				"given": "Natalia A"
			},
			{
				"family": "Heyer",
				"given": "Djai B"
			},
			{
				"family": "Wilbers",
				"given": "René"
			},
			{
				"family": "Verhoog",
				"given": "Matthijs B"
			},
			{
				"family": "Giugliano",
				"given": "Michele"
			},
			{
				"family": "Verbist",
				"given": "Christophe"
			},
			{
				"family": "Obermayer",
				"given": "Joshua"
			},
			{
				"family": "Kerkhofs",
				"given": "Amber"
			},
			{
				"family": "Smeding",
				"given": "Harriët"
			},
			{
				"family": "Verberne",
				"given": "Maaike"
			},
			{
				"family": "Idema",
				"given": "Sander"
			},
			{
				"family": "Baayen",
				"given": "Johannes C"
			},
			{
				"family": "Pieneman",
				"given": "Anton W"
			},
			{
				"family": "Kock",
				"given": "Christiaan PJ",
				"non-dropping-particle": "de"
			},
			{
				"family": "Klein",
				"given": "Martin"
			},
			{
				"family": "Mansvelder",
				"given": "Huibert D"
			}
		],
		"editor": [
			{
				"family": "Badre",
				"given": "David"
			},
			{
				"family": "Behrens",
				"given": "Timothy E"
			},
			{
				"family": "Koch",
				"given": "Christof"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					12,
					18
				]
			]
		}
	},
	{
		"id": "dicke2016",
		"type": "article-journal",
		"abstract": "Many attempts have been made to correlate degrees of both animal and human intelligence with brain properties. With respect to mammals, a much-discussed trait concerns absolute and relative brain size, either uncorrected or corrected for body size. However, the correlation of both with degrees of intelligence yields large inconsistencies, because although they are regarded as the most intelligent mammals, monkeys and apes, including humans, have neither the absolutely nor the relatively largest brains. The best fit between brain traits and degrees of intelligence among mammals is reached by a combination of the number of cortical neurons, neuron packing density, interneuronal distance and axonal conduction velocity—factors that determine general information processing capacity (IPC), as reflected by general intelligence. The highest IPC is found in humans, followed by the great apes, Old World and New World monkeys. The IPC of cetaceans and elephants is much lower because of a thin cortex, low neuron packing density and low axonal conduction velocity. By contrast, corvid and psittacid birds have very small and densely packed pallial neurons and relatively many neurons, which, despite very small brain volumes, might explain their high intelligence. The evolution of a syntactical and grammatical language in humans most probably has served as an additional intelligence amplifier, which may have happened in songbirds and psittacids in a convergent manner.",
		"container-title": "Philosophical Transactions of the Royal Society B: Biological Sciences",
		"DOI": "10.1098/rstb.2015.0180",
		"issue": "1685",
		"note": "publisher: Royal Society",
		"page": "20150180",
		"source": "royalsocietypublishing.org (Atypon)",
		"title": "Neuronal factors determining high intelligence",
		"URL": "https://royalsocietypublishing.org/doi/full/10.1098/rstb.2015.0180",
		"volume": "371",
		"author": [
			{
				"family": "Dicke",
				"given": "Ursula"
			},
			{
				"family": "Roth",
				"given": "Gerhard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					1,
					5
				]
			]
		}
	},
	{
		"id": "herculano-houzel2014",
		"type": "article-journal",
		"abstract": "Enough species have now been subject to systematic quantitative analysis of the relationship between the morphology and cellular composition of their brain that patterns begin to emerge and shed light on the evolutionary path that led to mammalian brain diversity. Based on an analysis of the shared and clade-specific characteristics of 41 modern mammalian species in 6 clades, and in light of the phylogenetic relationships among them, here we propose that ancestral mammal brains were composed and scaled in their cellular composition like modern afrotherian and glire brains: with an addition of neurons that is accompanied by a decrease in neuronal density and very little modification in glial cell density, implying a significant increase in average neuronal cell size in larger brains, and the allocation of approximately 2 neurons in the cerebral cortex and 8 neurons in the cerebellum for every neuron allocated to the rest of brain. We also propose that in some clades the scaling of different brain structures has diverged away from the common ancestral layout through clade-specific (or clade-defining) changes in how average neuronal cell mass relates to numbers of neurons in each structure, and how numbers of neurons are differentially allocated to each structure relative to the number of neurons in the rest of brain. Thus, the evolutionary expansion of mammalian brains has involved both concerted and mosaic patterns of scaling across structures. This is, to our knowledge, the first mechanistic model that explains the generation of brains large and small in mammalian evolution, and it opens up new horizons for seeking the cellular pathways and genes involved in brain evolution.",
		"container-title": "Frontiers in neuroanatomy",
		"DOI": "10.3389/fnana.2014.00077",
		"journalAbbreviation": "Frontiers in neuroanatomy",
		"page": "77",
		"source": "ResearchGate",
		"title": "Corrigendum: Brain scaling in mammalian evolution as a consequence of concerted and mosaic changes in numbers of neurons and average neuronal cell size",
		"title-short": "Corrigendum",
		"volume": "8",
		"author": [
			{
				"family": "Herculano-Houzel",
				"given": "Suzana"
			},
			{
				"family": "Manger",
				"given": "Paul"
			},
			{
				"family": "Kaas",
				"given": "Jon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2014",
					8,
					11
				]
			]
		}
	},
	{
		"id": "herculano-houzel2019",
		"type": "chapter",
		"abstract": "Narratives of human evolution have focused on cortical expansion and increases in brain size relative to body size, but considered that changes in life history, such as in age at sexual maturity and thus the extent of childhood and maternal dependence, or maximal longevity, are evolved features that appeared as consequences of selection for increased brain size, or increased cognitive abilities that decrease mortality rates, or due to selection for grandmotherly contribution to feeding the young. Here I build on my recent finding that slower life histories universally accompany increased numbers of cortical neurons across warm-blooded species to propose a simpler framework for human evolution: that slower development to sexual maturity and increased post-maturity longevity are features that do not require selection, but rather inevitably and immediately accompany evolutionary increases in numbers of cortical neurons, thus fostering human social interactions and cultural and technological evolution as generational overlap increases.",
		"collection-title": "Evolution of the Human Brain: From Matter to Mind",
		"container-title": "Progress in Brain Research",
		"language": "en",
		"note": "DOI: 10.1016/bs.pbr.2019.06.001",
		"page": "179-216",
		"publisher": "Elsevier",
		"source": "ScienceDirect",
		"title": "Chapter 8 - Life history changes accompany increased numbers of cortical neurons: A new framework for understanding human brain evolution",
		"title-short": "Chapter 8 - Life history changes accompany increased numbers of cortical neurons",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0079612319301669",
		"volume": "250",
		"author": [
			{
				"family": "Herculano-Houzel",
				"given": "Suzana"
			}
		],
		"editor": [
			{
				"family": "Hofman",
				"given": "Michel A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					1,
					1
				]
			]
		}
	},
	{
		"id": "herculano-houzel2019a",
		"type": "chapter",
		"abstract": "Narratives of human evolution have focused on cortical expansion and increases in brain size relative to body size, but considered that changes in life history, such as in age at sexual maturity and thus the extent of childhood and maternal dependence, or maximal longevity, are evolved features that appeared as consequences of selection for increased brain size, or increased cognitive abilities that decrease mortality rates, or due to selection for grandmotherly contribution to feeding the young. Here I build on my recent finding that slower life histories universally accompany increased numbers of cortical neurons across warm-blooded species to propose a simpler framework for human evolution: that slower development to sexual maturity and increased post-maturity longevity are features that do not require selection, but rather inevitably and immediately accompany evolutionary increases in numbers of cortical neurons, thus fostering human social interactions and cultural and technological evolution as generational overlap increases.",
		"container-title": "Progress in Brain Research",
		"ISBN": "978-0-444-64317-9",
		"language": "en",
		"note": "DOI: 10.1016/bs.pbr.2019.06.001",
		"page": "179-216",
		"publisher": "Elsevier",
		"source": "DOI.org (Crossref)",
		"title": "Life history changes accompany increased numbers of cortical neurons: A new framework for understanding human brain evolution",
		"title-short": "Life history changes accompany increased numbers of cortical neurons",
		"URL": "https://linkinghub.elsevier.com/retrieve/pii/S0079612319301669",
		"volume": "250",
		"author": [
			{
				"family": "Herculano-Houzel",
				"given": "Suzana"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "zotero-1571",
		"type": "webpage",
		"title": "Metabolic constraint imposes tradeoff between body size and number of brain neurons in human evolution | PNAS",
		"URL": "https://www.pnas.org/doi/full/10.1073/pnas.1206390109",
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		}
	},
	{
		"id": "fonseca-azevedo",
		"type": "article-journal",
		"title": "Metabolic constraint imposes tradeoff between body size and number of brain neurons in human evolution",
		"URL": "https://www.jstor.org/stable/41829953",
		"author": [
			{
				"family": "Fonseca-Azevedo",
				"given": "Karina"
			},
			{
				"family": "Herculano-Houzel",
				"given": "Suzana"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		}
	},
	{
		"id": "koch",
		"type": "webpage",
		"abstract": "Turns out some species are better endowed than we are in key cognitive regions",
		"container-title": "Scientific American",
		"language": "en",
		"note": "DOI: 10.1038/scientificamericanmind0116-22",
		"title": "Does Brain Size Matter?",
		"URL": "https://www.scientificamerican.com/article/does-brain-size-matter1/",
		"author": [
			{
				"family": "Koch",
				"given": "Christof"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		}
	},
	{
		"id": "zotero-1577",
		"type": "webpage",
		"title": "News Blog: Are Whales Smarter Than We Are?",
		"URL": "https://web.archive.org/web/20100727161600/http://www.scientificamerican.com/blog/post.cfm?id=are-whales-smarter-than-we-are",
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		}
	},
	{
		"id": "zotero-1578",
		"type": "webpage",
		"title": "(PDF) Cetaceans Have Complex Brains for Complex Cognition",
		"URL": "https://www.researchgate.net/publication/6328351_Cetaceans_Have_Complex_Brains_for_Complex_Cognition",
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		}
	},
	{
		"id": "marino2007",
		"type": "article-journal",
		"abstract": "A group of eminent cetacean researchers respond to headlines charging that dolphins might be \"flippin' idiots\". They examine behavioural, anatomical and evolutionary data to conclude that the large brain of cetaceans evolved to support complex cognitive abilities.",
		"container-title": "PLOS Biology",
		"DOI": "10.1371/journal.pbio.0050139",
		"ISSN": "1545-7885",
		"issue": "5",
		"journalAbbreviation": "PLOS Biology",
		"language": "en",
		"note": "publisher: Public Library of Science",
		"page": "e139",
		"source": "PLoS Journals",
		"title": "Cetaceans Have Complex Brains for Complex Cognition",
		"URL": "https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0050139",
		"volume": "5",
		"author": [
			{
				"family": "Marino",
				"given": "Lori"
			},
			{
				"family": "Connor",
				"given": "Richard C."
			},
			{
				"family": "Fordyce",
				"given": "R. Ewan"
			},
			{
				"family": "Herman",
				"given": "Louis M."
			},
			{
				"family": "Hof",
				"given": "Patrick R."
			},
			{
				"family": "Lefebvre",
				"given": "Louis"
			},
			{
				"family": "Lusseau",
				"given": "David"
			},
			{
				"family": "McCowan",
				"given": "Brenda"
			},
			{
				"family": "Nimchinsky",
				"given": "Esther A."
			},
			{
				"family": "Pack",
				"given": "Adam A."
			},
			{
				"family": "Rendell",
				"given": "Luke"
			},
			{
				"family": "Reidenberg",
				"given": "Joy S."
			},
			{
				"family": "Reiss",
				"given": "Diana"
			},
			{
				"family": "Uhen",
				"given": "Mark D."
			},
			{
				"family": "Gucht",
				"given": "Estel Van",
				"dropping-particle": "der"
			},
			{
				"family": "Whitehead",
				"given": "Hal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2007",
					5,
					15
				]
			]
		}
	},
	{
		"id": "marino2004",
		"type": "article-journal",
		"abstract": "Toothed whales (order Cetacea: suborder Odontoceti) are highly encephalized, possessing brains that are significantly larger than expected for their body sizes. In particular, the odontocete superfamily Delphinoidea (dolphins, porpoises, belugas, and narwhals) comprises numerous species with encephalization levels second only to modern humans and greater than all other mammals. Odontocetes have also demonstrated behavioral faculties previously only ascribed to humans and, to some extent, other great apes. How did the large brains of odontocetes evolve? To begin to investigate this question, we quantified and averaged estimates of brain and body size for 36 fossil cetacean species using computed tomography and analyzed these data along with those for modern odontocetes. We provide the first description and statistical tests of the pattern of change in brain size relative to body size in cetaceans over 47 million years. We show that brain size increased significantly in two critical phases in the evolution of odontocetes. The first increase occurred with the origin of odontocetes from the ancestral group Archaeoceti near the Eocene-Oligocene boundary and was accompanied by a decrease in body size. The second occurred in the origin of Delphinoidea only by 15 million years ago. © 2004 Wiley-Liss, Inc.",
		"container-title": "The Anatomical Record Part A: Discoveries in Molecular, Cellular, and Evolutionary Biology",
		"DOI": "10.1002/ar.a.20128",
		"ISSN": "1552-4892",
		"issue": "2",
		"language": "en",
		"note": "_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ar.a.20128",
		"page": "1247-1255",
		"source": "Wiley Online Library",
		"title": "Origin and evolution of large brains in toothed whales",
		"URL": "https://onlinelibrary.wiley.com/doi/abs/10.1002/ar.a.20128",
		"volume": "281A",
		"author": [
			{
				"family": "Marino",
				"given": "Lori"
			},
			{
				"family": "McShea",
				"given": "Daniel W."
			},
			{
				"family": "Uhen",
				"given": "Mark D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2004"
				]
			]
		}
	},
	{
		"id": "marino2006",
		"type": "article-journal",
		"abstract": "We test the longstanding hypothesis, known as the dive constraint hypothesis, that the oxygenation demands of diving pose a constraint on aquatic mammal brain size.Using a sample of 23 cetacean species we examine the relationship among six different measures of relative brain size, body size, and maximum diving duration. Unlike previous tests we include body size as a covariate and perform independent contrast analyses to control for phylogeny. We show that diving does not limit brain size in cetaceans and therefore provide no support for the dive constraint hypothesis. Instead, body size is the main predictor of maximum diving duration in cetaceans. Furthermore, our ﬁndings show that it is important to conduct robust tests of evolutionary hypotheses by employing a variety of measures of the dependent variable, in this case, relative brain size.",
		"container-title": "Marine Mammal Science",
		"DOI": "10.1111/j.1748-7692.2006.00042.x",
		"ISSN": "0824-0469, 1748-7692",
		"issue": "2",
		"journalAbbreviation": "Marine Mammal Sci",
		"language": "en",
		"page": "413-425",
		"source": "DOI.org (Crossref)",
		"title": "DOES DIVING LIMIT BRAIN SIZE IN CETACEANS?",
		"URL": "https://onlinelibrary.wiley.com/doi/10.1111/j.1748-7692.2006.00042.x",
		"volume": "22",
		"author": [
			{
				"family": "Marino",
				"given": "Lori"
			},
			{
				"family": "Sol",
				"given": "Daniel"
			},
			{
				"family": "Toren",
				"given": "Kristen"
			},
			{
				"family": "Lefebvre",
				"given": "Louis"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2006",
					4
				]
			]
		}
	},
	{
		"id": "burger",
		"type": "article-journal",
		"abstract": "Why some animals have big brains and others do not has intrigued scholars for millennia. Yet, the taxonomic scope of brain size research is limited to a few mammal lineages. Here we present a brain size dataset compiled from the literature for 1552 species with representation from 28 extant taxonomic orders. The brain-body size allometry across all mammals is 𝐵𝑟𝑎𝑖𝑛 = −1.26 (𝐵𝑜𝑑𝑦)2.34. This relationship shows strong phylogenetic signal as expected due to shared evolutionary histories. Slopes using median species values for each order, family, and genus, to ensure evolutionary independence, approximate ~0.75 scaling. Why brain size scales to the ¾ power to body size across mammals is, to our knowledge, unknown. Slopes within taxonomic orders exhibiting smaller size ranges are often shallower than 0.75 and range from 0.24 to 0.81 with a median slope of 0.64. Published brain size data is lacking for the majority of extant mammals (>70% of species) with strong bias in representation from Primates, Carnivores, Perrisodactyla, and Australidelphian marsupials (orders Dasyuromorphia, Diprotodontia, Peramelemorphia). Several orders are particularly underrepresented. For example, brain size data are available for less than 20% of species in each of the following speciose lineages: Soricomorpha, Rodentia, Lagomorpha, Didelphimorphia, and Scandentia. Use of museum collections can decrease the current taxonomic bias in mammal brain size data and tests of hypothesis.",
		"language": "en",
		"page": "19",
		"source": "Zotero",
		"title": "The allometry of brain size in mammals",
		"author": [
			{
				"family": "Burger",
				"given": "Joseph Robert"
			},
			{
				"family": "George",
				"given": "Menshian Ashaki"
			},
			{
				"family": "Leadbetter",
				"given": "Claire"
			},
			{
				"family": "Shaikh",
				"given": "Farhin"
			}
		]
	},
	{
		"id": "mortensen2014",
		"type": "article-journal",
		"abstract": "Possessing large brains and complex behavioral patterns, cetaceans are believed to be highly intelligent. Their brains, which are the largest in the Animal Kingdom and have enormous gyrification compared with terrestrial mammals, have long been of scientific interest. Few studies, however, report total number of brain cells in cetaceans, and even fewer have used unbiased counting methods. In this study, using stereological methods, we estimated the total number of cells in the neocortex of the long-finned pilot whale (Globicephala melas) brain. For the first time, we show that a species of dolphin has more neocortical neurons than any mammal studied to date including humans. These cell numbers are compared across various mammals with different brain sizes, and the function of possessing many neurons is discussed. We found that the long-finned pilot whale neocortex has approximately 37.2 × 109 neurons, which is almost twice as many as humans, and 127 × 109 glial cells. Thus, the absolute number of neurons in the human neocortex is not correlated with the superior cognitive abilities of humans (at least compared to cetaceans) as has previously been hypothesized. However, as neuron density in long-finned pilot whales is lower than that in humans, their higher cell number appears to be due to their larger brain. Accordingly, our findings make an important contribution to the ongoing debate over quantitative relationships in the mammalian brain.",
		"container-title": "Frontiers in Neuroanatomy",
		"ISSN": "1662-5129",
		"source": "Frontiers",
		"title": "Quantitative relationships in delphinid neocortex",
		"URL": "https://www.frontiersin.org/articles/10.3389/fnana.2014.00132",
		"volume": "8",
		"author": [
			{
				"family": "Mortensen",
				"given": "Heidi S."
			},
			{
				"family": "Pakkenberg",
				"given": "Bente"
			},
			{
				"family": "Dam",
				"given": "Maria"
			},
			{
				"family": "Dietz",
				"given": "Rune"
			},
			{
				"family": "Sonne",
				"given": "Christian"
			},
			{
				"family": "Mikkelsen",
				"given": "Bjarni"
			},
			{
				"family": "Eriksen",
				"given": "Nina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "hopkin2004",
		"type": "article-journal",
		"abstract": "Did mankind trade chewing power for a bigger brain?",
		"container-title": "Nature",
		"DOI": "10.1038/news040322-9",
		"ISSN": "1476-4687",
		"language": "en",
		"license": "2004 Nature Publishing Group",
		"note": "publisher: Nature Publishing Group",
		"source": "www.nature.com",
		"title": "Jaw-dropping theory of human evolution",
		"URL": "https://www.nature.com/articles/news040322-9",
		"author": [
			{
				"family": "Hopkin",
				"given": "Michael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2004",
					3,
					25
				]
			]
		}
	},
	{
		"id": "wrangham2017",
		"type": "article-journal",
		"abstract": "According to current evidence, Homo sapiens was unable to survive on a diet of raw wild foods. Because cooked diets have large physiological and behavioral consequences, a critical question for understanding human evolution is when the adaptive obligation to use fire developed. Archaeological evidence of fire use is scarce before ca. 400 ka, which suggests to some that the commitment to fire must have arisen in the mid-Pleistocene or later. However, weak jaws and small teeth make all proposals for a raw diet of early Pleistocene Homo problematic. Furthermore, the mid-Pleistocene anatomical changes seem too small to explain the substantial effect expected from the development of cooking. Here I explore these and other problems. At the present time no solution is satisfactory, but this does not mean the problem should be ignored.",
		"container-title": "Current Anthropology",
		"DOI": "10.1086/692113",
		"ISSN": "0011-3204",
		"issue": "S16",
		"note": "publisher: The University of Chicago Press",
		"page": "S303-S313",
		"source": "journals.uchicago.edu (Atypon)",
		"title": "Control of Fire in the Paleolithic: Evaluating the Cooking Hypothesis",
		"title-short": "Control of Fire in the Paleolithic",
		"URL": "https://www.journals.uchicago.edu/doi/full/10.1086/692113",
		"volume": "58",
		"author": [
			{
				"family": "Wrangham",
				"given": "Richard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					8
				]
			]
		}
	},
	{
		"id": "levin2021",
		"type": "article",
		"abstract": "Synthetic biology and bioengineering provide the opportunity to create novel embodied cognitive systems (otherwise known as minds) in a very wide variety of chimeric architectures combining evolved and designed material and software. These advances are disrupting familiar concepts in the philosophy of mind, and require new ways of thinking about and comparing truly diverse intelligences, whose composition and origin are not like any of the available natural model species. In this Perspective, I introduce TAME - Technological Approach to Mind Everywhere - a framework for understanding and manipulating cognition in unconventional substrates. TAME formalizes a non-binary (continuous), empirically-based approach to strongly embodied agency. When applied to regenerating/developmental systems, TAME suggests a perspective on morphogenesis as an example of basal cognition. The deep symmetry between problem-solving in anatomical, physiological, transcriptional, and 3D (traditional behavioral) spaces drives specific hypotheses by which cognitive capacities can scale during evolution. An important medium exploited by evolution for joining active subunits into greater agents is developmental bioelectricity, implemented by pre-neural use of ion channels and gap junctions to scale cell-level feedback loops into anatomical homeostasis. This architecture of multi-scale competency of biological systems has important implications for plasticity of bodies and minds, greatly potentiating evolvability. Considering classical and recent data from the perspectives of computational science, evolutionary biology, and basal cognition, reveals a rich research program with many implications for cognitive science, evolutionary biology, regenerative medicine, and artificial intelligence.",
		"note": "arXiv:2201.10346 [cs, q-bio]",
		"number": "arXiv:2201.10346",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Technological Approach to Mind Everywhere (TAME): an experimentally-grounded framework for understanding diverse bodies and minds",
		"title-short": "Technological Approach to Mind Everywhere (TAME)",
		"URL": "http://arxiv.org/abs/2201.10346",
		"author": [
			{
				"family": "Levin",
				"given": "Michael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					24
				]
			]
		}
	},
	{
		"id": "chodosh",
		"type": "webpage",
		"abstract": "Densely packed brain cells help birds achieve surprisingly complex cognition in a tiny head space",
		"container-title": "Scientific American",
		"language": "en",
		"title": "Bird Brains Have as Many Neurons as Some Primates",
		"URL": "https://www.scientificamerican.com/article/bird-brains-have-as-many-neurons-as-some-primates/",
		"author": [
			{
				"family": "Chodosh",
				"given": "Sara"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					29
				]
			]
		}
	},
	{
		"id": "cohen2020",
		"type": "article",
		"abstract": "General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artiﬁcially constructible. Narrow intelligence, the ability to solve a given particularly difﬁcult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classiﬁers, and translators. Artiﬁcial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI’s goals with our own has proven highly elusive. We present the ﬁrst algorithm we are aware of for asymptotically unambitious AGI, where “unambitiousness” includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us.",
		"language": "en",
		"note": "arXiv:1905.12186 [cs]",
		"number": "arXiv:1905.12186",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Asymptotically Unambitious Artificial General Intelligence",
		"URL": "http://arxiv.org/abs/1905.12186",
		"author": [
			{
				"family": "Cohen",
				"given": "Michael K."
			},
			{
				"family": "Vellambi",
				"given": "Badri"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					21
				]
			]
		}
	},
	{
		"id": "chollet2019",
		"type": "article",
		"abstract": "To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.",
		"DOI": "10.48550/arXiv.1911.01547",
		"note": "arXiv:1911.01547 [cs]",
		"number": "arXiv:1911.01547",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On the Measure of Intelligence",
		"URL": "http://arxiv.org/abs/1911.01547",
		"author": [
			{
				"family": "Chollet",
				"given": "François"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					31
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					11,
					25
				]
			]
		}
	},
	{
		"id": "holtzman2020",
		"type": "article",
		"abstract": "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.",
		"note": "arXiv:1904.09751 [cs]",
		"number": "arXiv:1904.09751",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Curious Case of Neural Text Degeneration",
		"URL": "http://arxiv.org/abs/1904.09751",
		"author": [
			{
				"family": "Holtzman",
				"given": "Ari"
			},
			{
				"family": "Buys",
				"given": "Jan"
			},
			{
				"family": "Du",
				"given": "Li"
			},
			{
				"family": "Forbes",
				"given": "Maxwell"
			},
			{
				"family": "Choi",
				"given": "Yejin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					31
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					2,
					14
				]
			]
		}
	},
	{
		"id": "simonyan2015",
		"type": "article",
		"abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
		"note": "arXiv:1409.1556 [cs]",
		"number": "arXiv:1409.1556",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
		"URL": "http://arxiv.org/abs/1409.1556",
		"author": [
			{
				"family": "Simonyan",
				"given": "Karen"
			},
			{
				"family": "Zisserman",
				"given": "Andrew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					31
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					4,
					10
				]
			]
		}
	},
	{
		"id": "athalye2018",
		"type": "article",
		"abstract": "Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.",
		"note": "arXiv:1707.07397 [cs]",
		"number": "arXiv:1707.07397",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Synthesizing Robust Adversarial Examples",
		"URL": "http://arxiv.org/abs/1707.07397",
		"author": [
			{
				"family": "Athalye",
				"given": "Anish"
			},
			{
				"family": "Engstrom",
				"given": "Logan"
			},
			{
				"family": "Ilyas",
				"given": "Andrew"
			},
			{
				"family": "Kwok",
				"given": "Kevin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					10,
					31
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6,
					7
				]
			]
		}
	},
	{
		"id": "devlin2019",
		"type": "article",
		"abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
		"note": "arXiv:1810.04805 [cs]",
		"number": "arXiv:1810.04805",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
		"title-short": "BERT",
		"URL": "http://arxiv.org/abs/1810.04805",
		"author": [
			{
				"family": "Devlin",
				"given": "Jacob"
			},
			{
				"family": "Chang",
				"given": "Ming-Wei"
			},
			{
				"family": "Lee",
				"given": "Kenton"
			},
			{
				"family": "Toutanova",
				"given": "Kristina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					5,
					24
				]
			]
		}
	},
	{
		"id": "gupta2018",
		"type": "article",
		"abstract": "Preconditioned gradient methods are among the most general and powerful tools in optimization. However, preconditioning requires storing and manipulating prohibitively large matrices. We describe and analyze a new structure-aware preconditioning algorithm, called Shampoo, for stochastic optimization over tensor spaces. Shampoo maintains a set of preconditioning matrices, each of which operates on a single dimension, contracting over the remaining dimensions. We establish convergence guarantees in the stochastic convex setting, the proof of which builds upon matrix trace inequalities. Our experiments with state-of-the-art deep learning models show that Shampoo is capable of converging considerably faster than commonly used optimizers. Although it involves a more complex update rule, Shampoo's runtime per step is comparable to that of simple gradient methods such as SGD, AdaGrad, and Adam.",
		"note": "arXiv:1802.09568 [cs, math, stat]",
		"number": "arXiv:1802.09568",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Shampoo: Preconditioned Stochastic Tensor Optimization",
		"title-short": "Shampoo",
		"URL": "http://arxiv.org/abs/1802.09568",
		"author": [
			{
				"family": "Gupta",
				"given": "Vineet"
			},
			{
				"family": "Koren",
				"given": "Tomer"
			},
			{
				"family": "Singer",
				"given": "Yoram"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					3,
					1
				]
			]
		}
	},
	{
		"id": "zhang2019",
		"type": "article",
		"abstract": "Increasing the batch size is a popular way to speed up neural network training, but beyond some critical batch size, larger batch sizes yield diminishing returns. In this work, we study how the critical batch size changes based on properties of the optimization algorithm, including acceleration and preconditioning, through two different lenses: large scale experiments, and analysis of a simple noisy quadratic model (NQM). We experimentally demonstrate that optimization algorithms that employ preconditioning, specifically Adam and K-FAC, result in much larger critical batch sizes than stochastic gradient descent with momentum. We also demonstrate that the NQM captures many of the essential features of real neural network training, despite being drastically simpler to work with. The NQM predicts our results with preconditioned optimizers, previous results with accelerated gradient descent, and other results around optimal learning rates and large batch training, making it a useful tool to generate testable predictions about neural network optimization.",
		"DOI": "10.48550/arXiv.1907.04164",
		"note": "arXiv:1907.04164 [cs, stat]",
		"number": "arXiv:1907.04164",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model",
		"title-short": "Which Algorithmic Choices Matter at Which Batch Sizes?",
		"URL": "http://arxiv.org/abs/1907.04164",
		"author": [
			{
				"family": "Zhang",
				"given": "Guodong"
			},
			{
				"family": "Li",
				"given": "Lala"
			},
			{
				"family": "Nado",
				"given": "Zachary"
			},
			{
				"family": "Martens",
				"given": "James"
			},
			{
				"family": "Sachdeva",
				"given": "Sushant"
			},
			{
				"family": "Dahl",
				"given": "George E."
			},
			{
				"family": "Shallue",
				"given": "Christopher J."
			},
			{
				"family": "Grosse",
				"given": "Roger"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					10,
					28
				]
			]
		}
	},
	{
		"id": "grace",
		"type": "article-journal",
		"language": "en",
		"page": "72",
		"source": "Zotero",
		"title": "Leó Szilárd and the Danger of Nuclear Weapons: A Case Study in Risk Mitigation",
		"author": [
			{
				"family": "Grace",
				"given": "Katja"
			}
		]
	},
	{
		"id": "hubinger2019",
		"type": "post",
		"abstract": "\"Gradient hacking\" is a term I've been using recently to describe the phenomenon wherein a deceptively aligned mesa-optimizer might be able to purposefully act in ways which cause gradient descent to…",
		"container-title": "AI Alignment Forum",
		"language": "en",
		"title": "Gradient hacking",
		"URL": "https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking",
		"author": [
			{
				"family": "Hubinger",
				"given": "Evan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					10,
					16
				]
			]
		}
	},
	{
		"id": "tolpegin2020",
		"type": "article",
		"abstract": "Federated learning (FL) is an emerging paradigm for distributed training of large-scale deep neural networks in which participants' data remains on their own devices with only model updates being shared with a central server. However, the distributed nature of FL gives rise to new threats caused by potentially malicious participants. In this paper, we study targeted data poisoning attacks against FL systems in which a malicious subset of the participants aim to poison the global model by sending model updates derived from mislabeled data. We first demonstrate that such data poisoning attacks can cause substantial drops in classification accuracy and recall, even with a small percentage of malicious participants. We additionally show that the attacks can be targeted, i.e., they have a large negative impact only on classes that are under attack. We also study attack longevity in early/late round training, the impact of malicious participant availability, and the relationships between the two. Finally, we propose a defense strategy that can help identify malicious participants in FL to circumvent poisoning attacks, and demonstrate its effectiveness.",
		"note": "arXiv:2007.08432 [cs, stat]",
		"number": "arXiv:2007.08432",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Data Poisoning Attacks Against Federated Learning Systems",
		"URL": "http://arxiv.org/abs/2007.08432",
		"author": [
			{
				"family": "Tolpegin",
				"given": "Vale"
			},
			{
				"family": "Truex",
				"given": "Stacey"
			},
			{
				"family": "Gursoy",
				"given": "Mehmet Emre"
			},
			{
				"family": "Liu",
				"given": "Ling"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					8,
					11
				]
			]
		}
	},
	{
		"id": "hubinger2021",
		"type": "article",
		"abstract": "We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.",
		"note": "arXiv:1906.01820 [cs]",
		"number": "arXiv:1906.01820",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Risks from Learned Optimization in Advanced Machine Learning Systems",
		"URL": "http://arxiv.org/abs/1906.01820",
		"author": [
			{
				"family": "Hubinger",
				"given": "Evan"
			},
			{
				"family": "Merwijk",
				"given": "Chris",
				"non-dropping-particle": "van"
			},
			{
				"family": "Mikulik",
				"given": "Vladimir"
			},
			{
				"family": "Skalse",
				"given": "Joar"
			},
			{
				"family": "Garrabrant",
				"given": "Scott"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					1
				]
			]
		}
	},
	{
		"id": "deisenroth2020",
		"type": "book",
		"publisher": "Cambridge University Press",
		"title": "Mathematics for machine learning",
		"author": [
			{
				"family": "Deisenroth",
				"given": "Marc Peter"
			},
			{
				"family": "Faisal",
				"given": "A Aldo"
			},
			{
				"family": "Ong",
				"given": "Cheng Soon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "olah2017",
		"type": "article-journal",
		"container-title": "Distill",
		"DOI": "10.23915/distill.00007",
		"title": "Feature visualization",
		"author": [
			{
				"family": "Olah",
				"given": "Chris"
			},
			{
				"family": "Mordvintsev",
				"given": "Alexander"
			},
			{
				"family": "Schubert",
				"given": "Ludwig"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "goodfellow2015",
		"type": "article",
		"abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
		"DOI": "10.48550/arXiv.1412.6572",
		"note": "arXiv:1412.6572 [cs, stat]",
		"number": "arXiv:1412.6572",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Explaining and Harnessing Adversarial Examples",
		"URL": "http://arxiv.org/abs/1412.6572",
		"author": [
			{
				"family": "Goodfellow",
				"given": "Ian J."
			},
			{
				"family": "Shlens",
				"given": "Jonathon"
			},
			{
				"family": "Szegedy",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					3,
					20
				]
			]
		}
	},
	{
		"id": "szegedy2014a",
		"type": "article",
		"abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.",
		"language": "en",
		"note": "arXiv:1312.6199 [cs]",
		"number": "arXiv:1312.6199",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Intriguing properties of neural networks",
		"URL": "http://arxiv.org/abs/1312.6199",
		"author": [
			{
				"family": "Szegedy",
				"given": "Christian"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Bruna",
				"given": "Joan"
			},
			{
				"family": "Erhan",
				"given": "Dumitru"
			},
			{
				"family": "Goodfellow",
				"given": "Ian"
			},
			{
				"family": "Fergus",
				"given": "Rob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					2,
					19
				]
			]
		}
	},
	{
		"id": "2022",
		"type": "entry-encyclopedia",
		"abstract": "JSTOR (; short for Journal Storage) is a digital library founded in 1995 in New York City. Originally containing digitized back issues of academic journals, it now encompasses books and other primary sources as well as current issues of journals in the humanities and social sciences. It provides full-text searches of almost 2,000 journals.\nAs of 2013, more than 8,000 institutions in more than 160 countries had access to JSTOR. Most access is by subscription but some of the site is public domain, and open access content is available free of charge.JSTOR's revenue was $86 million in 2015.",
		"container-title": "Wikipedia",
		"language": "en",
		"license": "Creative Commons Attribution-ShareAlike License",
		"note": "Page Version ID: 1110598393",
		"source": "Wikipedia",
		"title": "JSTOR",
		"URL": "https://en.wikipedia.org/w/index.php?title=JSTOR&oldid=1110598393",
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					16
				]
			]
		}
	},
	{
		"id": "wigner1955a",
		"type": "article-journal",
		"container-title": "Annals of Mathematics",
		"DOI": "10.2307/1970079",
		"ISSN": "0003-486X",
		"issue": "3",
		"note": "publisher: Annals of Mathematics",
		"page": "548-564",
		"source": "JSTOR",
		"title": "Characteristic Vectors of Bordered Matrices With Infinite Dimensions",
		"URL": "https://www.jstor.org/stable/1970079",
		"volume": "62",
		"author": [
			{
				"family": "Wigner",
				"given": "Eugene P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1955"
				]
			]
		}
	},
	{
		"id": "tao2011",
		"type": "article-journal",
		"container-title": "Electronic Journal of Probability",
		"note": "publisher: Institute of Mathematical Statistics and Bernoulli Society",
		"page": "2104–2121",
		"title": "The wigner-dyson-mehta bulk universality conjecture for wigner matrices",
		"volume": "16",
		"author": [
			{
				"family": "Tao",
				"given": "Terence"
			},
			{
				"family": "Vu",
				"given": "Van"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		}
	},
	{
		"id": "elhage2021",
		"type": "article-journal",
		"container-title": "Transformer Circuits Thread",
		"title": "A mathematical framework for transformer circuits",
		"author": [
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "olah2020",
		"type": "article-journal",
		"container-title": "Distill",
		"DOI": "10.23915/distill.00024.001",
		"title": "Zoom in: An introduction to circuits",
		"author": [
			{
				"family": "Olah",
				"given": "Chris"
			},
			{
				"family": "Cammarata",
				"given": "Nick"
			},
			{
				"family": "Schubert",
				"given": "Ludwig"
			},
			{
				"family": "Goh",
				"given": "Gabriel"
			},
			{
				"family": "Petrov",
				"given": "Michael"
			},
			{
				"family": "Carter",
				"given": "Shan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "martin2021a",
		"type": "article-journal",
		"abstract": "In many applications, one works with neural network models trained by someone else. For such pretrained models, one may not have access to training data or test data. Moreover, one may not know details about the model, e.g., the specifics of the training data, the loss function, the hyperparameter values, etc. Given one or many pretrained models, it is a challenge to say anything about the expected performance or quality of the models. Here, we address this challenge by providing a detailed meta-analysis of hundreds of publicly available pretrained models. We examine norm-based capacity control metrics as well as power law based metrics from the recently-developed Theory of Heavy-Tailed Self Regularization. We find that norm based metrics correlate well with reported test accuracies for well-trained models, but that they often cannot distinguish well-trained versus poorly trained models. We also find that power law based metrics can do much better—quantitatively better at discriminating among series of well-trained models with a given architecture; and qualitatively better at discriminating well-trained versus poorly trained models. These methods can be used to identify when a pretrained neural network has problems that cannot be detected simply by examining training/test accuracies.",
		"container-title": "Nature Communications",
		"DOI": "10.1038/s41467-021-24025-8",
		"ISSN": "2041-1723",
		"issue": "1",
		"journalAbbreviation": "Nat Commun",
		"language": "en",
		"license": "2021 The Author(s)",
		"note": "number: 1\npublisher: Nature Publishing Group",
		"page": "4122",
		"source": "www.nature.com",
		"title": "Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data",
		"URL": "https://www.nature.com/articles/s41467-021-24025-8",
		"volume": "12",
		"author": [
			{
				"family": "Martin",
				"given": "Charles H."
			},
			{
				"family": "Peng",
				"given": "Tongsu (Serena)"
			},
			{
				"family": "Mahoney",
				"given": "Michael W."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					13
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					5
				]
			]
		}
	},
	{
		"id": "radford2019language",
		"type": "article-journal",
		"container-title": "OpenAI blog",
		"issue": "8",
		"page": "9",
		"title": "Language models are unsupervised multitask learners",
		"volume": "1",
		"author": [
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Wu",
				"given": "Jeffrey"
			},
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Luan",
				"given": "David"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"literal": "others"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hebbar2022",
		"type": "post",
		"container-title": "AI Alignment Forum",
		"title": "Path dependence in ML inductive biases - AI Alignment Forum",
		"URL": "https://www.alignmentforum.org/posts/bxkWd6WdkPqGmdHEk/path-dependence-in-ml-inductive-biases",
		"author": [
			{
				"family": "Hebbar",
				"given": "Vivek"
			},
			{
				"family": "Hubinger",
				"given": "Evan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					13
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hubinger2019a",
		"type": "post",
		"container-title": "The Alignment Forum",
		"title": "Chris Olah’s views on AGI safety - AI Alignment Forum",
		"URL": "https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety",
		"author": [
			{
				"family": "Hubinger",
				"given": "Evan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					13
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "murphy2022",
		"type": "post",
		"container-title": "The Alignment Forum",
		"title": "Interpretability's alignment-solving potential: Analysis of 7 scenarios",
		"URL": "https://www.alignmentforum.org/posts/FrFZjkdRsmsbnQEm8/interpretability-s-alignment-solving-potential-analysis-of-7#Interpretability_Scenarios_with_Alignment_Solving_Potential",
		"author": [
			{
				"family": "Murphy",
				"given": "Evan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "radford2016",
		"type": "article",
		"abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
		"note": "arXiv:1511.06434 [cs]\nversion: 2",
		"number": "arXiv:1511.06434",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
		"URL": "http://arxiv.org/abs/1511.06434",
		"author": [
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Metz",
				"given": "Luke"
			},
			{
				"family": "Chintala",
				"given": "Soumith"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					15
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					1,
					7
				]
			]
		}
	},
	{
		"id": "parrish2022",
		"type": "article",
		"abstract": "The use of language-model-based question-answering systems to aid humans in completing difficult tasks is limited, in part, by the unreliability of the text these systems generate. Using hard multiple-choice reading comprehension questions as a testbed, we assess whether presenting humans with arguments for two competing answer options, where one is correct and the other is incorrect, allows human judges to perform more accurately, even when one of the arguments is unreliable and deceptive. If this is helpful, we may be able to increase our justified trust in language-model-based systems by asking them to produce these arguments where needed. Previous research has shown that just a single turn of arguments in this format is not helpful to humans. However, as debate settings are characterized by a back-and-forth dialogue, we follow up on previous results to test whether adding a second round of counter-arguments is helpful to humans. We find that, regardless of whether they have access to arguments or not, humans perform similarly on our task. These findings suggest that, in the case of answering reading comprehension questions, debate is not a helpful format.",
		"note": "arXiv:2210.10860 [cs]",
		"number": "arXiv:2210.10860",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Two-Turn Debate Doesn't Help Humans Answer Hard Reading Comprehension Questions",
		"URL": "http://arxiv.org/abs/2210.10860",
		"author": [
			{
				"family": "Parrish",
				"given": "Alicia"
			},
			{
				"family": "Trivedi",
				"given": "Harsh"
			},
			{
				"family": "Nangia",
				"given": "Nikita"
			},
			{
				"family": "Padmakumar",
				"given": "Vishakh"
			},
			{
				"family": "Phang",
				"given": "Jason"
			},
			{
				"family": "Saimbhi",
				"given": "Amanpreet Singh"
			},
			{
				"family": "Bowman",
				"given": "Samuel R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					19
				]
			]
		}
	},
	{
		"id": "korbak2022",
		"type": "article",
		"abstract": "Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and updating it to maximise the expected value of a reward function which captures human preferences, such as non-offensiveness. In this paper, we analyze challenges associated with treating a language model as an RL policy and show how avoiding those challenges requires moving beyond the RL paradigm. We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse: turning the LM into a degenerate distribution. Then, we analyze KL-regularised RL, a widely used recipe for fine-tuning LMs, which additionally constrains the fine-tuned LM to stay close to its original distribution in terms of Kullback-Leibler (KL) divergence. We show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by the reward function. We argue that this Bayesian inference view of KL-regularised RL is more insightful than the typically employed RL perspective. The Bayesian inference view explains how KL-regularised RL avoids the distribution collapse problem and offers a first-principles derivation for its objective. While this objective happens to be equivalent to RL (with a particular choice of parametric reward), there exist other objectives for fine-tuning LMs which are no longer equivalent to RL. That observation leads to a more general point: RL is not an adequate formal framework for problems such as fine-tuning language models. These problems are best viewed as Bayesian inference: approximating a pre-defined target distribution.",
		"note": "arXiv:2205.11275 [cs, stat]",
		"number": "arXiv:2205.11275",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "RL with KL penalties is better viewed as Bayesian inference",
		"URL": "http://arxiv.org/abs/2205.11275",
		"author": [
			{
				"family": "Korbak",
				"given": "Tomasz"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Buckley",
				"given": "Christopher L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					21
				]
			]
		}
	},
	{
		"id": "russell2022",
		"type": "article-magazine",
		"abstract": "Since its inception, AI has operated within a standard model whereby systems are designed to optimize a fixed, known objective. This model has been increasingly successful. I briefly summarize the state of the art and its likely evolution over the next decade. Substantial breakthroughs leading to general-purpose AI are much harder to predict, but they will have an enormous impact on society. At the same time, the standard model will become progressively untenable in real-world applications because of the difficulty of specifying objectives completely and correctly. I propose a new model for AI development in which the machine’s uncertainty about the true objective leads to qualitatively new modes of behavior that are more robust, controllable, and deferential.",
		"container-title": "Daedalus",
		"issue": "Spring",
		"language": "en",
		"title": "If We Succeed",
		"URL": "https://www.amacad.org/publication/if-we-succeed",
		"author": [
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	}
]